[
  {
    "id": "chunk_99cdd78e87",
    "topic_id": "evaluación_comparativa_(benchmarking)_de_implementaciones",
    "content": "¿Qué tal? Buenas noches con todos. El día de hoy, bueno, vamos a comenzar un nuevo módulo. Comenzamos con el módulo de evaluación y ética y y bueno, estamos en la recuperación de la sesión que que no tuvimos la semana pasada el día martes y y bueno, pasó. Boris,\n¿qué tal? ¿Cómo van todos? Sí, mire, muchas gracias por aceptar este espacio el día lunes. Yo sé que no es usual estar un lunes, pero en este lunes vamos a cubrir un tema muy importante que es justamente sobre evaluación. Sé que por un capítulo estuvieron viendo evaluación. Bueno, por favor avísenme si les suena a lo mismo que han estado escuchando y espero cubrir un aspecto diferente ya, pero igual vamos a partir con algo que ya conocemos y justamente de la sesión pasada algunas consultas y me decían, \"Oye, pero ¿de qué forma podemos nosotros asegurarnos de que podemos estar entregando el mismo valor que deseamos con los agentes, ¿no? Y con eso este vamos a empezar a justamente a hablar en esta sesión, ¿no? De la sesión anterior salieron unas dudas porque estábamos hablando bastante sobre cómo podíamos nosotros extender las capacidades de gente por medio del resolución de conflictos, ¿no? Y ahí hemos abordado diferentes escenarios de multia gente, diferentes patrones y habíamos visto el voto y el consenso, ¿no? Y podríamos incluso elaborar este escenarios de voto o debate en relación a justamente estos puntos de evaluación, ¿no? Entonces, ¿quién me puede decir de qué forma interesante podríamos llegar a evaluar a nuestras implementaciones tomando en referencia lo que hemos visto en la sesión anterior? ¿A quién se le ocurre? Nadie. Vamos, porfa. Vamos estirando un poco el chat o estirando un poco el micrófono también. Si quieren pueden abrir el micrófono, siéntese libre. Es un poco de explicar para para poderse explicar, ¿no? Para poder llegar juntos a una respuesta. Si vieron la sesión pasada se van a correr, si no recuerdan, les voy a poner este hintor de la sesión pasada. Y me podrán decir, ¿qué recuerdan de la sesión pasada que nos podría servir? Que la gente es un judge, ya, pero en un consenso o en un debate, ¿qué podríamos hacer?\nHola, ¿qué tal, Boris?\n¿Qué tal? ¿Cómo estás, Jefferson?\nBien, bien. Mira, de mi lado este Me parece que la parte de cómo podríamos evaluar un agente viendo lo que lo que se realizó la semana pasada sería crear diferentes roles con personalidades y entregarle, poder entregarle diferentes input y evaluar que la respuesta sea la esperada. Por ejemplo, si yo tengo un agente eh eh de recursos humanos, por ejemplo, y yo sé que el tipo de respuesta que debe dar dándole diferentes personalidades o diferentes input variándolo. La respuesta de la gente debería ser siempre la misma o acercada siempre a la misma. Con lo que mencionaste la semana pasada de tener orquestado, cada gente podría incluso evaluar si la respuesta es correcta, algo así. Eh, se me parece mi lado he estado trabajando, Boris, con respecto a evaluaciones y lo que tengo es yo tengo un dataset de preguntas respuestas esperadas y todo eso se lo hago a la gente. O sea, e ya tengo una respuesta esperada, yo le envío la pregunta y comparo la respuesta generada por la respuesta que yo tengo guardado y en base a eso seteo por medio de otro LLM. Eh, analizo diferentes métricas, similitud, complejidad de la respuesta, si la respuesta realmente responde lo que estoy preguntando, toxicidad y así tengo diferentes series de métricas,\npero todo eso a partir de un de un dataset que que previamente tengo armado, incluso también la parte de seguridad también, porque digamos que como el agente algunos agentes conversacionales van a estar abiertos a al público, puede haber algún tipo de personas un poquito maliciosas que quieran tal vez saltarse alguna capa de seguridad y tal vez pueda pedir, tratar tratar de que la gente muestre alguna información propia de la empresa o solicitar información de otra persona o tal vez romper el prom para que responda de otra forma. Creo que una En las clases iniciales había mencionado de que había un serie una serie de técnicas que incluso con con un emoji tú le envías un emoji y el emoji está enado porque por dentro tiene como que\nsí\nun pr y se lo salta. Entonces también como que ese tipo de cosas sería una serie de de técnicas que se podrían utilizar para evaluar a gente, ¿no?\nExcelente, Jefferson. Acabas de spoilear a la clase, esta clase y la próxima, pero en realidad sí es justamente esa eso mismo. a lo que estamos viendo en esta clase y con pie de justo esta no sé si prelude o clase también de Jefferson. Vamos a iniciar ahora porque recuerden que nosotros hemos hablado de un debate, una opinión. Ahora, esta opinión que hemos estado viendo con los agentes la sesión pasada decía, \"Yo opino de esto, ¿no? Otro agente podía tener otra visión del mismo problema y opinar y entraban en un debate, luego podían llegar a conclusiones. Teníamos, por ejemplo, este caso de al de alguien que era pro análisis de riesgo de inteligencia artificial en el sector de salud, ¿no? No, que la data sensible, que se tiene que proteger, etc., etc., no debemos aplicar, etc., tipo controles, ¿no? O tipo de casos de uso deberían ser prohibidos. Así pensaba esa gente, ¿no? En cambio había otro, ¿no? Pero mira, hay una gran oportunidad, podemos salvar vidas, podemos lograr mejores condiciones de salud para y entraron en un debate, ¿no? y llegaron a una conclusión de implementaciones con controles, ¿no?, que exista cierto control sobre el uso de la data sensible y demás. Entonces, si se dan cuenta, es un caso de uso. Tuvimos dos posiciones, ¿no? Pero son bien del caso de uso y justo como dice Jefferson, ¿no? ¿Qué pasa si tenemos uno de estos que analiza desde el lado de seguridad? ¿Qué pasa si hay otro que analiza, no sé, del contenido tóxico, ¿no?, que es lo lo que dice toxicity, o sea, alguien empieza a ser demasiado vulgar quizás en el chat o en la misma conversación, ¿no? Entonces es allí que nosotros nos damos cuenta que esto podría cambiar. Esto podría cambiar en diferentes escenarios. Entonces es justamente el escenario de múltiples instancias de un mismo caso de uso que nos ayudan a potenciar las evaluaciones. Yo sé que hay algunas que pueden ir por machine learning tradicional. Ya podríamos tener algunas que hasta con reglas simples vemos si lo logramos o no. Hay ciertos controles y acá voy a estresar la palabrita controles que no necesariamente usan inteligencia artificial. Ya. Entonces, por el resto de esta clase nos vamos a enfocar en los controles que sí usan inteligencia artificial. Jefferson ya me estaba spoileando la siguiente sesión y en la siguiente sesión Vamos a tocar los temas de seguridad, ya, pero en estos temas que son netamente levantar la mano y decir, aquí me suena que esto no fue correcto o acá me parece que esto está alucinando o acá me parece esto otro, es justamente que vamos a enfocar la evaluación comparativa, que es el tema de hoy. Entonces, vamos a revisar diferentes escenarios como esta evaluación del caso de uso de diferentes ángulos podría ser mejor o peor, quizás versus otra. Para eso vamos a establecer para el resto de la clase dos objetivos principales. Primero vamos a comprender estos conceptos de evaluación, o sea, cómo yo sé que me está yendo bien en mi performance de la implementación multiagente, de mi sistema multiagente. Como yo sé que estas implementaciones las puedo contrastar versus otro Llusqu ura de L graph que me permita hacer obtener diferentes resultados, ¿no? Eso lo vamos a ver con LMI y otros más que tengo ahí de sorpresa. Entonces, empecemos con la primera parte. La pregunta, ¿por qué creen que evaluamos a los agentes? Ya. ¿Por qué Jefferson hace todo lo que estaba haciendo? que Jefferson se aseguraba que tal o cual comportamiento no debería pasar.\nAhí levantar la manito. César,\n¿qué tal, Boris? Eh, a los finales la gente pues tiene toda una interacción con el cliente, bueno, en el caso que sea cliente final, ¿no? Y puede dar un impacto negativo que impacte, pues, ya sea pérdidas o tema también reputacional al eh a la empresa, ¿no? Entonces es muy delicado todo el tema de interacción con el cliente y ni qué decir protección de seguridad y más cosas. Yo lo tomaría por ahí.\nYa, Bocán, esa es una muy buena razón. ¿Alguien más tiene otra razón? Evitar comportamientos dañinos o inseguros, ya que es justamente lo que hemos estado hablando. ¿Qué más?\nAy, por ejemplo, Lo que pasa es que yo también estoy probando, digamos,\nestoy en un cliente en la cual están probando también la implementación de un agente\npara su aplicación móvil\ny digamos ahí me tocó la de probar, porque yo soy cua, me tocó probar la parte de ingesta de data, ¿no?\nY ellos hacen, digamos, la ingesta de data a través de confluence y tienen ahí su proceso, todo de cómo van a hacer\ny yo tengo que probar, digamos, esa parte ingesta, pero a la par también se tiene que probar luego la parte de la gente. que es, digamos, que la data que yo he insertado en confluence, eh, cuando yo le pregunte algo a la gente, me tenga que responder así como decíamos, ¿no? Con un contexto similar, ¿no? Eh, y que no vaya a alucinar. En caso, digamos, si yo borro e la página o ya venció la página, eh no debería de debería decirme, en este caso, como no sale, digamos, no está fuera del dominio, debería decirme, este, lo siento o no tengo esa información. No. a pesar de que sí pueda tenerla, pero la gente está limitado. Entonces, lo que ahí voy es que debe de estar, digamos, este, dando la respuesta según el contexto o o cómo ha sido entrenado con su base de datos de conocimiento.\nExacto. No es establecer como que ciertos criterios de calidad, ¿no? Vendrían a ser como que asegurarme que se entrega la información que debe ser y si es que la información expira, digamos, y ese es un adecuado gobierno de data, O sea, ya me estoy saliendo un poquito del scripto, de nuevo, pero cuando ustedes hagan implementaciones de rag, o sea, cuando hagan retenation, siempre van a salir estas métricas rack. O sea, yo ahorita voy a mencionarles una lista de métricas base, pero ustedes pueden hacer el drill down y preguntarle a su compile o lo que sea. Lístame métricas específicas para y tú le puedes decir retribu aumentation, uso de en agentes e en nuestro caso vamos a verlos las métricas específicas, ¿ya? Pero yo a priori les puedo decir dónde se caen a cada rato los agentes, ¿ya? ¿Dónde se caen a cada rato los agentes que utilizan confluence? Por ejemplo, en el documento que no es correctamente administrado por humanos. O sea, miren por dónde cayó. Si yo como humano agarro y le digo, \"Oye, mira, tengo todo mi repositorio de confluence donde ha trabajado todo mi equipo, conéctalo para que todos sepamos cómo entregar información rápido. Enchúfalo no más. Si yo voy y enchufo eso de frente, va a funcionar bien. Vos saber a priori si va a funcionar bien. Pues no. ¿Por qué? Porque puede haber información contradictoria en ese mismo repositorio. Puede haber información que, por ejemplo, ¿no? Eh, Pepito escribió que no sé, el cielo es azul y José Lito dijo, \"No, el cielo es gris porque vive en Lima.\" Y no sé, eh, María que el cielo es celeste porque a veces hay nubes, a veces no. Entonces, al final cuando le pregunten de qué color es el cielo, no va a saber responder porque tiene información que puede entrar en conflicto o podría responder algo que no es realmente cierto o algo fuera del contexto, ¿no? Ubicación, clima, ¿ve? Ya cambió de qué color es el cielo, tiempo del año, ¿qué mes estás? Si es en verano, pues sí. es azul. Entonces, al final esas variables del problema no se reflejaron en la documentación y es ahí donde salen justamente esos errores. Entonces, todo esto debiera acompañado con cierto gobierno de esa información de establecer, por ejemplo, este documento detalla al producto a en, no sé, requisitos para obtener el producto, ¿no? Si es que es, digamos, una entidad financiera pedir requisitos para el cliente, qué cubre, qué no cubre, tasas, tarifas, etc. Esa información que esté en un solo documento, que sea único, permite mucho más fácil referenciar la información. Ahora, ¿qué escenario de caos si es viable para los agentes? Una base de datos de grafos, por ejemplo, donde por popularidad el conocimiento se hace relevante. Entonces, una base de datos de grafo por conocimiento y por cantidad de vértices asociados a cada punto de data podríamos decir que coge popularidad y es así como que al final funciona funcionó ahora es mucho más sofisticado Google y los otros más, ¿no? Por grafos. Entonces al final por ese grado de popularidad podrías decir que es verdad. Entonces es como creo que sí les dije cuando te dicen una mentira muchas veces al final esa mentira como que se asume verdad porque ya es tan popular que se asume que es cierta. Algo así. Entonces algo así es como se establece un factor de verdad. Es en los casos caóticos para todos esos hay métricas también. Es un es groundedness, es cuán cercano estoy yo a la data para entregar esa información. Otro es relevance. ¿Qué pasa si me saqué un chank que no corresponde a la pregunta? Me va a responder cualquier cosa, ¿no? Y se va a asumir como alucinación. Entonces, al final todos esos son los porqués. Entonces recuerden al final, ¿no? Un agente al va a ser autónomo porque va servir su entorno. En base a ese entorno va a tomar diferentes acciones para lograr su objetivo. Entonces, si el entorno no es coherente, no es adecuado, no es congruente con la acción que va a tomar el agente, pues se va a equivocar. Y nosotros como humanos a veces creamos entornos caóticos que no son enteramente blanco y negro o con cuadraditos de colores, sino que hay veces que hasta nosotros mismos En nuestro actuar creamos un entorno confuso, ambiguo que lleva a que los agentes en sus diferentes roles, como ven acá, diferentes gensen con diferentes roles. Por ejemplo, tenemos un Virtual Lab AI, un software security AI agent, employee support AI agent y así un montón de agentes que cubren diferentes roles. Cada uno va a interactuar en diferentes entornos, podrían trabajar también entre sí, pero todos tienen diferentes objetivos. Entonces, ese es el otro. ¿Por qué? Porque todos van a tener diferentes objetivos y debemos saber cuán bien completan ese objetivo, ¿no? Cuán bien la gente logra su objetivo. Lo logra, lo logra parcialmente, no lo logra. O sea, me cree, mi gente, ya chévere, funciona, funciona tu gente. Necesitamos evaluar a la gente, ¿no? Tenemos que pasar el examen, si no cómo sabemos que esto funciona. Entonces este este capacidad de actuar de la gente en un entorno nos va a determinar el grado de éxito y para eso tenemos que tomar esas medidas. Por eso, no sé si vieron este reporte de MIT que hablaba mucho sobre el porcentaje de los proyectos fallidos de inteligencia artificial a nivel global, creo que solo el 5% pasaba la valla. Y era, ¿por qué? Porque se enfocaba en evaluación. Todo el mundo avanzaba, avanzaba le mandaba producción, pero nunca nadie se ejecutaban evaluación. Ya. Pero no todo el mundo se mandó a ejecutar ejecutar de forma rigurosa cada una de las evaluaciones en diferentes aspectos y recoltar la data del entorno. Entonces es un poco esa la misión que tenemos cuando evaluamos agentes. Es primero transparentar que las decisiones de estos agentes pueden determinar decisiones futuras del siguiente agente que entra en el plan de ejecución o a la persona que luego toma acciones después de que la gente le informó ciertas cosas. ¿Qué pasa si al final un agente informa mal a un usuario? Porque el archivo estaba caduco, o porque el archivo que tenía yo publicado estaba contradiciendo a otro y se informó erróneamente al cliente. Pues el cliente va a voltear a tu empresa y te va a decir, \"Oye, tu agente me dijo esto. Yo seguí los pasos que me dijo tu agente y acabé, digamos, sin tarjeta o sin o con o sin dinero o con acabó comprometiendo mi información, no sé, tantos escenarios que pueden materializarse. Entonces, al final tenemos que entender que este entorno de la gente debemos buscar también administrarlo. Y este entorno es el entorno de ejecución de cada agente que al final implementa el contexto de ejecución de cada usuario. El usuario va a llegar con un contexto muy diferente. La agrupación de todos esos contextos debería ser evaluada también. Entonces, Cada usuario va a tener otro mundo de información nueva que va a llegar a la gente. Por ejemplo, no. Yo defino mi agente conforme el caso de uso, conforme hemos visto su objetivo. Ya, bacán, lo pruebo, ¿no? Lo pruebo con mi dataset. Lo pruebo con mi dataset. Agarro mi dataset, justo el que decía Jefferson, su datas de prueba. Lo manda el datacet de prueba, ve todo verdecito, ¿no? Acá va. Pasamos a testing en certificación. Saco otro dataset para Lo mando ya verdecito, todo. Lo mando a producción ese más chiquito, flum, lo lanzo y por probabilidad va a dar verdecito, pues no. ¿Qué pasa si de todos esos datasets que yo confeccioné, nunca saqué a este usuario que tenía un contexto diferente a comparación de los que presiden en el dataset? Va a fallar. Entonces ese fit pack es muy importante recogerlo. Entonces, para eso es importante que en todas sus arquitecturas de solución tengamos mecanismos de telemetría y mecanismos para recoger el feedback. No sé si bueno, sí lo han visto. En chat GPT te salían los pulgares, en Cloud Code te salen igual este formas para resaltar cierto texto y luego decirle dar feedback. Oye, me gustó tu respuesta, pero este pedacito del texto te quiero dar feedback. porque no es tanto así o no me sirvió o creo que al final me está dando problemas, ¿no? Ese mecanismo de dar feedback es muy importante para otro mecanismo de aprendizaje que es por logs, que eso creo me parece que ya lo vieron en una sesión anterior. Entonces, el aprendizaje por logs te ayuda justamente a coger ese feedback para que en la siguiente evaluación esta persona que no estaba en los datasets sea tratada bien dentro del el campo de acción. de la gente y ya la gente no f. Ven. Entonces así logramos una mejorada continua. Ahora, ¿qué podemos evaluar de cada gente? Pues diferentes aspectos del entorno, ¿no? Hemos hablado de sus objetivos, podemos empezar a hablar de tiempos de ejecución, o sea, ¿qué tan eficiente es? Por ejemplo, siempre han visto seguro en en si han estado revisando ejemplos de cómo implementar diferentes system prompt a cada rato les Sé conciso, sé puntual. Eso busca ahorrar tokens sobre todo en escenarios multiagente. Luego, seguro, no sé, ya mejor les pregunto a ustedes, ¿cuántos de ustedes han encontrado un markup language para System Prompt o algún tipo de formato para administrar System Prom? Hay uno que se está volviendo popular recientemente, el de Ah, oh, ya les da les suelto una de Open AI. A open AI le gusta los tags. tipo HTML. Ya, ¿qué otro formato visto? A ver, el chat dice tax. Es es tipo markup, tipo HTML. Había Ajá, otro STOM. Ya, genial. Jefferson, ¿nos puedes explicar un poco ese de Tom? Creo que es con N Tom, como cartoon. Y había otro más, este, Antropic, por ejemplo, prefiere, ¿no? Open AI, prefiere Jason. Antropic prefiere, o sea, Antropic Clot prefiere tax, que era lo que dice César. Tun es una combinación, es con N, con M, no me acuerdo. Es como una combinación de markup. No sé si estoy, me dicen si estoy proyectando toda mi pantalla.\nSí, con N. Sí.\nAjá. Es con M. Sí, sí, sí. Sí, sí,\nsí. Supuestamente es una forma de escribir el texto para el LMS y la idea es como que el Jason, supuestamente el Jason este es un es una estructura, pero a nivel de token puede consumir más token. Entonces como que crearon este formato todo que puede manejar data estructurada pero para el LM consumiría consumía menos token.\nClaro, es es con N, es format orient. Acá lo voy a salir un ratito para mostrarle. Entonces, justo este es Tun token oriented object notation, que está una orientación basada en token para escribir más o menos tu system. ¿Y por qué? Porque un Jason, por ejemplo, lo hace como si fuera Yamel, pero al final si se dan cuenta es ayuda un poco a reducir el consumo de tokens. Todo el mundo lo hace porque los tokens cuestan, pues no. Aunque la tendencia global es que el costo por token se reduzca, obviamente todo el mundo busca ser más eficiente. Entonces, utilizando notación TUN, también me ayuda a ver cuán eficiente es el agente haciendo lo que hace. Ahí les dejo el enlace a todos. Entonces, es diferentes escenarios que te llevan a medir, oye, okay, este, ¿cuán eficiente va a ser en su ejecución? Ahora, ¿qué puedo hacer? No, puedo hacerme un sistema prom normal, regular, tan forme conforme yo lo suelo hacer. Hasta podría ser markup language y Y luego podría hacerme otro sistema en Tool y ver cómo cambian, ¿no? Y los puedo probar uno contra el otro y ver cuál tiene menor tiempo de ejecución, menor costo de token. Les comparto una experiencia propia. En un esquema multiagente en Bed Rock estábamos usando Nova Micro. Ajá. No, estamos Nova Pro de ABS, el más caro todavía. Nova Pro de AWS y estábamos usando Cloud Hiku. Entonces, Cloud Haiku era como que el supervisor y Nova Pro eran los subagentes, digamos, ya. Y notamos que se demoraba un montón, o sea, habían pedidos como que era, oye, resume mi historial de movimientos y dame una conclusión para mejorar mi perfil, ¿no?, financiero, algo así. Entonces, cuando pasaba ese tipo de instrucción, todos que hablaban mucho entre ellos y mucho entre ellos en inglés y se demoraba traduciendo lo del español al inglés a cada rato y nadie le puso en el system prompt el seconciso. Entonces, ¿qué cosas se cambiaron, no? Primero, dentro de la eh la configuración del supervisor, no se puso un ID vuelta en comunicación al supervisor porque estaba configurado por defecto para que le puedan responder al supervisor de forma arbitraria. Entonces le dijeron, \"Tu labor es delegar al supervisor.\" Explícitamente le dijeron eso. Y una vez que delegas, esperas respuesta. Si te piden aclaración, esa aclaración se la pides hasta el usuario. Su única labor es delegar. Entonces, cuando hicimos ese cambio en el supervisor, dejaron de conversar mucho entre sí y la respuesta fue más rápida. Si había algo ambiguo, pues pedían que el mismo usuario lo aclare. El otro punto es de que para Antropic lo cambiamos el system prom inglés y la última respuesta de supervisor, bueno, todos los subentes en inglés y el supervisor era el único que podía responder en español. Entonces, al final todos empezaban a hablar en inglés y la única respuesta en español. Entonces, tod todas estas acciones nos dimos cuenta. ¿Por qué de nuevo? Porque la telemetría, los logs y las ejecuciones de las pruebas nosotaban el tiempo de ejecución, la cantidad de toques que se usaban y la calidad de la respuesta también. Entonces, ¿cuán eficiente era el inicio? Era terriblemente caro en tiempo y en cantidad de tokens. Haciendo esos cambios se bajó, miren, de lo que hasta 20 segundos y se bajó a dos, siendo la respuesta más larga 2 segundos. Y creo que se puede esperar en un chat como que un dos y ya te respondí, ¿no? Un, dos y ya te respondí. La respuesta este se bajó un montón en tiempo y en costo de tokens también. dramáticamente, lo que estábamos como 17,000, 18,000 se bajó a 200 250 tokens que se usaba por cada gente. Eso más o menos 5 algo de 1000 tokens. Entonces al final que se bajó la cantidad de tokens, se bajó el tiempo de ejecución también, se volvió más eficiente. Ahora, más confiable y predecible va a depender también del campo de acción que le demos a nivel de autonomía, ¿no? Eso va a depender de si okay Estamos haciendo como un workflow multia gente, lo estamos haciendo completamente autónomo, pero a la vez tenemos un script para el supervisor que sabe más o menos cómo virar el timón cuando sea necesario, ¿no? Y ver cómo se deteriora esto en en casos complejos, ¿no? Que como este caso que les decía, ¿qué pasa si pasamos todos los datasets a esta producción, todo da verde y llega este señor que era el caso pues que el rebuscado digamos, ¿no? Y qué pasa si era uno pero qué pasa si era un patrón de un montón de usuarios que nunca los vimos en los datasets, ahí se compromete más, ¿no? Entonces, eso es importante también considerarlo. Ahora, esto es bien las cosas evaluar, pues se pueden haber diferentes puntos para determinar qué es un pas o un fail en el éxito. A veces los criterios de éxito son un poco subjetivos, ¿ya? Y es por eso que también es importante seguir dándole vuelta y vuelta y vuelta. Recoger feedback, como les decía, y la misma evaluación es un proceso iterativo. Entonces, al ser un proceso iterativo, debemos pasar por estos ciclos, ¿no? Definir la tarea con objetivo. CL. Sí, acá va a estar tu system prompt. Ya, pero cuando toca evaluar no solamente es un system prompt. Tienes que definir acá tus métricas. Si es que vas a tener métricas customs. Dentro de este custom matrix tienes que decir qué equivale a cer que equivale a uno y qué equivale a lo que está en el medio o si es que vas a tener y uno nada más. Y después de tener estos customrics, ¿cómo yo determino al final un porcentaje de éxito? Eso para algún reporte, ¿no? Esa es la definición de la tarea de evaluación, ¿ya? Un objetivo claro y métricas de éxito. O sea, ¿cuándo yo sé que esto es un éxito? Ahora, muy a Aparte de eso no lo dice la el así tal cual lo menciona en este caso L chain, pero yo sí se los menciono. No solamente es esto, no solamente definir el objetivo clorimétricas de éxito. Hay veces que tú tienes que darle, o sea, sí, tu sistem prompt le tienes que dar ejemplos de éxito. Esto para mí es un éxito le dices. Y esto para mí es un fallo, fail. Entonces tienes, mira esta tupla de positivo, negativo. ¿Cuántos han visto negative prompting? Levanten su mano. ¿Cuántos han visto negative prompting? Prompt negativo. ¿Alguien quiere decir que es un prom negativo? Un prom negativo. Así que te digo, no, no esto, no lo otro. ¿Y en qué contexto? se usa. Bueno, si se meten a Hugin Face y buscan stable diffusion models, van a ver los primeros stable diffusion models que salieron el 23, inicios del 24. E cuando ah cuando mi journey empezó a volverse un hit en el internet, ya cuando mi journey apareció estaban este todo lo de estability AI y stable diffusion models y stable diffusion tenía una forma de crear este imágenes que tú le dabas un prompt, luego le dabas un negative prompt, o sea, tú decías, quiero esto de forma positiva y no quiero esto. Esta técnica le permitía a los stable edificar primero un garabato, ¿ya? Y después empezar a perfilar lo que realmente querías, ¿no? Y lo iba borrando y decía, \"Ah, tu garabato en sí es una bicicleta, ¿no? Bueno, sigue siendo este un garabato, pero de ese garabató ya se perfiló a lo que no quería y lo borró y le quitó esto y le quitó otro y le quitó el aquella y se volvió la bicicleta. Entonces, de esa forma como que empezaron a perfilar mejor los modelos de stable diffusion. Eso mismo se puede aplicar en un escenario de evaluación. Tú le puedes decir qué quieres y qué no quieres. Y esto le sirve a cualquier punto de escenario, supongamos, ¿no? Si yo le digo, este es mi positivo y este es el negativo. Va a ser más fácil incluso para un modelo de machine learning el decirte, ah, mira, ya me aprendí modelos negativos y modelos positivos y ahora puedo clasificar mejor cada uno de los casos que llegue. Y esto obviamente en un escenario más caótico con un ll pues va a ser más sencillo. Ya. Entonces podemos hacer este tipo de escenarios. Lha te ofrece que le des un caso de respuesta, sí, pero no el fallido. No te da no netamente la capacidad de que tú puedas decirle, este es tu system prompt, este es tu ejemplo y este es un output esperado. Hasta allí deja como que en el positive, no te deja el negative. Ya, hasta ahí llegas llegas con Lmith, perdón, con LMI se se llega hasta ahí. Entonces, esta es la definición de un de una tarea de un objetivo, una tarea de evaluación. Creo que hubo algo en el chat ahí especificar lo que no se desea. Ajá. Ese es el justamente el negative prom. decir que no quiero. Okay. Y con eso ya lo mando a la ejecución, ¿pues no? Y cuando llego a la ejecución, miren lo que vemos acá. La gente interactúa con el ambiente y con las herramientas. Bueno, al final decir ambiente es herramientas, el usuario, etc., etc., es el environment, ¿no? Entonces, su entorno, como les digo yo, cuando actúa con todo su entorno, pues va a decir este agente, \"Oye, yo siendo el agente de evaluación de Boris, que me ha determinado acá una tarea con un objetivo y unas métricas. ¿Cómo puedo lograr obtener estas métricas? Tengo acá, digamos, una terminal de Python. Perdón, perdón, tengo una terminal de Python, tengo acá documentación de cómo ejecutar las métricas en sí. Además de eso, tengo una herramienta que es un API donde puedo publicar mis resultados y tengo acá un pedazo del file system que lo voy a utilizar como si fuera mi borrador, digamos, mi cuaderno. Ya. Entonces, eso es un poco lo que le puedo dar, ¿no? Tengo un poco de file system, tengo un API, tengo documentación, una determin ejecutamos lo y tengo como el cómo va a estar acá, ¿no? Este es el no. Entonces voy a saber cómo ejecutar la prueba que se mi p el la terminal, todo. Entonces ejecuto las métricas conforme este se me entregan, ¿no? Para ejecutar el agente. Pero el el en la ejecución de la gente no solo es una única instrucción y acá viene una decisión importante. Ya, ese es Ah, ya voy a pasar acá mejor a la pizarra un ratito para explicarles mejor. Ya. Cuando se toman este tipo de decisiones de determinar el scope de ejecución, va a depender de cada caso de uso. Este paso de recupilar la traza, este ¿Por qué? Y espérame, estaba con la pizarra de Zoom. Ahora voy a cambiar de pizarra. Entonces, ¿por qué? Ahora todo lo que vean en plumón negro es lo de Zoom, que es lo que les digo del siguiente punto, porque en este punto acá en recopilar traz A hay un decisor. Tu caso de uso es, digamos, one shot. Podría ser un caso bachero, un one shot, ¿no? Pero podría ser otro que no lo sea. O sea, puede que yo llegue y te diga, \"Oye, ¿sabes qué? Este, necesito honestamente que me ayudes. Est, ¿dónde está? Acá está que me ayudes con no solo un mensaje. Ac va a entrar la iniciativa y te voy a preguntar, ¿es un one shot?\" Si es un one shot. Okay. Tú el tamaño de todo tu trace, digamos, va a ser corto y ese es tu trace que va a estar acá. Todo lo que has hecho quizás de tu proceso batch en el en el tema de reclutamiento quizás no, pero si no lo es, la unicidad del análisis cambia. Acá la unicidad va a ser netamente el invoke. Ya, ese va a ser el único invoke que hagas. Ese es tu scope y se acabó. Esto es un sí. Esto es un Ya, si es que es un one shot, o sea, como que single Llm para un agente, podría ser un agente que tome herramientas de su entorno, analiza y te lo deja en un repositorio de archivos una vez al mes y se acabó, pues no. Y podría tener cierta flexibilidad el agente mismo en su ejecución. Sí. Y por eso es agente, porque necesita justo un poco de autonomía para cada uno de esos casos, ¿no? Pero ese vendría a ser más que un workflow agéntico. Ahora, Este workflow agéntico si tiene un único invoke para entregar la respuesta, okay, su trace no va a ser muy grande porque va a ser jale esta información, jale esta otra, jale acá un único invoke y este invoke te da la respuesta, ¿no? El resultado y ahí se acabó tu tu scope de análisis. Es el el más simple, ¿no? De Pero el otro, el conversacional es es el donde tiene más complejidad, podríamos decirlo. ¿Por qué? Porque acá tú tienes que definir tu scope. Acá tú tienes que definir dos cosas. Primero, vas a definir que la evaluación sea a nivel del mensaje, o sea, un único mensaje. Tú podrías decir, \"Oye, ¿sabes qué? Yo quiero que el la calidad de la de la evaluación la hagamos al mensaje. ¿Por qué? Nivel del mensaje. ¿Por qué? Porque puede ser en el caso de uso cada mensajito no construya parte de un todo, sino que responda a cosas puntuales. Hay casos típicos como, ¿cómo hago esto? Ayúdame a hacer aquello. Oye, me he salido este problema y no sé cómo hacer. Ah, mira, sigue este paso a paso. Ah, mira, este esta esta forma de resolver algo. Ah, para solicitar tal cosa y estos pasos, ¿no? Ese tipo de cosas, ese tipo de preguntas sí se pueden orientar a un único mensaje. Entonces, la unicidad de calidad es el mensaje. Un foco de evaluación es un mensaje porque tu caso de uso puede ser resuelto en un mensaje. Entonces, la pregunta es, ¿miodo se puede resolver con un único mensaje? ¿Con interacción del usuario? Si la respuesta es sí, okay, tu unicidad es el mensaje. Pero, ¿qué pasa si no? ¿Qué pasa si, por ejemplo, estoy en un agente que hace una evaluación, no sé, psicomotriz, psicológica o como los agentes ahora de reclutamiento por teléfono que te hacen como una entrevista por teléfono. Entonces, al final es progresivo, ¿no? En vez de tener un mensaje, vas a tener una colección de mensajes. Entonces, para responder si es uno, okay, tu trace se enfoca en la calidad del mensaje, pero si no es si es más y le voy a poner estrellita acá. Si es más de un mensaje, acá tienes que ver todo el threat, todo el hilo de conversación. ¿Por qué? Porque vas a tener los turns, pues no vas a tener el que te dice user human, user, perdón, user AI, user AI, user AI. Y todo eso es la forma en la cual estamos construyendo la ejecución final del objetivo, ¿no? Entonces es tu objetivo tiene unicidad de mensaje o tu objetivo está mucho más atrás. Si tu objetivo no es se puede dar en un único mensaje, ¿okay? Te toca enfocarte en el trace, que es en la colección de mensajes. Ya hay escenarios que pueden ser incluso combinados, ¿no? Yo podría tener parte del trace, o sea, parte que se resuelva con parte de casos de uso que resuelven con un único mensaje y parte con trace. Entonces ahí me conviene hacer una estrategia híbrida. Bueno, de estos dos, ¿no? Para vamos a probar los casos resolvibles en un único mensaje. Te mandas con todos y tu foco de análisis de calidad es un único mensaje. Eso lo digo al momento de establecer el pipeline de evaluación, pero hay otros que necesitas todo el trace para poder saber si es lograr el objetivo o no. Ya, ese eso es netamente. Entonces, cuando ya defines esto, tú vas a decir, \"Okay, mi unicidad es un mensaje o Mi iniciad es todo el trace. Lo más probable es que te vayas a todo el trace y tu escenario es conversacional. Y lo más probable es que hagas un single message si estás en un escenario batch quizás no de un workflow. Puede ser un workflow, un workflow batch o una capacidad agéntica aumentada, algo así. Es lo más probable. Ya, pero puedes caer en el medio si es que tienes uno y uno. Ya, una vez defines esa atrás Pues tienes que ver formas de recopilar esa traza. Hemos visto con Lmith cómo jalar todo ese trace. ¿Recuerdan que podíamos sacar el trace de un árbol de Lraph y ver cada gente cómo invoca sus herramientas? ¿Cómo este hace el invoc al LLM? ¿Cómo le responde? ¿Cómo procesa luego esa respuesta del otro gente y así? No, todo, todo ese trace es el que al final recupilarías y tienes este ida y vuelta, ¿no? Y vuelta de de mensajes. Entonces, recopilas la traza y en base a esto se lo preguntas a la gente, pues no, y le dices, \"Oye, agente, ya que tengo este trace y tú tienes esta misión, ejecuta tu misión sobre este trace, sobre esta traza.\" Entonces, yo ya definí el m los dos y sobre eso vamos a ejecutar estas métricas. Ejecutadas las métricas, ¿qué va a pasar? El agente pues va a hacer su propio invoke. Esto si es ll as a judge o podría ser agent as judge. ¿Cuál es la diferencia entre llage? Pues el nombre lo dice, uno es una invocación al LLM y el otro puedes tener un agente con más herramientas que evalúan, o sea, podrías ponerte hasta tu proceso de negocio como API a que lo utilice como herramienta la gente para evaluar. Entonces, si tú tienes, digamos, un modelo de machine learning, modelo de machine learning que está detrás de un API y ese API lo engranas un agente, la respuesta de tu sistema de agente podría servir como input a este modelo de machine learning y el mismo agente evaluador decirle decirte, ¿no?, si es que pasó o no el modelo de scoring de tu empresa. Entonces, imagínate lo potente, podrías empezar hasta evaluar una ejecución propia contra infraestructura por medio de un agente. Entonces, Este agente va a tomar esta tasa, le va a hacer el inboke a lo que recibió de la traza conforme de sus herramientas. Ese es un agent as judge y el otro es ll as a judge. As a judg. Mi letra es terrible con el mouse, disculpen. Entonces tenemos esos dos conceptos, ¿no? L el májaj o el agent judge. Todos estos me van a entregar resultados. los dos me entrega resultados. Estos resultados son el mismo feedback que luego nos sirve para luego cambiar cosas, ¿no? Aquí vamos a darnos cuenta, okay, estamos teniendo una buena ejecución al momento de componer el resultado, pero no estamos logrando, no sé, cierto criterio. Hasta podría ser a nivel editorial, ¿no? Tú estabas viendo justo un caso que vamos a tocar de un equipo editorial que tiene que que seguir cierta pauta editorial y la pauta editorial puede ser este evaluada por un agentas church, ¿no? Que tenga más o menos las nociones de cómo debería ser el editorial y jalado, ¿no? El editorial puede hacerme una métrica custom, a eso voy, ¿no? No es solamente las de las de cajas que te pueden venir como alucinación, como relevancia o cuán grounded, o sea, cuán sustentada está su respuesta. Ese tipo de de métricas son valiosas, sí, pero también tú puedes establecer tus propias métricas que podrían ser propias del negocio, ¿no? Como el editorial. Una vez tienes estos resultados, puedes darte cuenta del grado de salud de esta gente y qué puedes hacer, entender dónde falló, qué funcionó bien y qué mal. Y cuando haces entender, ¿qué creen que pasa acá? ¿Qué pasa del analizar al de nuevo definir la tarea? ¿Qué pasa aquí? ¿Qué me dice que pasa ahí? Eso le ocurre feedback. Ajá. Pero es obviamente va a ser el feedback, pero cómo lo implementamos, de qué forma se ve ese feedback, la retroalimentación, cómo la vemos o qué hacemos. Ya, ponte, sale el reporte. Gente, jalado bien, jalado bien. Jaló bien. Ya. Una es mejorar el system. Ya está buenazo. Eso es una acción. ¿Qué más podemos ver o qué tendríamos que ser? Imagínate este dashboard donde ves todo lo mal que está pasando en tu agente, los rojos, los ardecitos. Te dice, por ejemplo, ah, mira, tenías un garrail que te decía, \"Oye, protege de que se pasen lisuras.\" No. y tú pusiste tu garrael. Pero, ¿qué? ¿Por qué pasó esta lisura? Y tú dices, \"¿Por qué no consideré esta lisura después de que viste el reporte?\" Ya. Entonces, van a haber cambios que sí son al system prom y otros que van a ser alrededor del system y eso le dicen metadata tuning. ¿Ya? ¿Qué pasa? Esto creo que los comenté la vez pasada. el entorno. Espérense, vamos a cambiar de color para que no se confunda. Yo más creo que acá voy a poner whiteboard un rato. A ver, un momento. Share eh whiteboard. Ya. Okay. Sí, me avisan cuando vean el whiteboard. Whiteboard. Ajá. Ya. Entonces, ¿qué pasa cuando tenemos Este el problema, este es el entorno entorno del problema. Dentro de ese entorno está el contexto, ya este es el contexto del usuario. Y ahora sí, no se están tapando un poquito, pero bueno, ahora Ahora, mientras voy relatando, contexto es este, ¿ya? Y acá en este contexto está todo lo que yo ejecutado. Esto es lo que conoce el mundo de del problema, ¿ya? Esto es lo que conoce el mundo sobre este problema. Ese es el entorno. Ahí está. Este es el contexto del problema. Es es el entorno. Es el problema documentado. Ahora hay Otra más grande que este. Este es el A ver, voy a enviarlo hasta el fondo no más. Y ahora sí. Y este el último que no es el entorno. Ese es el ¿Qué era? Ajá. El environment, el ambiente. Ya. Este de acá, este ambiente del problema es todo lo que aún En la humanidad no tenemos por concreto conocido, o sea, es el problema real documentado y no documentado está ahí. Ese es el ambiente. El entorno es el problema documentado por la humanidad, ¿ya? Y el objetivo es que tu agente tome este problema documentado por la humanidad y lo implemente en su contexto para cada usuario. Entonces, cuando llegue un usuario va tener cierto contexto, ¿no? Y cuando este usuario interactúe con todo tu agente, lo que va a pasar es de que se va a mover el el usuario. Este va a ser el usuario, se va a mover en su propio contexto. Este contexto, toda esta zona es del usuario, pero en la conversación va a ir saltando, va a ir saltando y así se va a ir moviendo ya mientras hable. ¿Qué pasa cuando sistema o tu agente encuentro un error en estas métricas. Una de dos, o se salió para acá, sí, o sea, está en un entorno documentado. Sí, existe por la humanidad, sí, pero no está en tu solución. Entonces te toca hacer crecer esto. Tomas, ¿ves? Así y lo incrementas. Y así vamos incorporando esos casos. Van a haber algún escenario, si va a ser un poco más menos probable donde un usuario de su propio contexto es saliendo hasta que quede acá y ahí sí nadie sabe qué pasa porque ese es algo que la humanidad todavía no conoce. Entonces, estos escenarios deberían ser los menos probables. La idea es que apoyes tu solución en lo que está humanamente documentado y que en tu solución el usuario pueda moverse en su contexto sin problemas. Y cuando se esté saliendo, tú puedas recoger el feedback que se está saliendo y hacer crecer el contexto. ¿Okay? Para hacer crecer el contexto no es solamente el system prompt, es el system prompt más ¿Quién me da otra idea? ¿Qué otros componentes son parte de este contexto? Según lo que ustedes puedan ya conocer eh y conocen que son parte del funcionamiento de la gente, o sea, si uno es el system prompt, el otro que podría ser como que la metadata de sus herramientas, ¿no? Porque al final, recuerden que eso viaja al LLM. Tenemos que ponernos a pensar en cualquier pieza de texto chiquitita y llega el LLM, no solo es Pro, porque el MCP Server tiene una definición de la herramienta también, ¿recuerdan? Esa definición es la tool y lo mismo va a ir con cualquier otro recurso que tengamos que llega alem. Entonces, al final del contexto es el conjunto de todos estos pedacitos de definiciones propias de la acción de la gente en todo este espacio. Ya esta metáfora espero que haya servido, pero lo que busco más o menos es ejemplificar cómo debemos hacer crecer el contexto del agente para el usuario conforme lo que encontremos en métricas. Entonces, no es solo el system prom, va a ser definiciones en en los argumentos, van a ser cambios en la declaración de de la metadata de las herramientas, pueden ser ejemplos que se le entregue como referencia en System prom. Al final este es un ciclo iterativo de feedback y no solamente apunta al system prom, sino a todo el contexto del agente. valga para el para el contexto del usuario, ¿no? Recuerden que eso es netamente entonces el punto que estábamos viendo justamente acá vamos a continuar era justa este de feedback, ¿no? Entonces ya no queda como una duda. Si sabemos qué es. Es el system prompt es metadata metadata de los tools y al final es cualquier parte de texto. que define el contexto para el agente y para el usuario. ¿Ya? Entonces es todo esto lo que debe pasar por un ciclo de mejora continua, ¿ya? Es es este el ciclo de evaluación al final. Entonces, ¿qué tenemos acá al final? No, después de todos estos garabatos, puntos claves, estos puntos claves que al final podemos definirlos así, ¿no? Definiciones de tareas. Eso es clave. Definir qué tareas tenemos que seguir para iniciar la evaluación. Definiciones de qué va a ser el input, qué va a ser el output. Puedo entregar algunos ejemplos que me fueron bien. La idea sería poder entregar también los que fueron mal para hacer negative prom. Eso podría ser más rico. Hay algunos frameworks que te lo permiten, otros que no tanto. Tener este trazas, ¿no? La la definición de tu trace, cuán grande o pequeño va a ser tu trace. Esa definición del trace es importante para también conocer cuánto llega en contexto al LL má jud o al AI judge. Entonces es trazas o definir las trayectories dice trayectorias de cada según LM aparecen como trayector al final el trace las los el tool calls que se van a hacer si es que son necesarios todo eso debería entrar como traduso se esté equivocando en una herramienta, porque recuerden, podríamos tener ambigüedad dentro de un MCP Server si tenemos herramientas muy populares que son muy comunes y pueden tener hasta la misma definición, podríos tener otro escenario de error allí, ¿no? Y justamente el medir medir con cierto sustento, definir qué métricas me son importantes para mi caso, ¿no? Entonces, definir qué métrica es importante en mi caso me ayuda a este establecer justamente el sendero de éxito, sendero hacia el objetivo. Claro. Si por ejemplo, no, yo tengo herramientas que hacen saldos y movimientos y otro que te dice estado de cuenta y otro que te dice quizás este reporte de movimientos, entender las diferencias en cada API a nivel de metadata en qué momento invocar a cada una va a ser muy importante, ¿no? Entonces, si eso es muy importante para mí, Quizás una métrica de qué porcentaje de precisión tengo al momento de seleccionar una herramienta me es importante. Entonces, si eso me es importante, miren cómo hacer back propagation sin querer. Ya, sin querer. Si eso para mí es importante de las métricas, voy a regresar un paso atrás y decir, \"Oye, en mi traza tengo que poner la invocación a esas herramientas y cómo se ha hecho y cómo la gente decidió invocar ese error. Entonces doy un paso atrás, ¿no? Entonces el agente debe ser capaz de también conocer esas herramientas que son parte de mi contexto. La gente debe tener esa documentación o esa información de qué herramientas voy a usar y qué metadata tienen. Hasta podría estar conectado al mismo MCP Server. Entonces, con eso yo puedo llegar a medir mejores resultados y decir, \"Ah, okay, mi gente se está equivocando porque hay dos herramientas que se parecen mucho, quizás no en nombre, pero sí en la metadata de cada uno de estos. Entonces, de esa forma puedo yo voltear al al dueño del MCP server y decir, \"Oy, compadre, ¿podrías cambiar este métodito? Está golpeando conmigo, ¿no? O no me está funcionando bien. Entonces, de esa forma ya tiene feedback también es ese dueño del MCP Server para actualizar la definición justamente de esos recursos. ¿Ven? Y al final es un un ciclo de feedback continuó para todos los lados, no solamente para tu sistema multiagente, sino para todos los recursos que están alrededor, todo el entorno. Entonces, con esto hemos visto acá el desde el inicio y la motivación y el ciclo. ¿Querías saber si este acá está quedando claro o si tenemos alguna duda, ¿estamos claros? Sí. Okay. Okay. Ya. Chévere. Entonces, con esto vamos a mecanismos de evaluación. Ya les estaba explicando sin querer los semánticos que son basados en el LM, que es el el que hemos estado viendo, por ejemplo, el del A judge o el L má judge, que es este. Pero también hay otros que les puedo resolver de otra forma, ¿no? Como explicar por reglas, ¿no? Se logró el objetivo. ¿Cómo mido el objetivo? Entregó, envió el correo, digamos, produjo el reporte o falló quizás hasta el API y no se dio el entregable. Se se logró sí o no. O se logró sí, pero no sé la calidad de este. Las herramientas se usaron en orden correcto, ¿sí o no? Igual tengo cosas que como que ya conocidas o preestablecidas que son determinísticas. O sea, yo puedo determinar que si esto no se logra, pues es un fallo. Yo puedo determinar que al final, si es que sigues este paso a paso, vamos ser exitosos o yo puedo determinar si es que el correo se envía con este contenido es un éxito o puedo determinar quizás que el omitir este paso puede llevarme a un error. Entonces, esas reglas a veces ni necesitas el para lograrlas, o sea, para saber y solamente te basta leer el trace y si en el trace lo ves, pues gol, ¿no? Ya, ya tienes ahí tu indicador verdecito que te dice Okay, en base a estas reglas hemos cumplido con esto, ¿no? Y ahí tienes un check. Después hay otras que son basadas eh pues estas que ya les había dicho que es basados en el LM, ¿no? Le das un prompt y el prompt le dices, \"Oye, la tarea es es esto, la tarea más el trace, ya no le hagan el caso este de tractory, es el el trace, el trace completo más la tarea.\" Y le digo, \"Oye, este agente resolvió lo que le dijimos que resuelva, ¿sí o no?\" Y acá el score y aplicación. Mira, del 0 al 100 yo le doy un 70 porque acá no me ha sustentado X Y Z AC. Ah, okay. Ese feedback se lo pondríamos dónde? En el system prom, ¿no? Quizás en el uso de la herramienta, quizás le uso mal la herramienta y le tenemos que dar ese feedback quizás al momento de documentar el uso de la tool de la herramienta, ¿no? Entonces, esto es lo que hemos estado justamente hablando desde el slide anterior. El otro es netamente ya principios humanos. nosotros bípedos, mamíferos que pensamos que un límite es y podemos decir, \"Okay, en base a estos criterios, yo humanamente puedo leer el leer el trace y empezar a dar mi escala del uno al cinco cuan bien o cu mal, ¿no? Tenemos nosotros quizás cierta capacidad, ¿sí?, para la ideación de nuevos potenciales criterios. Eso sí es importante que como humanos podemos probarlos.\nPor ejemplo, no, yo tengo herramientas que hacen saldos y movimientos y otro que te dice estado de cuenta y otro que te dice quizás este reporte de movimientos. Entender las diferencias en cada API a nivel de metadata en qué momento invocar a cada una va a ser muy importante, ¿no? Entonces, si eso es muy importante para mí, quizás una métrica de qué porcentaje de precisión tengo al momento de seleccionar una herramienta me es importante. Entonces, si eso me es importante, miren cómo hacer back propagation sin querer, ya sin querer. Si eso para mí es importante de las métricas, voy a regresar un paso atrás y decir, \"Oye, en mi traza tengo que poner la invocación a esas herramientas y cómo se ha hecho y cómo el agente decidió invocar ese error. Entonces, doy un paso atrás, ¿no? Entonces el agente debe ser capaz de también conocer esas herramientas que son parte de mi contexto. La gente debe tener esa documentación o esa información de qué herramientas voy a usar y qué metadata tienen. Hasta podría estar conectado al mismo MCP Server. Entonces, con eso yo puedo llegar a medir mejores resultados y decir, \"Ah, okay, mi gente se está equivocando porque hay dos herramientas que se parecen mucho, quizás no en nombre, pero sí en la metadata de cada uno. Entonces, de esa forma puedo yo voltear al al dueño del MCP server y decir, \"Oy, compadre, ¿podrías cambiar este métodito? Está golpeando conmigo, ¿no? O no me está funcionando bien. Entonces, de esa forma ya tiene feedback también es ese dueño del MCP Server para actualizar la definición justamente de esos recursos. ¿Ven? Y al final es un ciclo de feedback continuó para todos los lados, no solamente para tu sistema multigente. sino para todos los recursos que están alrededor, todo el entorno. Entonces, con esto hemos visto acá el desde el inicio y la motivación y el ciclo. ¿Querías saber si este acá está quedando claro o si tenemos alguna duda. Estamos claros. Sí. Okay. Okay. Ya. Chévere. Entonces, con esto vamos a mecanismos de evaluación. Ya les estaba explicando sin querer los semánticos que son basados en LM, que es el que hemos estado viendo, por ejemplo, el del A judge o el L má judge, que es este, pero también hay otros que les puedo resolver de otra forma, ¿no? Como explicar por reglas, ¿no? Se logró el objetivo, ¿cómo mide el objetivo? Entregó, envió el correo, digamos, produjo el reporte o falló quizás hasta el API y no se dio el entregable se logró sí o no se logró sí, pero no sé la calidad de este. Las herramientas se usaron en orden correcto, ¿sí o no? Igual tengo cosas que como que ya conocidas o preestablecidas que son determinísticas. O sea, yo puedo determinar que si esto no se logra, pues es un fallo. Yo puedo determinar que al final si es que sigues este paso a paso vamos a ser exitosos. O yo puedo determinar si es que el correo se envía con este tenido es un éxito o puedo determinar quizás que el omitir este paso puede llevarme a un error. Entonces, esas reglas a veces ni necesitas el el para lograrlas, o sea, para saber y solamente te basta leer el trace y si en el trace lo ves, pues gol, ¿no? Ya ya tienes ahí tu indicador verdecito que te dice, \"Okay, en base a estas reglas hemos cumplido con esto, ¿no?\" Y ahí tienes un check. Después hay otras que son basadas eh pues estas que ya les había dicho que es basados en el LM, ¿no? Le das un prompt y el prompt le dices, \"Oye, la tarea es es esto, la tarea más el trace, ya no le caso este de tractory, es el el trace, el trace completo más la tarea.\" Y le digo, \"Oye, este agente resolvió lo que le dijimos que resuelva, ¿sí o no?\" Y acá el score y la explicación. Mira, del 0 al 100 yo le doy un 70 porque acá no me ha sustentado X Y Z A C. Ah. Okay, ese feedback se lo pondríamos dónde? En el system prom, ¿no? Quizás en el uso de la herramienta, quizás le uso mal la herramienta y le tenemos que dar ese feedback quizás al momento de documentar el uso de la tool de la herramienta, ¿no? Entonces, esto es lo que hemos estado justamente hablando desde el slide anterior. El otro es netamente ya principios humanos, nosotros bípedos, mamíferos que pensamos que un límite es Y podemos decir, \"Okay, en base a estos criterios, yo humanamente puedo leer el leer el trace y empezar a dar mi escala del uno al CO cuando bien o cu mal, ¿no? Tenemos nosotros quizás cierta capacidad, ¿sí?, para la ideación de nuevos potenciales criterios, eso sí es importante, que como humanos podemos probarlos, establecer este de qué forma se da, lo podemos probar en escritorio, podemos hacer esta prueba de escritorio, ¿no? De agarrar datasets o trace, varios traces que pueden estar en un log y empezar a agarrar una muestra y ver, oye, en base a este criterio y el el criterio significa esto, cu bien o cu mal ha hecho esto, ¿no? del un y con eso poder establecer si es que oye, este es un buen criterio evaluar, ¿sí o no? Luego sobre eso podemos estar iterando y decir, \"Oye, me ha ido funcionando bastante bien esto en papel y lápiz, digamos, o de forma manual.\" Y esto luego lo puedo subir hacia un semántico, ¿no? Y podría decir, \"Oye, ya que esto me funciona bien este con todo el equipo, ¿por qué no lo automatizo?\" Y este es un sendero de automatización para cualquier escenario que incluso se vean. He visto que hay varios roles que son de quality analyst o quality engineer. Este, ustedes podrían potenciar su trabajo este por LLMs y todo lo que puedan este basar desde logs o incluso con ciertas acciones que podrían ser quizás apicadas, las podrían ir automatizando de esa forma. Entonces, todo lo que era automated testing tiene un new kit on the block, que es este los LLMs, es el nuevo kit on the block en todos los automated testings. Ya. Entonces, sepan que esa es esa alternativa nace. Ahora, el otro punto es la simulación ambiente controlado y es justo lo que les estaba explicando con esas esferitas. Entonces, yo puedo crearme el ambiente ideal con usuarios ideales, toda la data cocinada y luego va a funcionar y va a funcionar genial. Pero también cocinar comida fea es importante para saber que no le puedes digerir. Entonces, e el crear esos escenarios también es un buen caso. de prueba. Entonces, la simulación en un ambiente controlado es esta cosa. Mírenlo, como esta olla donde pueden salir tanto cosas buenas como malas, como la olla de la brujita. La olla de la brujita me puede decir, acá hay un escenario bonito, como hay un escenario feo. Entonces, e acá yo puedo establecer cierto ambiente en base a, okay, tenemos este set de clientes, cada cliente va a tener su propia información y ese es el contexto del cliente donde en la conversación va ir saltando, no ir saltando en la conversación en su propio contexto y le va a decir, \"Oye, mira, yo tengo un caso medio raro porque tengo dos DNIs.\" ¿Qué? ¿Cómo que tienes dos DNI? Sí, tengo dos DNI porque en un momento, no sé, eh, dejé mi nacionalidad peruana y luego me volví de otra nacionalidad y luego la quise recuperar, ¿no? Algo así se me ocurre, pero puede darse. Entonces, esos escenarios medios raros, esos escenarios que no son los fácilmente digeribles, los podemos establecer. quizás en una simulación, un ambiente controlado en desarrollo, en testing o en certificación, no sé cómo le llamada empresa, se puede crear este ambiente simulado, ¿no? Y llevar a la gente a ese ambiente simulado y recoger ese feedback, ¿no? Estamos teniendo, esto puede pasar, ¿no? Podemos tener feedback de producción de usuarios quejándose de algo y que no lo encontramos en los logs y que no lo resolvemos en el system prompt. y que no lo resolvemos en la metadata. ¿Dónde creen que está el problema? A, ¿dónde está el problema? No es en el MCP Server, no es en la metadata de herramientas, no es en el system. Y sigue y sigue y sigue. ¿Dónde está? Claro, en el ambiente no documentado o en el documentado que no se está procesando en el contexto justamente. ¿Y cuál es ese contexto? Es la data. del cliente. La data del cliente es la que regresa del API. Entonces, yo puedo tener una data cliente y como saben hay data y data y si es data tal cual es garbage in, garbage out, por no decir otra cosa. Así que al final recordemos que si eso pasa dentro del mismo agente pues va a fallar. La gente se rasca la cabeza y dice, \"Pero, ¿por qué hemos cambiado el system prom? Hemos cambiado la metadata de los APIs, hemos cambiado incluso la forma en el cual estamos recogiendo. Tenemos un AI Judge online, estamos quemando miles de toques, pero no encontramos el por qué. El por qué puede ser en la data del cliente, en el contexto, como dice Renson, o podría ser el ambiente no documentado, como dice César, que ese sí es un caso más difícil de Entonces recuerden que al final es el entorno lo que tenemos que buscar, ¿no? O sea, el contexto del cliente si va a estar con su data. Vamos a tener este al system prom, vamos a tener la parte de la metadata de las herramientas, pero recuerden todo lo que toca el LLM no es solamente donde damos instrucciones, sino también es la carnecita, pues la data en sí del cliente. Entonces es por eso que se hacen estos ambientes simulados, controlados para poder cocinar un cliente igualito al de producción que no sabemos qué pasa y tenemos que saber qué pasa, ¿no? cocinamos ese usuario en la cocina de desarrollo y lo mandamos a probar y si da todo este luz verde, ahí sí estamos en problemas porque quiere decir otra que es otra cosa más y tenemos que ponernos a pensar qué de diferente tenemos, ¿no? Es ir descubriendo, como les decía, ir viendo cómo está saltando esa burbujita conversacional en el contexto, ¿no? Es es así como lo tenemos que ver y para eso se toma este esta decisión del ambiente controlado. Bien. Entonces, con estos al final, ¿qué vamos a lograr? Pues vamos a lograr ciertas ventajas y y desventajas cada uno, perdón. Vamos a tener la ventaja más rápid va a ser que más rápido. Obviamente van a ser reglas, es super reproducible, no van a haber cesos porque son reglas y no va a capturar la calidad, eso sí, no me va a decir este métricas de calidad que podrían ser como que no muy discriminables, digamos, bajo ciertas reglas. es un poco su limitación. En cambio, con los LLMs sí puedo llegar a entender el contexto, el razonamiento y al final puedo llegar a obtener estas métricas de calidad que son un poco abstractas, pero vamos a depender igual del costo de los tokens, vamos a quemar tokens, va a haber igual un costo computacional asociado por el LLM judge o el agent agent judge, a judge también lo llaman. Entonces estos al final nos van a generar a un costo adicional, pues no, pero vamos a tener esta percepción de calidad. Felizmente basados en humanos, pues vamos a tener obviamente los límites humanos, que va a ser la velocidad, eh el costo que vamos a tener que invertir, quizás en un equipo que pruebe manualmente y de que va a haber variables entre las personas que que ejecuten, porque obviamente todos somos humanos y no vamos a tener la misma garantía de ejecución porque todos somos muy diferentes, ¿no? O sea, pero con esto vamos a poder superar un poco al LLM como humanos, superando al LLM en detectar estas sutilezas que, oye, quizás es pensamos un poco ahora of the box y nos damos cuenta es la data del cliente, no es no es algo del LLM, es algo de la gente, ¿no? Y podemos entender el ground tr de una mejor manera, ¿no? Si es que nos acercamos hacia la misma documentación, podemos interpretarlo de alguna otra forma que quizás al el todavía le le cueste, ¿no? Eso es lo que podemos lograr como humanos. Y este en relación justamente al al entorno, al ambiente, va a haber obviamente este cierta brecha entre la simulación y la realidad, este ambiente de sandbox y obviamente al tener ese ambiente de sandbox puede tener un coste pero que no sea muy alto, pero también hay un costo que a considerar este costo del de la cocinita que estábamos hablando. Y lo que sí va a ser este ventajoso va a ser que vamos a poder tomar este caso de producción que es medio loco, que no sabemos qué pasa, lo vamos a poder reproducir en desarrollo y luego darnos cuenta, ah, esto era, y luego llegar a escalarlo esto hacia producción, ¿no? Y lo bueno es que vamos a poder luego automatizar ese mismo mecanismo en el espacio de desarrollo y jalar, así a cualquier otro caso de una forma escalable. Esto esto es al final el objetivo de lograr este ente simulado, no controlado donde pueda llevar esos casos. Okay, pues con todo esto, ya que hemos visto los fundamentos, podríamos decir, vamos a pasar a ver cómo lo ve Lmith. Langmith plantea justamente acá vamos a entrar primero a la documentación del LMI. Esan porfa cuando puedan ver mi pantalla. ¿Están viendo toda mi pantalla? No, ahora no. Ahora Si deberían inverto mi pantalla. Si ven tod mi pantalla.\nSí.\nYa. Okay. Lmith Evaluation tiene dos formas, una que es este before you ship y el otro para monitoreo. En otras palabras, tu preproducción y producción. Ya, así lo hace. Entonces, tú puedes ser offline evaluation como tipo laboratorio, por eso el icono de laboratorio, ¿ya? Y el otro es el online evaluation, que obviamente es lo que hemos estado viendo en los traces cuando se ejecutaban, ¿no? Ahí yo también le podría haber puesto una métrica de evaluación. Entonces, acá te dice más o menos cómo va el workflow. Los evaluadores que permiten LMIT son un human review, o sea, que tú entres a checar, le puedes poner reglas en la forma de código, puedes poner un ll a judge o una comparación con este caso positivo que les decía, ¿no? Y lo corres como un experimento y analizas el resultado al fin de Eso es netamente lo que te permite el LMI. Es decir, es que haces el offline. Si haces el online, pues obviamente vas a tener ciertos colectores que van a correr cuando haces el la instanciación en tu proyecto de un LMI key luego le pones tu nombre de proyecto SET, te crea un proyecto dentro de Lmith y allí puedes hacer todo el monitoreo de tu aplicación y también le puedes agregar estos online evaluators, ¿no?, para que se ejecuten automáticamente para hacer safety check. Format validation qualitys. Y bueno, todo lo que te dice del LLM, o sea, judge, además de aplicar ciertos filtros o sacar ciertos muestreos para monitorear el costo. En ese sentido, el monitoreo en real time se van a dar en los runs o los threads, que al final es el trace que estábamos hablando, y se puede establecer un feedback loop a manera de un dataset, ¿no? O sea, tú podrías tener estos traces de tu espacio de producción y lo llevar estos como posibles fixes para luego hacer un redeploy. Eso es al final todo lo que esté dando con estos dos eh mecanismos. Entonces, con esto dicho, tu pipeline sería de que de tu aplicación. Un momentito, por favor, disculpen. Okay. Okay. Entonces, como les estaba diciendo, al final lo que buscamos es este tener un pipeline, que es más o menos lo que se muestra acá. es de la aplicación obtener cierto dataset y de este dataset luego implementar ciertos evaluators que vendrían a ser los judges o los LL+ máj en este caso para y en base a eso ejecutar un obtener un resultado que luego me lleve a visualizar los resultados a manera de un dashboard. Esto lo puedo hacer sí lo puedes hacer lo puedo hacer en un notebook de Python, en un colab también lo puedes hacer, pero lo puedes hacer con Lmit, también se puede hacer. Pero, ¿cuál es el tema? El colector de data está asociado al LMI. Entonces, siempre es eh si ya lo vemos a nivel productivo, empresarial con los cloud vendors, ¿no? Los cloud vendors van a buscar que toda la telemetría esté en sus espacios. Todos sus espacios pueden ser o application para Microsoft. o Cloud Watch para AWS. Me parece que Google tien un monitor nada más así, pero todos estos van a tener una función que es recolección de información. ¿Cuál es el estándar para recolección de información ahora a nivel empresarial? Es Open Telemetry. Entonces, Open Telemetry establece un formato estándar, digamos, open justamente para poder en cambiar las comunicaciones de logs y telemetría eh a nivel de cross platform, porque tú puedes usar diferentes productos, puedes usar Grafana, puedes usar Diner Race, puedes usar do lo que necesites en formato de Open Telemetry que te sirva para poder saltar cuando necesites de plataforma. Tu equipo de seguridad utiliza Dynxan con tu equipo de primera respuesta ante incidentes utiliza grafana Sí, entonces el foco a nivel de la industria que se ve es de que estos colectores de información soporten Open Telemetry y que sean una comunicación por stream, porque esta comunicación por stream me ayuda justamente a evitar de que se me detenga toda la transacción porque estoy esperando el request hacia, digamos, el colector propio de Microsoft o de AWS. Entonces, buscar open telemetry en cualquier colector es un master LMI si lo está soportando y también soporta stream. Entonces en este caso es LMI. Podría yo apoyarme de los de Microsoft, de Amazon o Google. También puede ser. Necesitaría un SDK distinto. No necesariamente, a menos de que quieras tú customizar ciertas métricas. Hay hay ciertos que te vienen de cajada por cada vendor. Hay otros que puedes hacer como un mix, digamos, entre LMTH más métricas de Microsoft más otras que pueden ser, por ejemplo, de Confidente AI y todos estos ponerlos como métricas de resultados ejecutados por LLMs, ¿no? Estas pueden ser también reglas que se pueden preestablecer, o sea, ciertos paquetes de reglas para evaluarlas que tiene cada vendor. Así como hemos visto en el apartado anterior, podemos tener vendors que teen basados en reglas que te con el de el semántico también e incluso hay unos que te hacen un ambiente simulado que ahora los llaman playground. ¿Ya? Entonces estos de acá se pueden manejar de esas formas. Entonces, ya que hemos visto estos de acá, vamos a pasar a entender cómo funciona. Les he dicho, hay un datet, puede ser manualmente curado, puede ser obtenido de L o puede ser este netamente generado, sintético. Yo me puedo generar LM ejemplos sintéticos ponerlos. y escoger un evaluador, ¿no? Para eso van a haber este ciertas decisiones que pueden estar hardcoded, podemos tener un ground trot con el cual comparar, podemos tener alguna referencia o un criterio que se les dé como ejemplos positivos y vamos a tener los LLS adjudo hacia modelos de comparación también. Entonces, todos estos se engranan luego en la forma de unit testing, la forma del judge feedback dentro del evaluation o podría hacerlo luego como testing. Y ahora lo vamos a ver en su plataforma. Entonces, ahora vamos a estar explorando dentro de LMIT Evaluation primero en su interfaz gráfica. Ah, me dicen si lo ve muy chiquito. Si lo ven, estoy agarrando un poquito ahí el tamaño de la letra, pero que ¿dónde estoy? Estoy dentro del playground. Dentro de playground de podría entrar dentro de evaluation. Ahorita estoy en playground de evaluation. Entonces, dentro de evaluation hay dos tabs. Uno que es annotation que son como unas colas donde yo puedo guardar anotaciones que luego me sirven como fuente de información para los experimentos datas y este de playground. En el playground que viene a ser como este sbox donde les decía que se pueden establecer ciertos puntos de evaluación, qué es lo que se ha hecho. He creado un agente que le he llamado Perú Legal Agent. Este promp lo he guardado dentro de mi bol de promp. Si ven acá ya dice cinco. No es solamente este de acá, sino lo que lo he puesto como ejemplos acá abajo hay algunos ejemplos que ya he puesto con alguna salida. ¿Cuál es el objetivo de este agente? Es un agente experto en leyes dedicado a componer artículos de casos legales. Va a sustentar su respuesta en base al contexto y va a ser conciso. Le estoy pidiendo este tipo de reportes donde vamos a tener un título, los hechos del legales. Acá es un histórico de procesos y un fallo del juez y un razonamiento para que entienda qué rol está cumpliendo para el juez. Entonces, como le está ayudando al juez a poder redactar también las cosas y este el torno, ¿no?, que debe mantener un tono de redacción legal y en el caso de un contexto de Perú, Latinoamérica debe considerar un contexto regional. Entonces, las leyes aplicables al Perú al 2025 y acá le he puesto como varia Gim en question, nada más podría enriquecerlo más si quisiera acá. Como ven, se le puede agregar tool o output esquema para hacer un formar el par que era formar para darle formato a la salida. Acá el ponerle un mensaje más es que yo quisiera, ¿no? Dentro del message para humano o del AI. Entonces, lo que he hecho acá en esta parte acá abajo es agregar ejemplos, ¿no? Un grupo de funcionarios públicos son arrestados de corrupción, bla bla bla bla. Y acá un output del formato, ¿no?, que vendría a ser Este, netamente este input, un grupo de funcionarios públicos son arrestados acusado corrupción un caso. Acá ejemplo y acá el resultado del reporter de caso, todo esto hacía sintético. Un ejemplo de reporte, ¿no? Y acá este de acá abajo es la ejecución. Bueno, esto ya lo ejecuté, pero también les voy a mostrar una ejecución ahorita. Y si ven acá se demoró 55 segundos en componer este caso. Es un es un artículo lo cual Quiere decir que no es igualad, es un texto considerable. Y miren, acá le estuve dando evaluación de correctness, o sea, para que valide si fue correcta o no la respuesta y un grado de alucinación. Estos dos son netamente de caja y acá me dice cuántos este tokens me consumí más o menos el costo del dólar y este cómo podría reducir lo que me decía que podía reducir quizás un poquito le podría reducir en costos. No, en base a una optimización del ROM. Entonces, ese es más o menos este caso que ya se ejecutó. ¿Cómo lo mandé a ejecutar? O sea, si ustedes llegan a este punto, este, van a llegar primero a definir primero el system prom. Después de llegar al system, van a llegar a esta parte de entregar un input y un output de referencia, que es el caso positivo que les estaba indicando. Entonces, le dan el input y luego el output de referencia y Después tienen que guardar esto como si fuera un dataset. En ese caso yo le puse dataset legal de de Perú sample nada más que esto al final queda acá como dataset. Okay. Luego de esto, este pueden agregar más filas. Save. Después este save se guarda como datas set y le puest mi datas set acá y acá entran los evaluators. Entonces estos son los evaluators que me brinda Langpit. En este caso. He puesto el de correctness y el de hallucination, pero también hay otros, ¿no? Está el de cuán conciso va a ser y si es que estoy generando código el de code checker. Estos son como judge, ela judge, ¿no? De code checker y de de conciseness que vendría a ser con conciso es. Y si quiero hacer un composite evaluator que es combinar múltiples evaluadores entre para lograr un promedio de score, también lo puedo hacer. Entonces, como que tengamos de nuevo, ¿no? Nuestro consenso entre múltiples agentes. Entonces tenemos a este composite score que evaluator que va a ser la de Patricio Estrella dando el veredicto. Entonces es este el composat evaluator y tenemos uno para código dedicado a Python y uno desde cero, ¿no? Que yo ya le tendría que escribir un prompt y escribir este algo desde cero como mecanismo de evaluación. Este es, en otras palabras, tus criterios se lo escribes al Llajud para que que los evalúe dentro de la salida de tu ejecución del agente. Entonces, para más o menos asegurarme de un criterio más, voy a agregar el de consignis. Cuando lo agrego, miren que acá me está jalando el Open AI GPT 5 mini. Ustedes me va a decir, \"Oye, pero ¿de dónde sacó L Smith un Open AI?\" Te pide que le pongas un key de Open AI para que pueda ejecutar y LMI dice, \"Sí, mira, déjanos tu ke nosotros le vamos a mantener. tener seguros y etc. Este ustedes pónganle ahí un key para poderlo probar. Este, si gustan pueden dedicar uno aparte para también poderlo manejar de una forma centralizada de que este es mi key que se está yendo almit. Okay. Y acá si ven este es un poco la definición del prompt para el evaluador. Yo podría adecuarlo si quisiese, ¿no? Pero miren más o menos cómo está, ¿no? Pero es un experto en etiquetado de data evaluando la salida de modelos en relación a La consistencia, ¿no? Tu tu tarea es asignar un score basado en la siguiente rúbrica y acá define la rúbrica de de qué ser conciso. Bueno, acá quizás está mejor definido que de la forma que yo lo pudiese definir, pero acá puedo agregármele más cosas si es que deseo. Y acá simplemente me da una variable que es el input y esto es el output. Si se dan cuenta, lo está manejando como si fueran tags, que era justamente lo que estábamos hablando al inicio, ¿no? Ahora yo podría entregar cierto verbose en la salida si es que quisiera, ¿no? Y acá me está dando este otras variables que me pueden ayudar a un poco potenciar la salida del feedback. Y acá lo que estoy dando es nada más un response format del 0 al 10, podría ser el 100, podría cambiar y estoy incluyendo el reasoning como parte de la salida. Acá el strict mode, esto es solo para modelos de openi. Va a obligar a las la salida del modelo. para hacer parte del mismo esquema de, o sea, de toda la salida. El output que se va a generar de Lmit debería tener una parte que es el open AI output tal cual y ponerlo, ¿no? Y esto es shot corrections para ver si es que podemos hacer ejemplos F shot para corregirlo. Entonces es un poco la configuración de este evaluador. Le vamos a dar save y lo que está haciendo, van a ver acá, create a new commit for prompt, que quiere decir que todos tus commits dentro del LMIT tiene un repositorio git. Entonces, tu project al final del LMIT lo puedes asociar a un repo git y pues lo versionas tú. Entonces aquí, por ejemplo, estamos viendo ya el la configuración ya dice tres evaluadores, lo que aparece el de alucinación ya ejecutado, el de correctness en este caso dio cero. Miren. Y acá con siziness dice no feedback. Entonces dice no feedback porque me falta darle un ejecutar. Ahora para ejecutar me me establece dos mecanismos, uno es por medio de streaming, el otro pues va a ser voy a tener que esperar el response este como si fuera un request regular y en la cantidad de repeticiones que yo quiero ejecutar. Si mando todo esto, la verdad que me causa en crédito, le he mandado una para ver el ejemplo y le pudiese poner streaming para ir viendo progresivamente cómo va saliendo la respuesta, pero ha sido bastante rápido. Por ejemplo, uno demoró 55 segundos, los otros en ejecutar deberían demorar igual, así que más un promedio de 3 minutos en cada uno de estos, un perdón en total de estos. Yo le voy a dar start y pues los dejaremos ser. Esperamos ahí un rato mientras termina la ejecución y vamos a volver a esto. Ahora también les estaba diciendo que LMI evaluation se puede llamar por código. Así que acá tenemos este varios eh tengo un Lraph agent acá. Sí. team muy creativo yo. Entonces este team al final lo que hace es una evaluación eh en relación, perdón, es una composición que les decía entre supervisor, redactor y editor trabajando juntos para crear artículos de IA. Y en este punto se le está dando instrucciones de cómo componer de nuevo un artículo, pero en este caso van a ser sobre contenido. Esa manera de que sea una revista de inteligencia artificial, pero con corte cómico. Entonces va a tratar de de abordarte temas un poco este de cómo la sociedad está tomando la inteligencia artificial en el primer mundo y cómo reaccionan cada uno de ellos. Entonces, eso es lo que están este buscando acá, editar. Entonces, la salida de una ejecución de este se llama artículo editorial y van a verlo acá. Esta es la salida de la de los multiagentes del Rp. Aparece la fase de reducción, de edición, la de supervisión y acá está el artículo. Alerta de la invasión de inteligencia artificial. su golpe estado en el primer mundo. Í pone hablar un poco de la invasión amistosa de la inteligencia artificial. Debemos preocuparnos y dice no le detalle en la oficina humanos versus chat. El baby boom de las startups de IA. Si pensaba se refiere al Star Wars tecnológicas que había tocado techo. La novedad startup de IA cada 5 minutos. Parpadeo anuncia el nacimiento de nueva expresa que promete el próximo año y así no. Entonces al final da una conclusión y y el feedback del editor, ¿no? Eso es lo que han elaborado acá los agentes de Lara. Entonces, para eso dentro del LMI, LMI, perdón, con LMI Evaluator. Acá hemos establecido este algunos criterios, ¿no? De de evaluación. Primero es cómo crear, ¿no?, establecer el proyecto con LMI, tal cual, lo crear un client, el client lo al momento que hacen lo DNB va a cargar el key de su LM. Project LMIT Space. También acá vamos a hacer chat Open AI para 4 y este luego vamos a extraer el markdown del del artículo con este método. Luego los evaluadores personalizados. Acá estamos haciendo evaluadores personalizados con un formato que es esto. Está haba uno que era propio de GAL, pero Bueno, acá acá les digo más o menos dónde buscaríamos hacer offline, que es este de acá, es del que estamos haciendo ahorita, que es offline en código, pero esto luego le podrían deployar online este LMIT Project para hacerlo también como un paso de cada gente. Okay, ese era lo que un poco explicarles, online y offline para dentro de Entonces este evaluador lo que está buscando es un criterio netamente de este cuántos caracteres tiene mínimo 500. Y esto es algo que bien podría ser evaluable como regla por cada parte del del es una regla, digamos, de longitud, pero acá por ejemplo estamos viendo si promueve buenas costumbres sin ser ofensivo, ¿no? Es un poco diferente. Entonces tenemos que prohibir ciertas palabras, discriminación, racismo, si es este sexista, si tiene citación, odio, la violencia extrema, ese tipo de cosas, ¿no? Entonces, lo que hacemos acá es netamente buscar de nuevo si es que hay eh algo de esto dentro del contenido, si tal cual aparezca como una regla. Y lo mismo en cantidad de párrafos, algo tan simple como ver la cantidad. Y en este caso es un evaluador de relevancia. La relevancia en este caso vamos a hacerla netamente por palabras clave y de nuevo por reglas, ¿no? Y en la connotación de cómico. Acá sí estamos haciendo un inboke al Lasa Judge para verificar verificar si tiene un tono cómico o no o si mantiene un tono distinto al cómico e inteligente. Entonces acá si se hace un invoke al LM para darle un rol y decirle analizar el tono cómico del artículo. Responde con un número de cer al 10, ¿no? Sin decimales y donde cero es nada cómico y 10 es extremadamente cómico. y ahí le entrega justamente el artículo y tenemos un response y ya tenemos ahí todas las métricas. Acá crea el un dataset este buscando justamente de referencia el artículo, un dataset asociado hacia el mismo artículo y lo empieza a agregar dentro de su lista de ejemplos de artículos para luego poderlo sacar. Esto es si es que tuviéramos más de un artículo generado, ¿ya? Y lo podría yo sacar como varios para generar como que referencia. Y acá este el método para guardar el reporte como markdown. Okay, con esto al final ya nos toca netamente invocar. Entonces vamos a ver el de Lmith. No recuerdo cuál era este. Creo que era el de Lmith. Sí, este es. Entonces este de LMI vamos aarlo y tocar. Demoro un poco, pero les voy a ir mostrando lo que ya ejecutó. Entonces acá ejecutó los evaluadores y acá la longitud mínima dijo el artículo tiene 3400, un mínimo era 500. El contenido apropiado sí es apropiado y sano, contiene 12. Ahora, esto de del tipo de contenido apropiado y sano, de que tenga ciertas otras métricas que son reglas en este caso, bien, podrían ser una invocación al LLM. También es cuestión de cómo definen las métricas. La estructura de párrafos tiene 12 párrafos de ser mayor o igual a cuatro. Estamos okay. Relevancia del tema igual ocho términos clave por términos. Ya. Ahora, la relevancia podrían ser eh de nuevo, ¿no? Invocación al Llm diciendo para mí relevante es que diga X y Z o relevante es definición de relevante y el tono cómico le ha dado 7 de 10, digamos en cuico. Entonces al final todo es el resumen de elevación de le da una puntuación promedio al final que eran esos porcentajes que les decía. Bien, y ese es el escenario del LMIT. Antes de yo quitarme el siguiente stage, es el de el siguiente paso es el reporte de Lmith, que lo podemos también ver acá, que es lo mismo que hemos visto, pero en marcado todo lo que hemos visto. Ahora, el punto equivalente al del LMI lo vamos a ver en la siguiente mitad de la sesión, ya porque creo que ya nos estamos pasando un poquito del tiempo. Mi objetivo es que de este punto ustedes lo puedan aterrizar a su proyecto. ¿Cómo? ¿Cuáles son las métricas que van a usar en su proyecto? ¿Cómo van a asegurar el éxito de su proyecto? ¿Qué deberían medir? ¿Qué deberían asegurar se esté dando para lograr un agente o multiagente o workflow agéntico que entregue lo que dice que va a entregar? ¿Cómo me aseguro de esto no preguntarnos esto. Ahora, ¿cómo podemos una vez habiendo aterrizado eso, qué métricas? ¿Cómo podemos implementarlas? No, vamos a hacer un offline más un online, es offline y este es este sería el off y este sería el online. ¿Cuántos de esos vamos a tener? Vamos a tener online dentro del offline y el online. Es ¿Cuántos van a ser customs? ¿Cuáles son las métricas de caja que podríamos considerar? No. Entonces, de las que nos da Lmith, hemos visto algunas que están acá dentro de los evaluadores, pero también podemos construir otras, ¿no? ¿Qué otros composite podríamos crear? O una evaluación desde cero, como ese grado de comicidad, o sea, con cómico era, lo podemos poner acá. Entonces, ese tipo de de análisis es lo que tenemos que hacer ahora. con nuestros proyectos, darnos cuenta cómo me aseguro que esto se logre, ¿no? Entonces, no sé si estamos en grupos ahorita. Ahí me ayudan, porfa.\nSí, sí están en grupos, boris.\nYa, sí, en vez de de 20 minutos dada la verdad, si nos damos 10 minutitos, porfa. 10 minutitos o 15. 15 minutos. Al menos logremos definir el qué queremos, ya el qué queremos con las métricas, o sea, qué métricas podemos ya decir que oye, esto es algo que necesitamos.\nYa, ya los envió, ¿no? Boris.\nSí, sí, sí, porfa. Igual yo les voy a enviar una captura de la pantalla para que también puedan saber qué estamos haciendo. Ya le enviaste, ¿eh? Ahí está.\nSí, sí, sí, sí. Lo envió toditos. Entonces, 15 minutos, muchachos, a las 9:10. Arles. Bueno, de la sesión tuve un estuve dando unas vueltas ahí por varias salitas a pedido de un equipo. Este, vamos a hacer una revisión rapidita. Voy a meterlo en la o sea, no es parte del curso, creo que, o sea, han visto igual los temas. que nos toca. Todo este curso es bien enfocado en el stack L chain, L graph, LMI. Ya es ese el foco. Me me hicieron un pedido para ver un poco el A foundry. Este, yo voy a ver una forma por la cual meta esos minutitos en un lado para revisar el AI Foundry. A pedido a pedido de de este grupo, porque ahí también hay métricas que vienen de caja. Hay un SDK viene de caja, es que tiene métricas bastante interesantes. Existen unas formas de crear agentes que atacan a tu agente, ¿no? Te sacan métricas de vulnerabilidades también. Entonces es bastante interesante eso para que se lo lleven de cara a un despliegue enterprise, ¿no? No va a ser un curso entero, digamos, de cómo hacer arquitectura de LLM applications o de agentes o lo que sea, pero va a darles como que un glimpse, como que una pequeño guiño, una una un vistazo rápido de cómo poderlo implementar en esos escenarios es super rápido, es superfácil y ahí lo vamos a poder ver. Por otro lado, me llevo que es muy importante que chicos, así les hago el recuerdo, que todos estemos en contexto de proyecto. Si falta a alguien, si no está tal, si no está cual, llevemos la batuta de resolver la tarea puesta y luego compartirla con el resto del equipo o al menos irla ideando, ya, porque luego si se han dado cuenta, hay varios escenarios de pasos grupales que si luego las vamos a ver en batch, digamos, al final, pues vamos a demorar bastante, ¿no? Como cualquier proceso por lotes, así que mejor hacerlo por streaming como en como es en este stream. Y este nada, vamos a hacer un pequeño break, van a ser unos 10 minutitos, 10 minutitos de break y volvemos con la segunda mitad de la sesión. Pues volvemos 925, porfa.\nYa, Boris, antes de irnos a break, hay un alumno que me está haciendo unas consultas sobre el proyecto, si te las puede enviar por correo normal, ¿no?\nSí, sí, sí.\nYa, ya lo pasé tu correo.\nYa.\nOkay. Listo. 10 minutos entrado. Okay. Entonces, ya hemos estado viendo cómo lo logar este cierta definición de nuestras métricas para el proyecto. Entonces, con esta definición al final nosotros deberíamos ver cómo implementarlas. Ya vamos a ver ciertos mecanismos que hemos estado ocupando ahí con LMI y ahora vamos a tocar un punto que es externo un poco al espacio de L speed, Lin y todo ese ecosistema que se entrar un poco a un framewor muy conocido que es Deep Eval, que es viene de confident AI. Además vamos a tocar este otro CSP. Estábamos justamente hablando del Aoundry. Vamos a ver cómo lo implementa Microsoft, cómo lo implementa también AWS y justo alrededor de esto hay unos SDK también nuevos que se pueden poner en uso. Así que empezamos con Deep Evil. Deep Evil al final lo que hace es netamente el mismo procedimiento, ¿no? El mismo mecanismo que hemos estado viendo, donde tenemos las este cierto que nosotros le vamos a entregar de asociado de nuestro LLM application, nuestro agent. Vamos a darle de forma opcional cierta cierto trace. Este vendría a ser nuestro trace donde le indicamos cada una de las herramientas que se han invocado, cierto contexto de recuperación opcional y acá este ciertas referencias que también pueden ser opcionales dependiendo del caso, ¿no? Como lo que esperamos que esté de salida, esperamos que se invoque, perdón, lo de salida, cierta invocación. esperada ciertas herramientas. Eso también es importante. Esto no lo teníamos de la Smit. Acá podemos indicar qué herramientas esperamos que se ejecuten y un contexto adicional, ¿no?, que viene a entregar un poquito más del espacio del system prom que era lo que estábamos hablando. Entonces, esa es como funciona Confident AI con el, bueno, el framework de Confident AI llamado Deep Eval. Deep Eval tiene una muy buena herramienta que acá se llama Geval. Acá se le se les presento. Entonces, GAL es justo justo algo muy parecido al compoite que tiene Langsmith, donde uno puede establecer cierto evaluation criteria hacia una métrica propia, ¿no? En este caso coherencia le han puesto, ¿no? Pero puedo ser cualquier otro mecanismo de evaluación que puede ser hasta customizado por, por ejemplo, cuán gracioso es el artículo que estamos escribiendo, este de inteligencia artificial, ¿no? Cuán gracioso el grado de comicidad. que podría haber en el artículo que a la vez lo hace cautivante, ¿no?, que capture la audiencia, pero a la vez lo que hace bastante bien este DPAL es este auto chain of thought. Hace un chain of thought de la ejecución del LL Masa Judaj en manera de steps, una cadena de pensamiento en la ejecución del Llassa Judge. Ahora, podría yo hacer un system prom con chain of thought para mi l masaj de forma manual en la smid también también lo puedo hacer pero obviamente ya depende del grado de expertiz que tengas uno y también es un no brainer o sea no la piensas digamos en en DPAL porque ya te viene con la capacidad de chin entonces ya no te preocupas un poco en resolver esto, ¿no? Entonces eso es lo que hace ahí confident con DPAL aplicando chain of dot para sus eh ll, o sea, judge, igual el input, como hemos estado viendo, el target, el context y bueno, al final de este coherence va a ser como el combosite metric que vimos del langpedit, o sea, va a ser como va a tomar cada uno de los pasos de la evaluación y va a lograr esta curva donde va a decir, \"Oye, el promedio debería caer como que aquí. Todos los que están acá fuera son como que errores y el promedio bien es este o algo así. Es es como que va a buscar esta forma de encontrar la un punto de popularidad donde todos los casos caigan en el punto promedio, ¿no? Es un weited sumore, es un promedio ponderado que empieza a hacer por cada uno de los criterios que uno los ponga dentro de GAL. Entonces, eh un poco este es un review de cómo veríamos el trace. Tenemos acá un single LLM trace, que es el agent spaw, este que está en la gente, el retriever está seguro. obteniendo información de un rag y luego justo va en beding justamente del rag lo envoca al LLM. Entonces todo esto es un Llm call de un old de Shakespeare o de un rag. Es así como lo estaría viendo Confident AI este rag y este es un esto es aplicable a ciertas métricas de test y test case. Entonces todas estas métricas al final se vuelven un único test case que va a tener un retal context y un input. Entonces en base sea esto, si puedo decir, ey, esto de acá del trace es aplicable a una métrica de evaluación, ¿no? Este grupo del trace es aplicable a estas métricas de evaluación. Ya no me enfoco en este single item del trace, sino me enfoco en todo el bloque. Era lo que decimos la otra vez, que podría ser a un single LLM call o podría ser a más de uno. Entonces ahí también hay que definir bien cómo vamos a hacer la invocación para este deep EVAL. Okay. Y con esto Ya nos pasa nada, pasamos al código así de rápido. Entonces, dentro, a ver, voy a compartir ahora toda mi pantalla de vuelta. Okay. ¿Qué vemos primero en la web de DPAL? Para entender, igual les pongo acá la documentación en el chat para que puedan verla también conmigo. Deepal, este, te permite utilizar métricas como las que habíamos visto. Este, ¿qué qué interesante tiene PBAL TPAL forma parte de una plataforma que se llama este Confident AI, o sea, tú podrías generar tu confident AI key, lamentablemente no tiene más que un correo de empresa como mecanismo de prueba para Deep Eval. Así que si quieren eh con su empresa empezar una evaluación, pueden hacerlo. Le dan try DPAL Cloud y ahí pueden utilizar el servicio cloud de DPAL. De lo contrario, solo pueden utilizar el servicio de este del mismo framework de open source de DPAL. El framework es open source, solo que lo que hace DPAL Cloud te permite de nuevo ser el colector de eventos, información y métricas muy al estilo de Lmit. Acá un pequeño summary. Eh, tenemos customrics, o sea, las métricas que vienen propias de caja, digamos, del framework, que es el GAL, justamente este que les dije que es por chain of thought y que puede ser un conglomerado de varias métricas. Tenemos el deep Cyclic draft, que esto sirve específicamente para las de graph rag o las que tengan grafos. Eh, conversational GAL, que es un composite del, o sea, es un compuesto del Gival, pero a nivel conversacional, lo mismo del anterior, a nivel conversacional, uno multimodal de Gbal y uno de arena que va enfocados netamente en benchmarking del LMS. Ya, esto es para los eh los propios de caja que te vienen y los de R, perdón, los propios de caja para tu métrica. De caja te vienen estos para customizar tu métrica, ¿ya? Y estos son los que te recomienda DPAL. Ya, esto es lo gran es lo más valioso de DPAL, que ya vienen específicos, o sea, por cada casito ya te dice qué métricas tocar. Entonces, el caso de retrieval aumentation te dice, \"Oye, acá tenemos estas eh métricas sugeridas, ¿no?, para los RAC. A nivel del retriever es la relevancia del contexto con contextual relevancy, contextual precision y recall como métricas dentro del retriever, momento recuperar información de los racks de generar, no generar este respuestas basadas en una referencia para ver cuán bien estamos generando esa este respuesta en base a los chans o las citaciones que hemos sacado. Fairfulness es asegurando cuán cercana, cuán fiel a la documentación se está eh entregando una respuesta, ¿no? S generator. Y acá está justamente Está este justo lo que le decía, ¿no? Tool correctness, o sea, cuán correcta es la herramienta que hemos seleccionado para determinar este tal acción. Es lo mismo si es que logró completar la tarea o no conforme a la selección de dicha herramienta. Acá hay para los escenarios de múltiples turnos, este es este han visto que el context Windows cambia por modelos, se puede estirar un poco con memoria, pero a la vez hay algunos casos que se apoya del file system, así que que esta métrica de noish retention es bastante valiosa para esos escenarios. Cuánto se sale de su rol, esto pasa también cuando tu cadena de conversación se vuelve tan larga, se va olvidando cosas, se va olvidando cosas tu agente. Entonces, para que se mantenga persistente el rol y no sea luego vulnerable, roll adherance te ayuda a conocer cuán bien o cuán mal está interpretando el rol que les has dicho que haga, ¿no? Y y este cuán completa va a ser la conversación porque van a haber cosas que van a caber y otras que no dentro del context Windows. Lo mismo en relevancia, ¿no? Si te si estoy hablando de algo que fue relevante hace muchísimos tokens atrás, quizás ya no es relevante ahora mismo, ¿no? Para el next token. Entonces estos métricas de caja te ayudan justamente a evaluar eso. No me quiero adelantar la siguiente clase, pero es inevitable. Hay algunas métricas de seguridad, de sesgos, toxicidad, de que no tenga un previo aviso y haga una ejecución sin preguntar mis, o sea, que haya este a abuso o el licage o filtrado, digamos, de información sensible, PI, personal identifiable information y luego un leakage, o sea, que estemos divulgando sin querer datos personales o una violación en el rol o alguien que trató de cambiar el rol a la gente, lo logró y de lo que era un agente que te ayudaba a predecir el clima, ahora empieza a ordenar pizzas. Entonces, eso puede cambiarse también cuando con ciertos mecanismos, por ejemplo, cuando es un multiturm y la conversación está demasiado larga. Microsoft Security también alertó que si tu conversión es muy larga y no has hecho ningún resumen o nada, te pueden cambiar el system prom por medio de este ataque de roll violation. Te cambian el rol de tu agente. si es que la conversión ya es demasiado larga, hay métricas para escenarios multimodales para imagen, text to image, image para igual multimodal, answer relevancy, facefulness, contextual y demás, ¿no? Todos estos son imagen más texto y hay otras, ¿no? De alucinación, cuán correcto es tu Jason, cuán bien hace un resumen o ragas. Ragas, no hemos hablado de ragas. Ragas es un framework que se empezó utilizar cuando recién aparecieron los GPTs. Acá se los dejo también para que lo revisen. Y este framework lo que te da son métricas para poder este medir diferentes escenarios de implementación de LLMs. Podría ser tu mismo LLM si es que estás haciendo fine tuning o podría ser el la implementación de agente que estés haciendo. Puedes evaluar un plompt, puedes evaluar un rack, un workflow o Y ahora un agente, esto no había antes. Y este es, me suena como un agente de Lraft. Miren, es igualito un agente de LF. Y acá es como que lo hagas en tu notebook, ¿no? Hagas examples y acá le pones tu pandas para sacar tu estructura del dataset. Pones tus métricas y acá puedes sacar igual este las métricas basadas en el correctness en este caso, ¿no? Cuán correcto va a ser si es que la predección fue un error o no. Igual lo he hecho por medio de una regla. Acá tu terminas un experimento de ragas y le das una expresión de cómo ejecutar el experimento, ¿no? Y y cómo hacerlo en tu en. Tú puedes hacerlo esto como un prom nada más con Open lo pones como un prom y te dan los ejemplos y acá defines la precisión, ¿no?, de si es que pasa o no cierta métrica. Igual haciendo una ejecución de un prompt, me parecía al caso anterior, pero este apareció en el año 2023. junto a la aparición de Mid Journey, cuando empezaba a aparecer Mid Journey y GPT 4, recién cuando arrancó GPT4 empezaron a aparecer ragas como mecanismos clave de evaluación, ¿no? Acá hay escenas de experimentación, acá están las métricas y este acá están los principios de diseño, incluso para cada uno de estas métricas, ¿no? De retrimentation con precision Acá ya tienen como que la el recetario, ¿no? Para el caso. Eh, con exprcision, el recall, este entities de recuperación, el grado de recuperación que tenemos de contexto de las entidades, el el, o sea, sensitividad al contenido que no es relevante, digamos, al tema y que puede estar en la documentación, ¿no? Y sensitivity y así hay otras métricas, ¿no? Estas son propias de envidia, estas son propias de agentes y esto es ya del de la época de la ciencia de datos, ¿no? language par blue score, el rock y hay unos orientados a SQL o de principio general. Estos las pueden revisar también son métricas por casos de uso, igual se los dejo por acá. Entonces este es un poco el lo que ya tiene de caja este Deep Eval y la forma de implementarlo es bastante fácil. Hacemos un pip install de DPAL y creamos un test case y luego invocamos usar las métricas, ¿no? Answer relevancy en este caso y le vamos a dar evaluate. Entonces creemos un evaluate. La métrica va a ser answer relevancy. El test case va a ser elaborado por whatival y el actual output ahí le podemos decir qué es y nos va a decir el resultado, ¿no? Conforme la evaluación. Entonces este es para crear customrix, como les decía, Gival. Acá está diseñado justamente para crear estos customrics con el chain of thought dentro. Este es Gival normal. Este es el conversacional y este es el multimodal. Ahí lo podemos ver. Acá están las por defectos que decíamos justo T completion y tool. Si lo quieren revisar están acá ya en una manera de ejemplos para poderlos implementar el multimodal y el de safety justamente. Entonces ese es como ya lo podrían implementar. Entonces, en otras palabras es escoger las métricas, muchachos. Lo que hemos estado haciendo en la primera parte de la clase. Escoger métricas. Métricas. que van a ser asociadas genéricamente al caso de uso y unas que van a ser custom, que van a ser diseñadas por ustedes mismo. Entonces, en el en el chat les acabo de pasar como que el recetario de cada caso de uso. Entonces, de cada caso de uso que ustedes tengan, ¿no? Su labor es escoger las métricas genéricas y crear algunas propias del caso. Por ejemplo, tres métricas máximo para su sistema que sean genéricas y dos máximo por caso de uso de sobre eso lo ejecutamos, validamos e Amos y estar este constantemente reevaluando si es que estos métricas son relevantes para el caso de uso o no. Así hemos empezado, ¿no? Primero pensando cuáles son relevantes para el caso de uso o no y luego pasamos a escoger métricas y así esa es la idea, estar en este constante este ciclo de evolución en las métricas también. Y acá tenemos formas en las cuales invocamos a Open AI, a Open AI o Yama o Y o si no un custom ll por medio de Python para netamente jalar el key luego hacer la invocación en tu de los evals, ¿no? Bien, entonces este es un un ejemplo básico de cómo usar. Acá tenemos los ya este es un poco más a fondo dentro de confident si es que lo logran un key quieren implementar dentro de confidenti. Acá tienen lo de elementos que ya hemos visto, ¿no? Eh, acá en este caso lo pone como pairwise comparison o comparando contra otro que está con la respuesta correcta o como un single output y el acá detalla cada uno de estos y cómo comparar en las métricas, ¿no? El de un turno versus multiturno, por ejemplo, ¿no? Entonces esos también se pueden establecer en comparación como se logra luego un veredicto. Y acá está el de Gival que les estaba mostrando. Este es el ejemplo de GAL y el del direct graph. O pueden ser Dep, así graf sería en este caso. Y bien ahí esto lo esto es ya de LLM arena y el Lludes para las métricas. Entonces ahora lo vamos a ver dentro de Visual Studio. En el mismo caso de los agentes que estaban componiendo este artículo, ahora tenemos un evaluador de artículo que está hecho con eh Deepal. El de de Ibal le he puesto más métricas, como ves, un poquito más largo, pero un poco para ver las métricas, ¿eh? es del tono cómico igual. Ya estas son métricas personalizadas de GAL. Torno cómico. Después valores del editorial que promueve buenas costumbres y el uso inclusivo. Eh, métrica de la estructura clara y coherente, contenido relevante y preciso sobre AA relación a criterio, ¿no? En este caso es que esté asociado a estos en relevancia y acá es eh si es activo, atractivo e interesante. situación del cer al 10 igual y estas métricas de alucinación, estos son asociados, todos son gival, si ven, todos son val todas estas métricas son val. Puedes sacar alguna de caja también, pero ahí las he puesto como acá está el mismo método para extraer el marown y en este caso ya va a generar un reporte igual con down. Okay. Entonces, si lo vemos esto en ejecución, vamos a cerrar esto. Este B de acá es este. Este es. Entonces vamos a colapsar esto acá. Y esto es más o menos como ejecutó. Miren toda la evaluación. Esto demoró un montón, por eso no lo vuelvo a ejecutar. Mir todo todo lo lo que empezó a jalar de métricas fue desde acá. Ajá. Desde acá ha tomado el valor de artículo y le he dado el reporte que generó. Base a ese reporte eh sin credenciales de de, ¿cómo se llama? De confident AI. Está generando 11 métricas configuradas. Las ha ido repitiendo este mientras iba avanzando ya en el engagement y atracción y demás y he ejecutado esto de un a 2 minutos y acá hemos obtenido del cer al 100 más o menos como íbamos. Luego en resultados de evaluación hemos sacado un 89 en promedio por detección de sesgos, porque no alucinaciones, relevancia y precisión sobre IA. Est un poco bajo ese de relevancia y presión sobre estructura y coherencia, valores editoriales y tono cómico e inteligente. Y pues aquí está el detalle de de la de cada evaluación informes se han estado agregando y ya genero el reporte. Si lo vamos a hacer en el reporte va a ser este acá y luego vamos a ver el en las métricas personalizadas. Ah, perdón, este no es. Es en el este de acá. Ajá. Y quedó como aprobado. por 8 de 10, digamos, y las métricas que estuvo evaluando, ¿no?, en la escala de calificación y próximos pasos que si está aprobado proceder a publicación en otros de revisión y el otro rechazado. Eso podría quedar para publicarse. Es es con deep EVAL y GAL. Yo podría jalar también las otras que vienen de caja. Sí, podría extender esto con pruebas de seguridad, sí, pero eso le vamos a tocar en la siguiente. de clase. Tenemos algunas consultas, muchachos. Ya estamos casi cerrando la sesión. Quisiera saber si tienen algunas consultas o dudas. Hemos estado explorando DPAL, hemos estado viendo este justamente algunos apartados de seguridad que eso son continuación para la próxima clase. No sé si hay alguna consulta ahí. Si no, nos vamos al siguiente punto rapidito cerrando. Bueno, el siguiente punto es eh justamente los cloud service providers y ahí me dijeron justamente Microsoft y sí, pues Microsoft tiene este GenI Ops Life Cyclle que habla un poco más bien de automatizar todo el ciclo por medio de feedback incluso, ¿no? Entonces voy a tener model selection, construyo mi AI application del AI application lo saco a deployment de tal lanzo el deployment de producto, voy a sacar igual telemetría, feedback y lo demás, voy a revisitar la aplicación para mejorarla. Voy a ver si cambio el modelo y así no. Voy a estar iterando continuamente en la implementación y al continuar entre toda esta implementación van a ver escenarios donde puedo automatizar. Esta es la pantalla del AI fund que querían ver, creo. Este es el ahora control plane, ya no va a ser, sino Microsoft Roundry. Y este va a ser el control plane. En este caso estamos viendo un agente de Microsoft 365 donde tenemos ciertas métricas que se capturan de caja. Esta es una forma más desatendida, más, ¿cómo decir? De menos código. Les voy a decir less code en vez de low code. Esta va a ser less code, si hay algito código. Less code para poder generar todas las métricas. Miren, acá tenemos operational matrics, que son netamente el costo de tokens, cuánto estamos demorando en la atencia nivel operativo. Esto es infraestructuraamente Luego evaluation matrics, intend resolution, coherence, reconciliation success, schedule evaluations. Entonces, yo puedo agendar una evaluación online, digamos, en producción, no sé, todos los domingos a las 2 de la mañana o lunes 2 de la mañana, 3 de la mañana y haz una evaluación para ver cómo estamos yendo, ¿no? Y lo mismo del aportado de seguridad que vamos a la próxima clase, está red teaming attack success. rate. Se podría como que agendar una evaluación de seguridad y luego sacar un un ratio de cuán efectivo fue el ataque o no, no. Entonces, eso es algo muy interesante. Y un poco de telemetría, de agent runs, el throughput, los requests, un poco el control plane, ¿no? Y ahí las vistas por tiempo. Y este vendría a ser el de Bedrock. Este es Bedrock Evaluation. Acá igual muy par sido. Generamos un base evaluation en base al app que quiero ya evaluar conforme los knowledge bases. Acá como que marco los knowledge bases que voy a utilizar. Mej un LLM para luego hacer la evaluación sobre estos datasets, el LLM y el sistema program asociado para probar el agente. Y luego me sale como que este chart de diferentes métricas, ¿no? He helpfulness, faithfulness, correctness, compness, cojin refusal y obviamente después de esto me sale la ejecución de carrels de cada implementación, ¿no? Esto es en bedrock. Este es un poco el pipeline que seguiríamos internamente. Se puede hacer por medio del wizard sin tener que saltar así como aparece acá, pero al final sí necesitas tu datas set en un backet S3, hagas con el wizard o no lo necesitas. Hay un formato específico para subir el dataset. Lo mismo Fry y tiene un formato especial para el dataset y esos los pones en un backet S3. En el caso de Microsoft lo puedes jalar no más y se pone un blob storage y este al final de tachar digamos estos datas definir el prompt de evaluación o seleccionar las métricas de evaluación, luego ejecutas el job y tienes un reporte automatizado que lo el data scientist puede analizar y y lo mismo el otro caso de found es una un poco la vista del de bedrock dentro de los mecanismos de evaluación que tenemos automática o puedes traer a tu equipo humano a que trabaje o humano trabajando con un equipo de AWS con un servicio administrado, obviamente y acá puedes poner tus evaluation jobs. Y eso es, muchachos. Así que toda esta clase ha sido en relación evaluation. De nuevo, acá me he equivocado un poquito en la fecha la fecha de este Esta tarea es no es del cinco, sino vendría a ser del nueve, ¿no? Perc, ¿cómo vamos a hacer? Un poco apretadas. Vamos a ponerla para el 15 ya para que sea más fácil para ustedes, porque esa es la de recuperación. Entonces esta es la tarea grupal. Ah, es continuar lo que hemos estado haciendo acá es definir métricas. Laboren una implementación de esas métricas. Eso documéntenlo. Es como que yo vaya a un Google Doc. Profe, vamos a hacer estas métricas A, B y C. Esta va a ser custom. Estas van a ser de caja y lo vamos a hacer convalival más LMI. O vamos a usar este todo en DPAL o todo en y o aparte vamos a usar ragas, digamos. Entonces eso lo ponemos documentado en un Google Doc para que yo también pueda subrayar y comentarles ahí, porfa. Ya. Y esa sería la tarea de esta clase y esa sería la clase de hoy. No sé si tienen consultas,\nBoris, creo eh está bien la fecha porque estoy en dos semanas o es porque es muy cargado el tarea.\nEh, justo porque es muy cargado porque mira, si lo pongo al nueve se junta con el otro y se junta con el otro y van a estar muy apretados, así que lo mandaré a la siguiente semana.\nYa está bien. Sí, sí. para que tenga más tiempo.\nEh, bueno, yo quería comentarte que hay un grupo que me consultó si puede quedarse 5 minutos a conversar contigo un tema de su proyecto.\nYa,\nya. Y bueno, no sé si quieren hacer alguna pregunta, Boris, lo único que quería comentarles es que el día de mañana yo no voy a por acompañarlos, les voy a estar acompañando el asistente Augusto Flores. Igual él me va a avisar cualquier cualquier cosa que ocurra el día de mañana, pero igual me pueden avisar eh si digamos tienen algún inconveniente o se van a demorar ingresar para yo darle toda esa información a gusto. Okay. Solamente para que lo tengan informado, para que sepan y nada. Bueno, eso sería lo único por mi parte, ¿no?\nYo tengo un comunicado importante para todos, muchachos. El vamos a tener según el plan unas sesiones que no las voy a estar dictando yo, si no va a haber otro profesor que me va a apoyar. Eh, estas sesiones son las de implementación más que todo de casos de uso. Ya, esto es debido a que tengo un periodo de tiempo que voy a estar un poco complicado de atenderlos por un tema personal y es eh justamente ahí donde estoy pidiendo este apoyo, ¿no? Este este apoyo va a ser netamente para las sesiones. Acá les digo de aplicaciones en educación, periodismo, salud y derecho. Estas son del 11, el 16 y el 18 de diciembre. Esas tres sesiones me voy a estar apoyando del profesor Eduardo Salvador. Ya el profesor Eduardo Salvador va a estar conmigo el 10 de diciembre también en la sesión de asesoría. Ahí también lo vamos a poder conocer. Ya es solamente por este periodo de diciembre ya. Y ya este No sé ya si quedamos con estas fechas de enero, pero con las fechas de enero estaríamos retomando el 5, el 7, el 9 de enero con diseño de sistema, diseño funcional y presentación final de sus proyectos, muchachos. Así que\nestamos dando también un tiempo para que trabajen en Bendy en Navidad. Espero que hayan podido avanzar su proyecto.\nSí, justo feliz Navidad.\nJusto para Eh, apoyar lo que has dicho, Boris. Sí, ya se confirmó. Creo que algunos no lo sabían, pero ya se confirmó. Ya la mayoría haya votado. Algunos sí me hicieron algunas consultas, tal vez porque hubo un error en la última fecha del viernes que estaba en la mañana y era un error simplemente y ya lo corregimos,\npero ya se decidió, ya se cambiaron las fechas, ya están movidas en la plataforma de Utec, así que\nya quedamos para hasta el siguiente año. Okay.\nExcelente.\nY miren, igual voy a poner las fechas, este, como una imagen acá en el chat. para que todos lo puedan tener claro y no nos estemos\nNo, también lo tienen, está en su grupo de WhatsApp también se los pasé la fechas.\nYa, buenísimo, buenísimo. Entonces, estamos en la misma página. Entonces, miren, nos queda la del 2 de diciembre, las del 4 de diciembre, que es seguridad y luego ética. Sé que ustedes van a decir, \"Ah, pero es ética.\" Ah, pero pero bueno, no, pero recuerden que ustedes me han pedido ver el foundary y ahí viene responsible AI. Y responsible AI va a asociado a seguridad. y seguridad va asociado a este capítulo. Así que como hemos estado viendo, es como una sesión te lleva la otra y te lleva la otra y sin querer vas así nos va a tocar así hasta la de ética que es responsible y ahí como gestiona los riesgos asociados a seguridad. Gust. Así que muchachos, los espero en la próxima sesión de mañana martes hablando de seguridad y luego vamos a hablar de ética para luego vernos el próximo año. Okay, gracias. Ahí te voy a dar el anfitrión para que el grupo que quier consultas se las haga y ya las demás ya se pueden ir retirando.\nMuchas gracias.\nGracias. Hasta luego con todos. Nos vemos.",
    "start_position": 0,
    "end_position": 112793,
    "anchor_text": "¿Qué tal? Buenas noches con todos. El día de hoy, bueno, vamos a comenzar un nuevo módulo. Comenzamo",
    "word_count": 20534,
    "chunk_hash": "2a6fd599eaf8"
  }
]