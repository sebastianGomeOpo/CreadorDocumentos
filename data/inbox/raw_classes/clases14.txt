Hola, ¿cómo estamos?
¿Qué tal? ¿Cómo están, chicos? Buenas noches. Vamos a dar inicio a esta sesión. Voy a estoy grabando ya la sesión. Boris, un gustazo tenerte acá en esta sesión. Mi nombre es Augusto. Voy a estar cubriendo esta sesión. Espero que les vaya excelente en esta clase. Buenísimo. A ver, dame un momentito, porfar mi cámara.
Listo. Vamos preparándonos, chicos. Busquen un lugar cómodo, pero no tan cómodo. Y un lapicero para tomar apuntes. Mientras esperamos Chicos, por favor, su paciencia. El profesor se está reconectando. ¿Qué tal? Disculpen, por algún motivo mi antivirus me está bloqueando la cámara, pero vamos a empezar igual. Genial. Entonces, arrancamos la sesión. Ya está grabando. Solo quedaría que pueda compartir su presentación. Okay, ya deberían ver mi pantalla, ¿verdad?
Ya, buenazo. Bien, esta sesión a diferencia de las anteriores, no vamos a hablar mucho de los agentes per sé. Si no, vamos a hablar un poquito sobre este lo que está alrededor de la construcción de agentes. La si bien hacemos una recapitulación de la sesión anterior, hemos estado hablando de cómo administramos diferentes herramientas, cómo administramos el conocimiento y cómo los ponemos estos en agentes, ¿no? Entonces, para empezar me gustaría un poco tener su feedback. Díganme ustedes, ¿qué retos o qué este encontrar un un poco difícil o un poco más complicado en la implementación de agentes de conocimiento y agentes de herramientas. ¿Cuál les pareció un poco más complicado? Pónganlo ahí en el chat si gustan. Pueden indicarme qué parte el de conocimiento. Ya, o sea, con eso están implementando RC. ¿Qué qué otro aspecto más les pareció complicado? Solo solo a César le pareció complicado. Todos los demás podrían implementar entonces un agente de conocimiento. Y todos orquestación de herramientas, ya la orquestación de herramientas en sí, la integración de herramientas. Ahí decía Carlos. Y no sé si alguno de ustedes me puede decir qué es lo que está encontrando un poco más complicado de cada uno. Creo que la mayoría ha dicho RAC. No sé. Ahí César. Oh, Enrique.
Sí. ¿Qué tal, Boris? E en el ejercicio que nos dejaste para poder hacer, en las pruebas que hacía, este, algunas veces no obtenía la información que necesitaba o que realizaba en la consulta y todavía no tenía claro cómo cómo quizás calibrarla, ¿no? Mencionabas que eh es altamente probable que es el chunking, pero o el chunking, pero no sabía cómo, ¿no? Entonces, eso lo vi un poco eh complejo el tema de cómo calibrar y de verdad traer la información que se estaba eh obteniendo.
Ya, ya te entiendo. Entonces, un poco el entender el cómo funciona por abajo o las estrategias de chunking.
Estrategias de chunking por mi lado, al menos.
Ya. Sí, porque la verdad hay diferentes formas de partir la data, ¿no? Es que la data también es muy variada. Podríamos tener diferentes escenarios por la cual la data se presenta, ¿no? Por ejemplo, el caso que habíamos visto era de reviews de restaurantes, entonces la unicidad hacia el review era pues más fácil, ¿no? Empezar a tomar review por review como si fueran documentos y los empiezo a agregar. Igual este había unos escenarios con cierto overlap, ¿no? Entonces esos me podrían servir quizás para quizás documentos un poco más largos donde se tenga que considerar ciertos en los cuales yo tenga que implementar, pues, ¿no?, el el cómo hacerlo un poco más más legible, digamos, ¿no? Pero sí hay diferentes retos. ¿Quién más este ha encontrado ese tipo de retos?
Ahí. Buenas noches. Eh, lo mismo que había comentado sobre César,
sobre lo que César comentó, ¿no? Yo lo dejé por el chat, pero es lo mismo, o sea, las técnicas para la indexación de la información, básicamente a nivel de RAJ es a veces en donde tú tienes que saber cómo indexarlo, ¿no? Cuál es la mejor forma de indexar, cómo es que vas a obtener la información
y eso puede cambiar de acuerdo a diferentes escenarios que de información que tú estés indexando, ¿no? Entonces, Eso hace de que tengas un abanico más de posibilidades o de metodologías o de buenas prácticas de cómo poder indexar esto. Entonces, aprenderlas todos de ellos es un poco más complicado, ¿no?
Ya. O sea, términas de indexación entendiendo.
Sí.
Ya. Este, la indexación, cuando tomamos indexación, miren lo como la misma, ¿cómo te digo? Entenderla es lo que este te entiendo que es el reto, comprender cómo se da la indexación,
¿no? Sino que cuando tú indexas la información normalmente RG,
eh, por ejemplo, si es mucha información, ¿cómo? Si es mucha información, ¿cómo lo indexo? Cuando le hago una consulta al RAG, en los chants, no me va a tener, quizás me va a tener el resultado. resultado medio cortado, no me lo va a dar completo, ¿no? Entonces, ¿cómo qué técnica aplico para que al fin y al cabo me pueda dar todo el resultado o me pueda dar un resumen de ese resultado? Pero que a mí lo que me ha pasado bastante es de que cuando es mucho texto solo te da una parte, ¿no? Y no te lo da completo, te lo corta, te lo entrecorta. Entonces
es que ahí lo que ha pasado es tu chan ha sido muy chiquito a comparación de lo que tú esperabas que saque.
Claro. Entonces, mira, si te das cuenta es un poco determina el change, digamos, ¿no? Y determina el tamaño de tu pedazo de texto indexado, más o menos lo que tú esperarías. Entonces, un poco la pregunta es, ¿cómo esperarías que fuera indexada la información? ¿Con qué citación promedio en tamaño? Por eso hay habían diferentes técnicas, ¿no? Uno que era fixed, otro que era dinámico, otro era basado en contexto, otro era super irregular y que tenía este diferentes tamaños. laro del texto. Entonces tiene diferentes estrategias por las cuales lograr el mejor chunking. Entonces ahí que podría decir un poco apoyarnos en justo el chunking basado en contexto que ese podríamos hacerlo un poco apoyados de otro LLM que te pueda ayudar a revisar ah esta parte del del texto está en en relación a esta otra y que podamos tener cierto overlap, digamos, en en lo común. De esa forma podrías tener indexación como que más eh certera.
Okay.
Agentes con avatares dice José. ¿Te refieres un poco, José a la parte de definir roles? Entiendo.
Eh, no, a mí se me ha hecho complicado crear agentes que tengan avatar en real time, o sea, una persona por atrás hablando. ¿Cómo? ¿Cómo así? No, no te entiendo. O sea, ¿te refieres a que traspase la comunicación a una persona?
No, no, no. Un avatar, o sea, un como una persona creada con
pero que hable
y que sus gestos y lo que va hablando tenga coherencia con con el texto que va generando la en este caso, ¿no?
Ah, tú dices ya que ya, pero te refieres más al al UI, o sea, al video de la videollamada, digamos. O sea, hay un servicio de Asure que hace eso, pero aú así no es muy bueno. Ajá. Ajá. El el de Speech Services tiene justo inición de avatares.
Sí. No, no, ese no es muy bueno. Este hay otros servicios como de Heen y otros este de generación de avatares.
Claro. Pero, por ejemplo, en tiempo real. El de es en tiempo real.
Exacto. Es no es no es real time. Pero podríamos ver algunos mecanismos, podríamos revisar algo de eso también. Genial. Se obtiene a veces resultado de que uno espera con consultas RAC. Sí, por eso hay diferentes métricas del RAC que justo estábamos viendo, ¿no? Entonces, un poco lo que me llevo es de para revisar un poco más de RAC. Vamos a a ver si es que podemos buscar un espacio más para entrar más a fondo del rack. Ya. Este, más que todo en el track de evaluación, o sea, ahí me gustaría también compartir un poquito más de evaluación, pero esta esa parte del curso la la dicta otro docente, ¿ya? Y este yo voy a más o menos tratar de perfilar mi contenido un poquito para que no hablemos lo mismo porque si no al final van a haber una clase repetida. La idea no es que tengamos una clase repetida, sino que converse tanto mi clase como la de Víctor, me parece. Entonces al final este esos son mecanismos de Lo que va a ver es un poco apoyarse en un concepto que se llama reinforcement learning by human feedback. Ay, perdón, learning by human feedback. Ahí lo pueden ir googleando. Reinforcement learning by human feedback habla justamente tocar el feedback del log del chat y utilizarlo para mejorar. Ya, hasta ahí lo dejo. Pero todo lo de métricas es un track de evaluación que también lo debemos tocar. Ya. Y y hay bastante sobre rag justamente ahí les había puesto en contexto la sesión pasada deep eval, ¿no? Con métricas de evaluación de rag. Eso te ayuda un poco a ver justamente esto, ¿no? De que no se obtiene a veces el resultado que uno espera. Eso se llama groundedness. Entonces que tu respuesta está como que en relación a la situación que esperas, ¿no? Ese es un grounded groundedness, o sea, porque está como que fundada la respuesta. ¿Cuán fundada está la respuesta en base a los Entonces ese es un una métrica que ahí se puede lograr. Entonces, bien, ya que tuvo su feedback, les se los agradezco mucho. Eso lo voy a tomar también para poder este reforzar una sesión que la sesión que toca como cierre de de este módulo. Y en este módulo lo que, o sea, en este capítulo, perdón, vamos a comprender lo que está más bien alrededor. Entonces, seguramente han han visto en algún punto del tiempo este el Paper ese de Google que habla del del hidden dept. Hidden deptical dept creo que technical dept in machine learning systems. Este paper es antiguísimo, ya me antiguo, pero habla justamente de esta imagen. Cuando tocaban los machine learning systems, recién cuando arrancaban, hablaba de que esta porcita del medio es tu machine learning code y mira todo lo que genera. alrededor para que esto realmente funcione. Configuración, data collection, data verification, feiture restraction, process management tools, análisis tool, machine resource serving, infrastructure monitoring, todo esto va alrededor de tu machine learning que era estit y ese paper que de Google justamente habla de que esa es la deuda técnica que te deja los sistemas de machine learning y lo cual no exceptuúa a justo las aplicaciones de AI. Entonces esta sesión justamente habla de eso el periférico, ¿no? Es comprender qué componentes no hay son estratégicos. También comprender una arquitectura de infraestructura enterprise de un agente para también apuntar hacia ya la implementación con a nivel de infraestructura. Entonces, para emprender con esto, vamos a revisar primero este justo lo que ya habíamos tenido en contexto, ¿no? Un agente que tiene conocimiento, uso de memoria y herramientas. Vamos. Vamos a ver opciones de esta sesión en a nivel de frontend y telemetría. Vamos aquí tocar a LMIT y en opciones de frontend vamos a ver Streamlet y con eso vamos a entrar luego a un mecanismo de invocación ya más bien orientándose a multiagente que es en azul que algunos equipos ya les había comentado más o menos un poco. Entonces si volvemos a a lo que decíamos de un agente, de un agente hemos estado hablando justamente de que un agente se apoya en memoria. Hemos revisado herramientas, hemos revisado el conocimiento que hemos visto justamente el rack. Aquí hemos visto los tools que hemos visto que se pueden definir de diversas formas con el textorator de @tool o si no con la clase netamente de tool. ¿Ya? Entonces estos estas dos formas yo sé que hay un proceso de migración ahí, pero igual están siendo utilizadas. Hay un tema de de gestión de memoria que hemos visto el conversation conversation memory buffer. Después de estos les había recomendado otro también que era mem zero, si es que empiezan a apoyarse de un mecanismo externo. Entonces todos estos te soporte cre un agente, pero alrededor si yo le pongo este agente en la cancha debo asegurarme de que no me voy a llevar al conversation history data sensible, por ejemplo, ¿no? ¿Qué pasa si es que estoy trabajando con un agente que soluciona temas financieros, que podría ser hasta temas de cobranza, cobranza de una tarjeta de crédito y esa tarjeta de crédito se filtra o los datos de la tarjeta pasan por el conversational history. Yo no quisiera que luego esa data este se vuelva sensible y al final una papa caliente dentro de mis sistemas, ¿no? Entonces lo lo ideal es de que dentro de esa data no estructurada yo la pueda filtrar, ¿no? Filtrar esos personal identifiable information, todos estos PII. tienen diferentes mecanismos, ¿no?, para poder ser filtrados. Los primordiales son los reys. Podría usar expresiones regulares para para filtrarlos. Podría usar es con esto este algunos filtros a que apuntan hacia el DNI, por ejemplo, o al celular o al correo. Entonces, es esos que son de una estructura específica por medio de una invocación a un Reyex que podría estar en el punto del frontend, incluso podría filtrarlos, pero también me tengo que preguntar, ¿qué pasa si los filtro? ¿Va a proceder la transacción? ¿Sí o no? La gente los necesita dentro de la transacción, ¿sí o no? Entonces, ¿en qué momento ya es el mejor momento para filtrarlos? Y la respuesta es un poco en la gestión ya de persistencia. O sea, hay puntos en los cuales sí puedo más o menos decirle al LLM que no entre en contexto con data sensible, que es ideal, ¿ya? Pero a la vez entre herramientas va a ser necesario que estos datos que son importantes se puedan poner en contexto sin que un LLM los tenga que, o sea, reincorporar dentro de su proceso de feedback. ¿Okay? Entonces, el punto crítico de la extracción de esta data es al momento de persistencia. En otras palabras, podría yo tenerla en movimiento, este, digamos, libremente en la aplicación. Es un poco menos probable, voy a decir esto, menos probable es que alguien venga y te ataque a la memoria volátil de tu web apparer información de la memoria volátil del web app si es que el web app está bien protegido. Eso es lo menos probable. Por eso no te estoy diciendo que eso no se va a dar, sino que es menos probable de que se al menos probable pues es posible asumir de que esta data en movimiento puede estar encriptada Pero dentro de un, perdón, puede estar en movimiento tal cual de forma natural RAW, pero dentro de un canal que sí puede estar encriptado. El canal siendo encriptado, no es un canal encriptado con la data legible. Esto te permite transaccionar conforme la necesidad y mantener aún así inscripción en el canal. Lo que se busca es que obviamente sea encriptado de punta a punta, no de extremo a extremo. Entonces, teniendo un canal encriptado, Y los datos en movimiento es posible transaccionar. Si es es recomendable persistir los datos tal cual, no necesariamente. Entonces ahí tenemos que determinar en qué momento es el momento preciso para empezar con esta anonimización. Después hay otras capacidades que van relación a la carga en load balancing. Si es que nosotros vamos a voy a cambiar un poco acá el color del lápiz este Si vamos al el a pensar un poco en en administración de carga, vamos a necesitar pensar en tokens por minuto y este es el trupeut. Ah, en ciertos eh en ciertos momentos eh se hacía un cálculo delput de tokens para poder estimar una carga hacia una suscripción. Por ejemplo, en Asure Open AI tú tenías que calcular los tokens por minuto. de cada suscripción para poder determinar como tu suscripción que tiene cierto nivel de trupput solicitado por un formulario, era era así en el pasado. Este te permite escalar a ciertos niveles dentro de tus ambientes de desarrollo, de certificación o testing y de producción, ¿no? Entonces, lo que tú tenías que hacer era prácticamente tomar tu cuota y como que partirla, ¿no?, en esos ambientes. Y encima, eh, o sea, lo que busca uno que este abajo, este como que intermedio y este alto, ¿no? Al ser producción. Y dentro de estas cuotas, si tú tenías más de una implementación, tenías que partirla también, ¿no? Es que tuvieras, por ejemplo, tres agentes, ¿no? Agente uno, agente dos, el agente tres y como que tenías que partir la cuota. Esto este cambió un poco dentro de Asure y ahora este ya no se tiene que uno preocupar de los tokens por minuto a menos de que sea netamente para el nivel de la factura. Porque ahora este Asure tiene un globally distributed API. Entonces al final el point está globalmente distribuido y al estar globalmente distribuido, Microsoft se encarga de load balancing y tú te encargas nada más de una subnet dentro de tu suscripción. En otras palabras, te abstrae un poco la gestión de la carga en el Load Balancer más Adelante. Vamos a ver acá un apartado que se llama el AI Gateway, que justamente ayuda a resolver estos de acá. Ya por ahora lo vamos a estacionar hasta el load balancing, pero luego del load balancing crece hay un un concepto que es el AI gateway que nos permite hacer switch entre modelos inclusos distribuidos globalmente. Open AI también tiene un tierre ahora que te permite hacer este load balancing. Ah, AWS también hace global distributed load balancing. Este también con Antropic, no estoy muy seguro si Antropic tiene el mismo offering de globally distributed. Yo creería que sí, ya, pero esto te ayuda un poco con el balanceo de carga de los tokens por minuto que llegan a tu end point. Eso a nivel de tokens. Luego, después de todo esto, nos toca pensar en la infraestructura tal cual, pues, ¿no? O sea, balanceo de la carga de un web app, por ejemplo, que hay diferentes componentes que se pueden utilizar. acá desde un balancer tal cual hasta un front door o cualquier aplicación que tenga un gateway en el medio que te ayude a controlar las políticas de desfogue y cortado circuito, ¿no? Para la solución en sí. Bueno, esta es ya las herramientas y dentro de observabilidad y telemetría, hay varios productos que nos pueden servir de la mano, que más que todo son como que agentes instalables. Este tipo de agente no es el tipo de agentes que nosotros conocemos, sino le llamaron agent, pero en son aplicaciones cliente, como que te instalen un client de un server, algo así. Entonces este client lo que hace es una labor que netamente es la recopilación, recopila información. Entonces esta recopilación de información lo hace con un nombre que se llaman colectores. Entonces estos colectores, estos data collectors en el mercado tienen diferentes nombres, ¿no? Entonces, podemos encontrar data collectors como propios de Azure, propios de AWS, propios de, por ejemplo, de Asure, eh, Data Collectors están como application Insites en AWS está como Cloudwatch. Este, después tenemos conectores asociados a diferentes mecanismos de recolección como el de Open Telemetry, por ejemplo, que es un nuevo estándar a nivel enterprise. Entonces, toda la ingesta por Open Telemetry también es un es un generalmente cuando tenemos mecanismos de seguridad basados en integración a recursos, ¿no? Porque obviamente para recolectar la información te tienes que conectar al recurso. Entonces estos colectores al final nos permiten recuperar información de del comportamiento de la gente, ¿no? Logs que podrían ser logs de la misma infraestructura, logs de invocación de herramientas, logs de cómo desarrolló el agente su cadena de pensamiento, logs de cómo a cierta herramienta de cómo hizo el retrieval aumentation, si es que hizo un reranking al momento de sacar la los chunks o no. Toda esa trazabilidad nos sirve para también determinar cuán bien o cu mal está yendo o qué escenarios de error hemos encontrado donde, o sea, jaló una información que no debía ser y también identificar cuando ciertos escenarios desafortunados se materializa y con eso me va a saltar al frente que son los g rails que son los que controlan que estos escenarios no afortunados se materialicen. Entonces cuando yo tengo un Garrail, hay diferentes formas de implementarla. La open source generalmente se hace con un SK de justamente el mismo nombre. Grails. Grails.aiai es la es el el framework de justamente la empresa. que se llama igual carrails. Entonces acá hay un SDK que lo que hace es netamente ejecuta modelos de NLP y también algunas invocaciones es que yo tuviera el servicio de pago a sus modelos propios para ejecutar G rails, o sea, para ejecutar una evaluación del input y el output. Entonces, un rail de input lo que hace es en el in, supongamos tenemos un mensaje de entrada. El mensaje de entrada va a pasar luego por un Garril y este señor se va a encargar de evaluar aquí cuán nocivo podría ser este mensaje si es que cae dentro de una categoría de violencia o de contenido sexual o de apología este a la discriminación o si es que tiene contenido político o o que haga incitación a violencia, ese tipo de de escenarios, ¿no? Y estos los validan justamente este servicio de Grails. Lo que hace esta capa es un poco filtrar. En otras palabras, todos los garrils lo que buscan es hacer un filtro, un filtro del input hacia el input que yo debería responder. ¿Ya? Entonces es netamente eso. Si yo tengo un guerril de input, que por ejemplo, supongamos, ¿no?, eh filtra todo el contenido sexual, no va a ser lo mismo para todos los agentes, supongamos, ¿no? Uno de bancos finanzas, pues obviamente lo tiene que hacer. Pero supongamos el caso de tener un agente que lo que hace es esta, me encantó que era un agente que toma el audio de una ambulancia cuando un paramédico está teniendo una emergencia, genera la transcripción y esta transcripción le sirve a un agente para empezar a elaborar una ficha de triaje para un hospital. Y mientras el paramédico está en la ambulancia con el paciente de camino entre toda la bulla, es capaz de generar la transcripción, armar un resumen y crear la prioridad de atención dentro de la sala de emergencias. Entonces, para hacer eso posible, necesitamos garrails de alta performance. ¿Por qué? Porque en ese escenario yo voy a tener que bajar un poco la flexibilidad a cierto tipo de contenido, por ejemplo, el de violencia. ¿Qué pasa si hemos la urgencia es en relación a a un delito? O, ¿qué pasa si ha pasado un accidente importante? Necesitamos que el paramédico describa todo tal cual es. Entonces ahí bajamos los niveles de permisividad de ese contenido violento. Entonces, en cambio, en un agente que pueda ser un asesor de banca y finanzas, digamos que asesor bursátil, ese tipo de contenido como que no debería aparecer, ¿no? Entonces ahí el carril se comporta diferente y se pone más estricto. Entonces, esos son los comportamientos que podemos establecer en diferentes filtros de contenido con los g rates, tanto de input como también de output, porque lo que podría pasar es que un input genere un output que no debe generarse, ¿no? Como que le fuerce, digamos, cierto input a votar, digamos, secretos o a empezar a a crear una respuesta que luego al final puede hacer un daño reputacional para la empresa. Entonces, ahí se filtr de contenido, tanto de entrada como de salida. Esos son los carrils. Y entrando con estos perfiles también de control, entran también los de seguridad. Y acá hay un tema que luego vamos a ver en seguridad que se llama, o sea, hay un capítulo dedicado a seguridad que es el prom hacking. Proming, ¿no? Y seguro han visto en internet como este algunos este identifican ciertos ciertas plataformas de ella y que también hacen cosas que no deberían hacer, sobre todo yo he visto muchos videos de Grock, la de X1, la de Elon Musk haciendo cosas que no son tan apropiadas, ¿no? No muy éticas. Entonces eso te lleva a un abuso del recurso, ¿no? Y más propenso a que sea vulnerable, ¿no? También los escenarios de proming. Entonces, si bien teníamos la observabilidad y telemetría para generar un aspecto de seguridad y control. También la observalidad y telemetría ayuda a generar unos mecanismos de evaluación y de mejorad continua. Y justo acá está esta parte que les había mencionado al inicio, Reforcement Learning by Human Feedback. Entonces es este feedback el que te ayuda a mejorar continuamente dentro de tu aplicación. Y los EVAL son las los mecanismos de evaluación que vamos a poder encontrar en Frameworks, por ejemplo, como deep Eval. yal y este esto es si es que quiero ejecutarlo super open sourceado. Ah, no había mencionado los garils. Acá ahora también les muestro. Hay escenarios de garrails dentro de Asure, dentro del producto de Azure Open AI, ya vienen de caja dentro de Bedrock también. Este, Google también tiene sus propios Garrails. Entonces, cada producto administrado tiene una capa de Garrails. En otras palabras, han tomado este SDK más sus propios modelos, su propio AI para generar G rails que te protejan y eso todos lo tienen free de caja, o sea, todos te lo dan gratis porque al final es un control que lo que hace es demostrar que su producto es bueno, ¿no? Entonces por eso le ofrecen de forma gratuita. Y lo mismo con Evals. Los EVALs aparecen en varios mecanismos. Por ejemplo, hay EVALs dentro de Strands, que es el framework de AWS. También hay EVALs dentro de Bedrock de AWS que viene tienen también de caja y incluso acá hay un tema de AI judge dentro de Bedrock. Entonces acá se pueden hacer jueces para evaluar a tu solución y este Asure también tiene mecanismos de evals que recién están siendo implementados, la verdad está en preview. Este Asure, por ejemplo, tiene un mecanismo de evaluación dentro del AA foundry y tiene también un SDK que justamente se llama Evaluation SDK. Entonces lo que hace es mostrarnos un panorama de que, okay, tenemos que hacer más cosas alrededor que no son justamente un agente. O sea, aquí tienen alguna duda les he tratado de explicar así como a manera de resumen esto. Vamos a ver unos cuantos más a detalle. Okay, vamos a seguir.
Hola, eh, Boris, disculpa.
Sí, sí, sí, dime.
Eh, mira, en el lot Así.
Mm.
Podrías hacer la comparación contra un balanceador eh de webs o de APIs.
La verdad es muy parecido.
Ya. Ajá.
No sé si acá yo puedo presentar un Creo que se puede poner acá en Zoom un whiteboard. Creo que no se puede whitebo por acá. No. Ah, sí se puede. Ah, vamos a ver. Ajá, genial. Sí, están viendo una pizarra.
Sí, pues
ya. Excelente. Ya. Suponte que este es un APIM. Ya. Entonces, si este es un APIM, o sea, que es es un API management al final. Ya management. Voy a usar muchos términos de discúlpame si es que voy a poner muchos términos de Ya, pero este es al final un gateway y este administra la capacidad de de diferentes APIs. Si tú ves acá, acá dentro qué hay. Cuando tú generalmente estableces un API management, acá dentro tú tienes diferentes políticas de balanceo, ¿no es cierto? Tienes este políticas de corto circuito, de desfogue, digamos, ¿no? De desfogue y hay otras políticas de corto circuito. Sin querer ya voy a tocar el gateway, pero ya pues caballeros, van va a ser la repeticada en todo caso. Entonces acá tú tienes estas políticas dentro de la manchement y tú vas a tener alrededor diferentes APIs, ¿no? Vas a tener este acá, API 1 y este API puede estar en un AKS ya que tenga, no sé, uns que tenga dos pots, digamos, ¿no? Dos pots. Y este puede ser un AKS que tiene desplegados dos pots, ¿no? AKS con dos pots. Un AKS con dos pots. Este es el API 2 que tiene un AKS que tiene la capacidad de escalar de dos hasta cinco pots, ¿ya? De forma dinámica. Entonces, tú al final lo que haces es, okay, este que tengo acá, pues lo va a monitorear a este en relación a sus políticas de comunicación, ¿no? Y a este. Entonces, cada vez que tú veas de que este a nivel de balanceado de la carga está creciendo en capacidad, o sea, para eso existe la telemetría y la observabilidad también. Entonces, tú ejecutas una política conforme lo que lees de acá. O acá tú, ¿qué le estás poniendo? Una capa de observabilidad. Lo has poniendo así tal cual. Ya. Pero al final esto te puede activar o ejecutar cierto policy acá. ¿Cuál podría ser este policy? Cambiar este, supongamos, ¿no? Está llegando a niveles altos, entonces tú le activas el policy de balanceo. Entonces, si tú tenías, por ejemplo, este AKS con una réplica de este otro AKS que estaba apagado, pues agarra y manda un request y como estaba en idle, se activa. Entonces ya tienes un balanceado de dos AKS con el mismo API. ¿Y quién se encarga de rotear y mantener el El como que la comunicación hacia estos dos es el API management porque también tiene además de esa políticas el ruteo, ¿no? Es lo que gestionas tú con APIs. ¿Qué de diferente hay acá versus lo de el dels? No, acá tú como unidad tienes el request y dentro de ese request, cuando tienes alta comunicación, tú tienes acá requests. por minuto, ¿no es cierto? Entonces, esto es lo que al final te determina. Este, puede ser una métrica que determine también el el este el crecimiento, ¿no? También el gesto de memoria, ¿no? Hay varias métricas de infraestructura que se pueden juntar acá para decir, "Okay, activemos la política." Entonces, algo parecido es con las implementaciones de solo que en vez de ser requests son tokens. Y acá es tokens por minuto, porque al final los points de acá pueden ser tu open AI. Este puede ser un GPT4 o mini que tiene un end point. Un point. Ah, o sea, te estoy diciendo que es un point dentro de Asure Open AI. Dentro de Open AI. Tú lo tienes ahorita como global distributed o como un point nada más del servicio y Open y se encarga de escalar y luego te cobra, pero por atrás está escalando así. Si tú lo administras en tu propia suscripción, te encargas de esto. O sea, si Open AI administrara este, por ejemplo, ¿no? Asure antes no administraba un API desplegado globalmente, tú tenías que administrar los tokens por minuto. Ahora, ¿de qué te sirve administrar los tokens por minuto? Eh, primero te ayuda a gestionar también el cc imiento de infraestructura, cosa que sabes también que tu aplicación corre en tantos spots de un AKS, digamos, ¿no? Segundo, te ayuda a determinar escenarios de ruteo del Lelems, ese es un caso interesante, ¿no? Que tú podrías destinar más de un point de Open AI a que resuelvan a tu agente, pero tienes más de un point. Eso que te habilita resolver la cantidad de tokens por minuto y tener redundancia. O sea, por ejemplo, tú Puedes tener un end point que esté en, no sé, Estados Unidos East US2, ¿no? Que es la de la que esté en Siarel, me parece, Cer del este y el otro que puede estar en Central, que es es en Houston o Virginia, me parece que es. Entonces, si tú tienes más de un data center, este, con un end point, te ayuda a tener mayor redundancia, cosa que si es que tú tienes un data center que se cae, se cae este en point, pero tú tienes el otro de respaldo para mantener el servicio. Eso es te sirve muchísimo en escenarios intensivos de de request, ¿no? Por ejemplo, aplicaciones masivas de de usuarios como a ver, no sé, las billeteras digitales, por ejemplo, que se utilizan muchísimo. Imagínate la salida de un concierto, uno de los conciertos más grandes del Perú o que que juegue la selección nacional y todo el mundo está este utilizando esas billeteras digitales justo al momento del partido. Entonces, sí, justo como entonces Esos escenarios podrían materializarse, ¿no? Entonces, el punto este que puedas tener estas redundancias que te ayudan justamente a resolver la disponibilidad de tu aplicación y que tengas estos mecanismos para justamente resolver la disponibilidad de tus ends de Open AI y de tus agentes. ¿Cómo se ha encapsulado esto a nivel de Asure? A nivel de Asure, tú tienes hoy un point Este es assure open AI, digamos, ¿no? Assure Open AI, cuando tú creas una instancia te permite tener más de un end point, pero este point que tú pones que puede ser, digamos, un GPT, eh, ahí lo voy a agrandar un poquito más. Ya creo que escogí una pésima forma. Vamos a poner este ya que este sea un GPT cu o mini. Le voy a poner así ya. Este al al ser deployado, ¿qué está haciendo por atrás? Te está dando un único punto de acceso de asure open AI. Primero eso, ¿ya? Entonces este punto de acceso va a tener como que una ruta, que sea, por ejemplo, tu suscripción, este, tu suscripción punto, no sé. tu RG punto tu AI foundry, digamos, o no sería tu assure open AI open AI y el point name que este sea en point 1. Ya, esta ruta no es tu point, esta ruta es tu gateway que está acá atrás y lo que hace Microsoft deploya globalmente así de forma aleatoria las instancias y como son modelos fundacionales no se llevan nunca tu data. Entonces puede estar en cualquier lado de la tierra pero al final lo que tú consumes es solo un único punto y encima esta ruta está dentro de tu virtual network. En otras palabras, le está haciendo un tuneling. Acá hay un un tunneling. Tuneling. a nivel de red hacia un global gateway. Ese global gate pues está en toda la tierra y toda la tierra de de los cuatro minis, o sea, puedes tener toda una malla de despliegue de JPT4 mini en toda la tierra porque voy a usar un modelo fundacional. Entonces, si tú estás conectado en un momento, digamos, a la instancia de ISUS 2, si se cae te vas a la próxima central US, digamos, y si no, te vas a la próxima y así y eso es por la tabla de latencias que tiene ya es asociada desde tu punto de conexión. Entonces, acá dentro estaría tu gateway. Entonces eso es lo que ha hecho recientemente Microsoft y se llevó de encuentro toda esta infraestructura y la metió acá dentro y te lo dejó así. Entonces, ese es uno de los beneficios de usar cloud que te ayuda un poco a reducir toda esta gestión de infraestructura y al final tú tendrías que administrar tus policies, tendrías que hacer todo esto. Eso no quita de que puedas usar luego un AI gateway. Ah, o sea, o sea, sí lo podrías usar, pero al menos te resuelve un poco el tener la necesidad de de gestionar tu propio tráfico, corto circuito, políticas de balance. Eso ya te lo resolvió en el en el offering de Open AI con Global Depoyment. Y esa es la principal diferencia que podríamos tener en con APIs, ¿no? En este caso los APIs se cae, el API se cayó, es muy parecido, ya se cae y ya no está disponible y luego tienes que desplegar el mismo recurso otra vez, pues va a ser lo mismo en relación al control del end point, ¿no? Pero en este caso para los para los end points que teníamos antes, tendríamos que calcular los tokens por minuto, eso te digo antes, mira que fue un antiguo, ¿no? El 2024. Entonces teníamos que calcular los tokens por minuto y empezar a a distribuir el trupput, digamos, en la suscripción y empezar para gestionar así nosotros la carga, pero poco a poco fue evolucionando y ahora tienes el global gateway y ya te olvidaste de todo eso. O sea, si es que esto lo llevas prem, vas a tener que implementar todo esto. Lo mismo tiene ABS en Bedrock. Bedrock también tiene en Antropic Global Deployment y también con los modelos Nova también tiene este despliegue global. Entonces esto de acá al final si es que yo me pongo a ejecutarlo en un Oyama y va a ser una aplicación grandísima. Okay, esto lo voy a tener que gestionar. Me voy a tener que hacer un AI gateway premis que me permita gestionar todas estas políticas on premis, ¿no? De levantar un pot, bueno, sería un un contenedor que ejecute lo mismo que estaba ejecutando el otro que se cayó o para que haga offloading de la carga, ¿no? Y todos deberían ejecutar observabilidad. Al final me extendí un poco, pero no espero haber respondido. No sé si tienes una duda alrededor o
Claro. Sí, gracias. Te agradezco la respuesta. Más claro.
No sabía porque, bueno, uno se ha acostumbrado, bueno, yo estoy acostumbrado a usar un API key, ¿no? Y llamar a la inteligencia y que responda, ¿no? Pero estos escenarios donde pues mucha carga no los lado. Sí. Ajá.
Claro. O sea, si es que hay mucha carga y estás en tu boyama, tu oyama va a correr hasta donde te deja el fierro. Si el fierro ya no ya no da. Pero ya pues necesitas uno más, ¿no? O necesitas estirar el fierro un poco más. Así va a tocar. Pregúntale a WS. Entonces eso va a pasar, ¿no? Es gestión de infraestructura al final. Entonces esos son los escenarios, muchachos. Voy a regresar ahora a presentar de vuelta. Ya.
Okay. Entonces vamos a compartir esto, ¿no? Entonces estábamos acá, creo que hay una consulta Ah, no, era las risas de AWC. Ya. Entonces, este, ese es un escenario y este es un ejemplo como yo lo llevaría hacia ASUR, ¿no? Este esta implementación no es muy Ashwur, la verdad es muy de nuestro curso ya, por más el que aparece asure. Pero, ¿por qué pongo asure? Para que más o menos me entiendan y voy a garabatear acá de nuevo para explicarles qué necesitamos de una aplicación a nivel end to end que sea de un agente. Ya, un único agente vive Acá ya solo hay uno. Acá si se dan cuenta, hay este igual cómputo, funciona como si fuera un API. Pues sí, me va a tocar este balancear la carga dentro de este cubernetes. Ya, pero antes de llegar al cubernetes, cuando un usuario llega, lo primero es resolver la capa del front. Para resolver la capa del front, ¿qué tend ía que hacer primero asegurarme de que tenga un mecanismo por el cual puede identificar al usuario, ¿no? Que en este caso se está utilizando este producto que es entra Microsoft, entra B2C, ¿no? Que al final establece un punto de conexión cifrado del usuario hacia el cliente, hacia el negocio, ¿no? Punto de conexión asociado dentro de un directorio activo. Entonces, en otras palabras, le creas un usuario en tu catálogo a tu usuario para que él una vez dado de luego regrese con ese valor criptográfico de su usuario y puede iniciar sesión además de su usuario y password, ¿no? Entonces eso te permite tener usuarios catalogados dentro de tu directorio y que también tengan mecanismos de autenticación que incluso pueden estar apoyados con otros servicios como el de Authenticator de de Microsoft, ¿no?, que te envía un código OTP para el inicio de sesión o quizás hay unos ahora hashys que son unos llaveros que también se puede utilizar. como USB para iniciar sesión. Entonces, esos esos mecanismos de autenticación modernos se podrían aplicar a estos escenarios con el uso de este producto. Puedo usar usuario y password también, profe. Sí, se puede usar usuario y password. O sea, el objetivo es asegurar de que le des un perfil de seguridad a tus usuarios. En este caso está apoyado dentro de un directorio activo y con un mecanismo criptográfico de asociación a cada uno de los usuarios. Este es un mecanismo robusto. Ya habiendo resuelto eso, ¿qué debería hacer yo? Valor criptográfico me dijo, "Sí, que procede el request hacia acá en este producto que es front door, ¿qué tengo adentro?" No, primero tengo un content delivery network. Entonces, si yo tengo que entregar, digamos, recursos de este frontend como imágenes o cosas del streamlet, voy a tener una una subnet dentro de este front end que me va a poder tener como una carpetita de mis archivos comunes que son referenciables para la web. y que también se pueden instanciar y quedar aquí a memoria en como una memoria temporal. Entonces, al final de al el usuario está accediendo a una parte del del de la puerta de bienvenida, como dice el front door, donde yo pueda alojar ciertos recursos que son comunes para todos los usuarios, ¿no? Entonces ahí ya tengo ese content delivery network o content delivery repository. Después que tengo acá increíblemente tengo load balancing desde el inicio para las cargas del front, cosa que si tengo un ataque de denegación de servicio, también voy a poder validar de que no estoy ante ese escenario y si es que tengo ese escenario, activar, ejecutar algunas políticas de deflexión del ataque. Por ejemplo, la deflexión del ataque más común es bloquear IPs. Empiezas a bloquear las IPs de donde te están atacando, pero a veces el ratio de velocidad del front door no es tan grande como la velocidad del ataque. Entonces, acá se pueden apoyar de otros servicios como Cloudwatch, por ejemplo, que mientras tienes tu Asure avanzando con el bloqueo, Cloudwatch también lo puede estar haciendo alrededor. Entonces, paralelizas un poco el bloqueo ante una denegación de servicio. Y este además de esto tienes un web firewall, pues obviamente vas a tener un web app, va a tener ciertas configuraciones de seguridad de caja, pero también le puedes establecer un propio web application firewell, ¿no? Que te establece un punto externo antes de que llega al mismo web app que no se vea este vulnerado, ¿no? Este puede ser también un firewall de, por ejemplo, si ya tenías bloqueadas ciertas IPs, las empiezas a catalogar dentro de tu firewall como no receptivas para el tráfico y pues deniega el la intención de llegar al web y que nos habilite un protocolo de comunicación seguro, ¿no?, que va enforzar el que se comunique todo esto por medio de un canal https. Además de todo esto, tener una inscripción de canal de TLS1.2, que son los de caja generalmente de Microsoft. ¿Y por qué ese mecanismo de inscripción? Porque te ayuda a generar un canal encriptado de punta a punta desde el punto de originación de conexión, desde donde hacen el request hasta el punto que se genera el response y luego sale. Entonces, todo ese canal queda encriptado. Entonces, con eso hemos visto solo este componente que es front door. Front tiene varias cositas adentro. Luego pues tenemos el clásico web app. que esto si bien, o sea, voy a explicar primero asure, luego les voy a explicar cómo se podría manejar en en open source, en un premis y en otros vendos, ¿sí o? Entonces luego sigue el web app frontend. He puesto acá como un ejemplo Streamlit porque un no sé quién mencionó, no sé qué equipo mencionó también Streamlit. Streamlitad sí tiene sus templates ya como que muy parecidos a Open Aa Chat GPT y entonces eso ayuda bastante. Sin embargo, hay otros también de Oyama que es Open Web. que también es es muy bueno, te permite también hacer gestión de usuarios, te permite tener este un catálogo también de de cada uno de estos usuarios y que tú le puedas administrar. Esto podría correr también un Docker container muy aparte del web app, pero al final este web app lo que está buscando es este también podría ser un container, discúlpenme, también podría estar inyectándose a un contenedor, o sea, si quieren bajarse el contenedor de Open Web UI, también lo podrían este levantar acá y luego personalizar. Pero al final este web app es el responsable de ser el punto de conexión a la gente. Este es el punto clave en el cual yo me comunico hacia la gente. En otras palabras, desde acá yo llego al agente. Si este agente se cae, pues se murió toda la aplicación, ¿no? Entonces, ¿qué falta aquí? Un cierto gateway, ¿no? Falta acá un cierto gateway. O si es que tengo este un cubernetes, pues lo puedo poner con un mecanismo de replicación, ¿no? Que replique pots automáticamente, que haga upscale y luego que haga downscale cuando sea necesario, ¿no? Y de esa forma podría tener también un poco de redundancia para que el servicio se mantenga. Okay. Entonces, con eso, miren, del punto de consumo de la gente ya llegó luego el agente. Acá son múltiples contenedores que pueden subir y bajar en tamaño, este, todos ejecutando pues al agente creado en la donde aparece la capa de memoria, pues del chain con el conversation buffer memory o conversation memory buffer y estos que al final pueden implementarse luego a un quech for race, ¿no? Los podríamos manejar así, ¿no? Como que volátil y luego se persiste en temporalmente en raíz, ¿no? Otra forma de persistencia del historial de conversación que es la más usada es pasarlo a una base de datos no relacional como la de cos DV. Acá generalmente se utiliza el app de Mongo DV para persistir la conversación y obviamente el agente tiene que funcionar con su LLM hacia Assure Open AI, ¿no? Asure Open AI de caja viene con ciertos garrails, les había dicho, ciertos filtros de contenido, así que se pueden hacer se puede hacer uso de esos mismos garra ya, pero supongamos que no estoy allí. Puedo utilizar este framewor Sc abiertos como el de Garrail CI les había puesto referencia y ejecutarlos estos en otro contenedor, cosa que al momento de que llega request, lo primero que tendríamos que hacer es llegó acá el input, este input antes de ir al orquestador tendría que viajar a rail c para ver, oye, ¿puedo dejar pasar esto sí o no? Si es que es un sí, pues voy al orquestador, el orquestador recibe el input, ejecuta lo que tiene que ejecutar el el agente. El agendestrator, el orchestrator tiene varios nombres, lo van a ver en la documentación. Este le dicen router también, pero al final lo que hace es terrutea o si no orquesta la invocación a herramientas, a conocimiento, a todo lo que tiene alrededor del agente. Entonces tomaría el agente este input para generar su propio output y el output de nuevo regresaría al webup y el webup preguntaría a Garrail, "Oye, ¿podemos responder esto? ¿Sí o no?" Entonces, si es que es un sí, recién entrega la respuesta. Si es que es un no, se lo devuelve diciéndole, "Oye, acá ha pasado algo que no debió pasar." Eso lo persistimos acá en el Cosmos DV y deberíamos ya tener este los mecanismos claros de persistencia información.
Sí, Aldo,
ahí sí una consulta. Miren el Garay.
Ajá.
¿Por qué no se usa simplemente un este un pron, no? En en un agente y que vigile las
Es que sí, es ese es un mecanismo más robusto todavía que los Garretes. Es un mecanismo mucho más robusto y ese es un este AI judge. O sea, lo que tú me estás diciendo, ese es un AI judge. Ese es un AI Judge online. El AI Judge Online podría vivir por ya olvídate de que este Jud online este acá no hay G. Acá hay un AI online. ¿Qué harías? Lo mismo pues no llega el input. Dices al Jud, oye, nos acaba de llegar esto. Lo podemos atender. ¿Sí o no? Sí, que lo atienda y lo pas acá. Lo atiende como tal. Esto ha producido. Oye, ¿está bien lo que ha producido? ¿Sí o no? Sí. Y lo revuelven. No, pero acá gastas tokens. Ya cuando tú implementas car rails lo que utilizas son métodos de NLP que podrían estar siendo usados, por ejemplo, en frameworks como NLTK, como este todos los que son dentro del del esquema de ciencia de datos que van asociados a a la implementación de estos modelos o implementación de controles de basados en climatización, tokenización del texto para determinar si un contenido es nocivo o no. En otras palabras te estoy diciendo que acá es más ligero y no te cuesta token en el uso de SD CO de G raises. Ahora, ¿qué es más efectivo tener un AI jud? Eso sí, de todas formas esta es la bazuca. digamos, ahora, pero te cuesta no solo en tokens, sino también te puede costar en latencia, porque al ser un modelo el este judge, al ser un LM un poquito más grande del que está acá, pues sí se va a demorar un poco más que este. Entonces, se va a demorar un poco más en decir, "Ah, sí, mira, después de revisar y en base a a qué y empieza a fundamentar, ¿no?, todo lo que ha hecho el agente este de negocio y luego después de que revisa todo lo que es fundamentado, emite su opinión y te dice, revisando todo esto, me doy cuenta de que pues sí debemos atender o no rechaza la la atención. Entonces, esto podría tener un poquito más de latencia, pero sí es la bazuca, ¿no? Al final, si estás en un escenario, ¿no? En que tú necesitas, por eso hay varios que implementan el AI Judge, pero no para todo, lo implementan para escenarios donde hay una decisión crítica, por ejemplo. No, como definir la línea de crédito de una persona, te tienes que asegurar de que no hay accesos, te tienes que asegurar de que la información que ha provisto no sea malinpretado. Te tienes que asegurar de que los números en cálculos estén correctos y no sean hechos por el LLM, sino por una herramienta y que sean explicables en cálculo. Tienes que dar cuenta de que el judge necesita otras herramientas para revisar cálculos también. Entonces, como que va a tomar un poco más de tiempo, ¿verdad? Entonces, en esos escenarios en que le puedes decir al usuario, "Un momento, por favor, estamos revisando o estamos tom procesando su solicitud, digamos, mientras lo mantienes esperando al judge, lo mantienes esperando al cliente porque el judge está funcionando. Esos escenarios en que el cliente puede esperar, okay, pong un judge." Pero los escenarios en los que no, por ejemplo, muéstrame mi saldo. Ahí, ahí no puedes esperar, ahí tienes que ser superrápido. Ahí necesitas un tipo de control ligero que te ayude a salir, ¿no? Y también la la acción no lo amerita, ¿no? El tener un judge este que procese bastante tiempo solo si debe mostrar el saldo o no un cliente. Entonces, varios lo manejan como un escenario híbrido. Se apoyan en G rails propios de caja, digamos, o utilizan un framework de G rails y a la vez un chat para esos escenarios como que más intensivos. Entonces, al final se va como un híbrido. Puedes tener escenarios que tengan judge y a la vez te puedes engarrar para controlarlo.
Sí.
Okay. Okay. Entonces, al final, miren, esta solución se va a apoyar en los garrels para controlarlo, ya sea acá o en el judge, como habíamos visto. Y luego el siguiente componente que tiene acá en en El diagrama es pues el conocimiento, ¿no? Que habíamos hablado de un search para Asure para hacer el RAC. Nosotros hemos usado una base de datos vectorial con croma que tenía también mecanismos de búsqueda, los indexaban, ¿no?, que están justo los índices y estos índices iban asociados al conocimiento, ¿no? Que pu tener documentos, podría ser historia de transacciones que puede estar también indexada. Acá podría haber un py vector también. Entonces tenemos herramientas por este otro lado. Podría tener un tool, digamos, una herramienta que se invoca este una vez nada más y hay otras que podría tener otro tipo de funcionalidad, ¿no?, que puede ser invocada también por un método efímero. Hay otros que podríamos necesitar persistencia y ahí sí tendremos que cambiarle a otro tipo de infraestructura. Y finalmente, si se dan cuenta, acá no aparece más que la observabilidad al final. ¿Y por qué al final lo he puesto? Porque al final LMI es una plataforma SAS en Entonces tienes también otro key lado dentro del ecosistema LCH para enviar toda tu información hacia este espacio que incluso es el thread, o sea, todo el historial del de la comunicación, el chat y podemos ver también otros mecanismos de telemetría y reporting dentro de Lmith. Esto no quita que los usuarios se puedan apoyar en mecanismos de telemetría propios de la plataforma como este application insights. oir monitor, ¿no?, que son propios de la plataforma, jalan telemetría y ahí también podrían hacer la misma revisión. Ahora, ¿cómo manejamos esto sin que sea Asure? Pues sin que sea Asure, el la forma de manejarlo sería por proveer algún mecanismo de autenticación de usuario que sea seguro, ¿no? Un usuario, un password y quizás algún mecanismo de enrollamiento dentro de algún catálogo que tengamos o creemos como un directorio de usuarios, algo así, en alguna base de datos que nos permita asegurar que el usuario deja como que alguna cookie criptográfica o que administre este, no sé, un multifactor authentication, que le envíe un token a su WhatsApp. Recuerden que WhatsApp este Cloud Appi también ahora tiene el mecanismo de enviar tokens de inicio de sesión. Si han iniciado sesión con la TAM, por ejemplo, les envía un token también por WhatsApp o les envía por mensaje de texto, este infobip, este twilio, hay diferentes herramientas que hacen envío de OTP, o one time pass code y eso también podría ayudar a generar un multifactor authentication. Y el otro mecanismo que es el último que ahora se está viendo se le use Hash kiss porque dicen que este incluso ahora los OTP podrían ser reemplazables por el uso de ya que aprende, o sea, en otras palabras, si yo soy un atacante y veo que utilizas multifactor authentication, como que te voy estudiando a lo largo del tiempo, me aprendo todos tus tokens y luego con podría determinar cuál es el token más probable que luego podría seguir. Entonces, por eso es que dicen que no es muy seguro. Así que ahora están utilizando hashs. Los hashys es como un llavero criptográfico, ¿no? Es es lo mismo que le decía con B2C, solo que acá tiene una persistencia física, lo llevas como en un USB y luego lo enchufas. Entonces, Al final los hash kiss es otro mecanismo, pero proveer un mecanismo de autenticación de clientes. Ahora, content delivery network, load balancing and security. ¿Qué necesitamos acá? Pues habíamos hablado de un gateway, ¿no? Entonces acá necesitamos, sí, un AP, un API management o un API gateway. Este gateway que a la vez tenga un web application firewall o si no un firewall tal cual y configurar el firewall de firewell, ¿no?, que nos va a proteger para establecer políticas de denegación de IPs y por eso activar acá un Didos protection. Acá si obligado pondría cloud watch, qué estoy hablando este cloud fair se llama un servicio equivalente a cloud fair que te permita evitar esos ataques deción de servicio, ¿no? Cloudfare, entonces Cloudfare es una opción open source, digamos, o bueno, que no es asociado, es un servicio SAL, no es asociado, digamos, a un infraestructura de de ASUR, ¿no? O de cualquier cloud provider. Va a forzar enforce enforcement de https. Eso también lo logramos. Okay. Ahora, un web app. Supongamos que el web app lo corre un docker container. Este docker container puede estar en Heroku, en cualquier otro lado que no sea web. Eh, como de cloud podría ser un hero podría ser local. Si es local, no sé, un Apache Server, digamos, que corra Python. Apache server que corra Python, que tenga Streamlit. este que me permita gestionar contenedores, que me permita este exponer el API, importar este primero instalar Python este y instalar luego todas sus dependencias y exponer el end point, ¿no? El point para que luego el gateway lo le haga referencia y este a su vez consumir este lo mismo, ¿no? Un espacio de contenedores donde yo este podría consumir el end point de donde esté de la gente y en este point este, perdón, en este espacio de contenedores acá tener hacia la gente la la definición por medio del landin y que en vez de utilizar que chef Ris podría usar redis open source. Este hay otras formas de utilizar memoria volátil también este asociadas por archivos también Chroma también te podría utilizar aquí este cosmos DV pues mongo DV podría ser servir acá o cualquier otro tipo de base de datos. Una postgress también podría servir. Este si es open source o non prem acá pues tendríamos oyama o también hay unos ahora de hag face este también aparecían otros que estado viendo de para implementación de modelos en local digamos no podría ejecutar esto local y pues si hemos visto que Chroma nos ayuda con toda la base de datos vectorial acá también tendría a PG Vector como otra opción, Wavate es otra opción y así este podemos encontrar más alternativas en Prem. A nivel de G rails, pues es otro contenedor con el SDK de Garrails y este las herramientas pues estas pueden ser otros contenedores que están implementando cada una de las herramientas, ¿no? El servicio de observabilidad al en este caso es un SAS, así que aún así a pesar de ser este open source, yo lo mantraendría para dejar Langmid igual como capaz de de observabilidad. Podría tener otro tipo de información, sí, pero tendría que luego hacer mi revisión manual, o sea, en otras palabras, tendría que construirme toda la carretera de procesamiento de información y reportería. Así que ahorrarse un poco eso también ayuda con el servicio de LM. Ya. Entonces, con todos estos garabatos vamos a ir a otro garabato. Este es un ejemplo de este cómo lo podríamos hacer con estos que habíamos mencionado, ¿no? Ya tenemos memoria por template, los índices, la cadena agentes, los callbacks, todo esto que pertenece a un agente que se ejecuta por medio de la framework con Open AI como backend. Si se dado cuenta, estamos en un mecanismo de backend for frontend. Esa es la arquitectura de referencia que deberíamos seguir. Funcion con un front y con un backend for front end. Okay. Y deberíamos ser capaces de implementarlo. Profe, con todo esto, todo eso lo vamos a ver en caso. Sí, ahorita les voy a mostrar. Paciencia, please. ¿Cómo se maneja la información crítica considerando que el spint la información queda registrada? Ah, sí, esa es una muy buena pregunta. Deberíamos pasar por un PII detection o un PII strip. antes de de mandarlo a Lmit. O sea, por ejemplo, yo les había dicho, ¿qué hacemos con los datos TAC? Al momento de ponerlo en volátil, les había dicho que los Garrails acá al inicio pueden ejecutarse, ¿no? Entonces, yo podría establecer un garrail de reyex al momento de que llega acá el dato y luego lo voy a persistir. Podría enviarlo al Langmith así tal cual. Es una decisión que al nivel de la data sensible yo diría que no. Ya. Entonces, lo que generalmente se hace Es como que acá ya se va a persistir, ¿verdad? Acá ya se va a persistir y lo que habíamos revisado es antes de persistir había que anonimizar. Entonces, antes de persistir se pasa por un bloque de anonimización que generalmente está en este server de Garra, que acá están los PII strips, o sea, los que identifican datos personales y al final del chat le hace el strip y esto se maneja de forma asíncrona. Ya. Entonces, al final, cuando tú tienes este este punto de stream y se va a generar la respuesta, tú tienes que mandarle este PII strip para quitarle la data sensible y luego mandarlo al DP. Eso debería pasar siempre en todos los casos. ¿Ya? Entonces, con eso, justo en este diagramita, iba a mostrarles el código que es esto, revisión de LMIT y Streaml, que es lo que vamos a ver ahora. Ahora ya son 8:22. Vamos a revisar el código ahora y como siempre después de revisar el código vamos a hacer una pausa. Ya. Entonces les voy a mostrar acá el código. Sí, ejemplos de PI. Ahorita no he traído algunos, pero podemos luego revisarlos. No he traído ejemplos de PII detector, pero o sea, vamos a hacer algo. Voy a sacar de acá Mm hmm hmm hm. Voy a abrir esta parte de acá. Vamos a hacer un uso de copilot. Dentro de copilot vamos a hacer este ya eh genera en Python un script para identificar datos sensibles como DNI peruano, email y otro dato más. Número de celular al final, miren esto, si bien lo estoy generando, hay una web donde se puede probar que es Rey X101. Yo siempre he usado esta por décadas, esta web me ha servido muchísimo y es este basado en la comunidad, lo cual hace genar. Entonces, acá hay un montón de bibliotecas de cómo hacer un reyex. Por ejemplo, yo busco email, busco email. Acá me dicen, por ejemplo, conforme a los votos, ¿no? ¿Cuál ha sido como que el más popular? Y acá, mira, este, por ejemplo, maching email addresses hacia acá hay un estándar incluso de cómo hacer el match hacia un correo electrónico y acá es un montón de de código de de expresión regular. Pero al final expresiones regulares. Voy a poner una simple. Quizás este de acá. Le doy open en en Ya. Sí, le voy a abandonar. Y acá me puso el rex de por caracteres especiales, incluso. Qué locura por caracteres especiales de un correo, ¿no? De, por ejemplo, acá si le pongo, este es un Gmail. Si, si ven ahí. Entonces, lo que ha hecho es darme un match por grupos de información, el dominio, el nombre de usuario. Miren, si es que es de algún com es, por ejemplo. las extensiones del dominio. Entonces, al final te ayuda a obtener una visión de no solo un match, que podría ser mail este @domain.com. Tenemos ahí el el match, ¿no? El match un que te dice, "Sí, acá hay un correo electrónico en este piezo de texto, ¿no? Entonces puede venir un usuario en el chat y decir, "Oye, no puedo acceder a mi cuenta. Mi correo es eh mail @ domain.com y no funciona. Entonces, cuando pase todo este texto allí, nuestra labor es hacer el como que el split de todos estos. Acá mira, no no me está yendo el rey pero si es que acá te está identificando un único correo en el texto, ¿ya? O sea, tendría que hacerle el split, pero hay otros que únicamente buscan solo el correo sin importar este cómo empiece o cómo termine el texto. Y al final te hacen el highlight del correo, ¿no? Entonces, por ejemplo, vamos a buscar otro. Yo vamos a buscar otro de correo. Este es inmenso. Opening montage. Hay un hay varios ejemplos de correo. Estos son codificación y conforme el lenguaje, ¿no? Si lo quiero también en P. Y ese es este diferentes formas de ver el correo. Ya esta se ve interesante. Esta es para me parece. Ajá. Y acá te dice match de PR possible. Ya. Okay. Y acá te explica más o menos cómo cómo está haciendo el match. Entonces si yo le mando esto, debería buscar acá. Este es en webs, por ejemplo, links. Y al final, como les estaba mostrando, vamos a generar un rey Email check. Lo busco acá. Miren, Google ya me dio acá este Reyx. Poner acá. Ya ahí está buscando algo que empiece position to start of line y acá le voy a poner que busque nada más sin importar donde esté el email@ domain.com. No, no el rey pero ya es más o menos esta es la explicación. Acá está buscando un correo. Vamos a ver si así tampoco tampoco lo ejecutó. No está haciendo el match a pesar que lo dijo. Entonces acá inicia como que el posición de la cadena de texto. Acá está identificando cualquier letra de la A a la Z mayúscula o minúscula, 0 o 9 y que tenga después un y que pueda tener un guion. Pero que no tenga más ni menos. Va a ser match en la primera parte. La siguiente parte es un @ seguido igual de un a la z con un a la o mayúscula o minúscula o un número o un guion y que luego va a estar seguido de un punto o una sucesión de dos. Ah, es que acá aparece justo dos letras de la A, la Z. Ya como términos. Creo que acá debería ser tres. Sí, tampoco, pero este estos reyes al final lo que deberían buscar es este justamente cómo implementarlo en Python, digamos. Acá estaba. Ajá. Acá, por ejemplo, hay un Reyx que está haciendo un match hacia un correo. Este es el correo. Entonces acá nosotros validamos un poco cómo va a ser el input del correo y esto lo podemos sacar y stripear, o sea, de la burbuja. Este, lo que debemos hacer es netamente este plantear de esta burbuja un replace de esta etiqueta hacia el nuevo texto que vamos a poner. Entonces, de la burbuja le damos replace a este texto y le ponemos entre corchetes email, por ejemplo, ya lo podemos empezar a manejar. Entonces, por ejemplo, acá detectar datos sensibles en Python y acá Por ejemplo, un patrón de patrones. Acá aparecen los patrones de cada uno. Por ejemplo, un DNI peruano, un email, un celular peruano. Entonces estos al final en texto, ¿no? Mi nombre es Juan Pérez, mi DNI es este. Este, ¿cómo lo empezamos a reemplazar? Podríamos tener acá una etiqueta en vez de que diga este nombre del patrón. Reemplazamos los valores por etiquetas dentro del mismo texto y luego eso sí lo mandamos hacia Lchain. Por ahora en este ejemplo no he hecho la implementación de PII, pero la siguiente clase se los puedo mostrar al inicio de la clase. Ya. Entonces, vamos a ver este código que lo que está haciendo es tomar un poco las implementaciones que habíamos visto de primero, ¿no? Tener unentic rag. El agentic rack era el caso este de los restaurantes que los reviews de restaurantes y obtenía calificación del cer al 10, resumen de review, plata estrella, distri, todo tipo de comida, ¿no? Inclusive le hemos puesto un base modelic con descripción del plato, recomendaciones, referencias que tools había usado, ¿no? Y lo primero que estábamos haciendo acá era primero este agente ya le puse ya no no solamente una herramienta, sino le he puesto dos herramientas. Entonces para ver estas herramientas vamos a ir acá vector La primera es que cargue los reviews, carga las reviews de la data CSBV, la CSB. Le habíamos puesto en ese punto del tiempo las opciones de Oyama o de Open AI embedings y pues acá los pone dentro de de la base de datos de Chroma o como restaurant reviews dentro de collection Restaurant Reviews y el DV location está seteado como Chroma Open AI rest reviews que es este de acá. o si no o llama reviews, que es este de acá. Entonces el de el seteo de herramientas primero es obtener el rack directamente ejecutar el reteneration con esta función, ¿no? Como un tool rock as tool, digamos, el rock como si fuera una herramienta propia de la gente. Y la otra en este caso este en el caso es me conseguí esta URL, estaba revisando un poco dentro de mesa 247 y encontré de que había, por ejemplo, la ubicación ID7 correspondía a Miraflores y este tenía una paginación de 25. Entonces, si si buscan este esta URL acá este llegarían a obtener este esta información que al final es data de restaurantes que te dicen este si tiene qué tipo de restaurante es, si ha salido en Zum o no, eh si tiene un precio promedio más o menos de un plato. este si tiene delivery propio, ¿no? Este tipo de de escenarios, ¿no? ¿Qué tipo es? Fish and seafood, nombre de comida, provincia de Lima, Distrito Miraflores pude más o menos acceder a esta información de de mesa 247. Entonces dije, "Ey, si esto está disponible, ¿por qué no usarlo?" Entonces si si es que, o sea, acá yo no le he dicho nada de que le dirija a esta herramienta en la funcionalidad, simplemente le he dicho que esta herramienta 247 buscar en mesa 247 una herramienta para listar restaurantes en Miraflores, Lima a través de mesa 247, ¿no? De entrada es un tipo de comida, por ejemplo, ceviche, marina, o sea, para que busque dentro de esa información lo que hace es netamente buscar dentro de edadings, o sea, primero establece la conexión, hace el web base loader y carga dentro de Chroma Div mesa 247, que es este de acá, eh, carga todo lo que jaló de esa URL y lo setea, ¿no? Pero sí, cada vez que invoca la herramienta lo vuelve a setear y lo vuelve a setear. Puede establecerlo de otra forma para que lo sete una vez y luego buscar en la base de datos para que sea un poquito más rápido, ¿no? Entonces al final eh luego ejecuta el query de disponibilidad restaurantes especializados en y el foot type dentro de del querer y entra la información en Jason y acá establece, ¿no?, los rangos de similaridad. Entonces, si esto lo vemos dentro de entic Este lo debería ejecutar. Este, a ver, un momentito, porfa. Cerrar estos dos en un nuevo terminal spash. Ya está. Y vamos a entrar Python. Y este esic rock. Entonces, preguntas sobre este restaurante esas Lima, ¿no? Este, quiero que me recomiendes eh un buen ceviche eh Miraflores. Entonces va a entrar al Executor Chain y ya está usando acá mesa 247 como herramienta con ceviche. Entonces, lo que A ver, vamos a verlo. Est completa acá está jalando resultados de 24 Mesa 247 y del retrieval ahí lo jaló este nombre de restaurant en la mar y acá los reviews también, o sea, ha jalado los dos reviews y mesa 247. Y acá está elaborando ya su respuesta, ¿no? Y acá me dice tools usadas mesa 247 y el retrieval tools. Uso los dos, ¿no? El ceviche es un superable y acá me da referencias, ¿no? De de donde donde ha puesto este sus referencias, ¿no? Vicente Bisla Yumadre antojados, ¿no? Las referencias te recomiendo el ceviche clásico de la mar que tiene una calificación de nueve. También puedes considerar el ceviche de bonito del mercado y una calificación de ocho, ¿no? Entonces este es como se maneja el agente tal cual. Y ahora lo que vamos a ver es esto que lo hemos probado, así como agente, cómo lo puedo pongo en modo solución, digamos, ¿no? Entonces, para eso tendríamos que pensar, oye, lo tengo que empaquetar, no lo tengo que poner disponible para que sea lo usable por otro, o sea, como que un contenedor. Entonces, esto como que hay que volverlo una clase, ¿no? Al final. Entonces esta es la el modo clase de de este agente, ¿no? Entonces acá tenemos un restaurant agent y el restaurant agent se define al final con un model de finition. Va a tener su seto de temperatura. Igual va a cargar los keys desde load. B este sí está importando las dos herramientas está utilizando la chain agents executor bytic, lo que hemos visto justo las sesiones anteriores, ¿no? Y el conversation buffer memory también. Entonces al final cuando creamos el prompt pues es el mismo system prompt igual los templates. Y acá necesitamos que justo al momento de inicialización se está establezca también el prom de la gente, ¿no? Que es justamente estos dos primeros steps, ¿no? Si ven este se invoca acá en el self justamente y luego se le agregan las tools, ¿no? Como create open tools agent y ahí aparece la definición de herramientas, el prompt y el LLM. Y luego el invoke agent. Al final lo que vamos a hacer es traspasar las invocaciones que tengamos, o sea, sumamos un poco el diagrama. Si sumimos el diagrama, yo tendría que tomar lo de Streamlit y enviárselos justamente a este point o digamos en otras palabras establecer como un objeto del del tipo agente para empezar a enviarle, ¿no? Entonces necesitaría invocar ese agente. Entonces si yo quiero establecer esto como class, necesito un front. Entonces este es un primer ejemplo de front, ¿no? Que igual utilizando Streamlit vamos a hacer un load este de las de las aplic de los kiss y este caso es un paso a paso nada más ya pero acá está mostrando cómo poner un hacer un chat básico, ¿no? Que es el chat Open AI. Establecemos su temperatura, modelo y acá le ponemos un título nada más, el nombre de la página y acá le se guardan se van a guardar algunas variables dentro del dentro de Streamlit, ¿no? Que es con el session state. Acá se va a guardar open model. el seteo de la memoria, conversation buffer memory y este también este los mensajes, ¿no? Que en este caso roles si están contentos restaurantes con una pregunta se ve ya y acá está netamente el display de los mensajes del del por rroll y el contenido. Y este es netamente para customizar el ask me anything del chat input y este por cada mensaje como se va a mostrar en pantalla y cuando dice el spinner, ¿no? Al momento que tenga thinking y acá este el session el dentro de session como vamos a poner la las respuestas de cuando hagamos este predict. Entonces si yo hago esto en stream blit, si se han dado cuenta, no he invocado la clase aún, pero voy a obtener una funcionalidad genérica. Entonces voy a poner nuevo terminal, voy a poner bash. Okay. Y este bash me va a permitir este voy a tener que hacer streamlit run y el nombre de front.pway y ejecuta en local host esta solución. Entonces le voy a decir, "Hola, este, recomiéndame sobre ceviches el thinking lo respondería, ¿no? Con bueno, este es no es mi agente, no es el agente que hemos visto, pero si no es de conocimiento general en México. México se dicho pueden ir ya, pero o sea, más o menos esa es la respuesta de un Lelm genérico, ¿no? Genérico. Entonces, si yo quiero ver un poco qué pasó ahí atrás, este, puedo revisar acá mismo en la consola y aparece conversation conceiendas y respondió al no. Entonces ese es una forma básica, ¿no? De de implementar este streamlet. Hay diferentes controles dentro de Streamlit y pueden este buscar justo los de hay templates justo de justo este parecidos a al chat app de Open AI en Chadle GPT like clone. Este pueden utilizarlo por medio de L chain, por ejemplo, buil map using L chain. Y esto es como que super embebido, que es lo que acabamos de ver acá. Incluso tiene una cajita para que le pongas tu key pruebes ya. Y no pongan su key acá obviamente ya. Pero acá te dice cómo cómo seguir, ¿no? Pip install stream y l chain open. En este caso y acá te aparece, ¿no? el código de cómo seteas tu formulario, digamos, ¿no?, de de Streamlit. Acá le estás definiendo, por ejemplo, el el key net con el dentro del sidebar para que tú pongas ahí tu password dentro del key. Luego este genera la invocación al al modelo que nosotros tenemos como Llón, ¿no? El invok, ¿no? El invoke text y ahí lo pone y este es el formulario que My Form este le ha puesto como formulario a este a este apartado de acá como si fuera un inform lo seteas y ahí aparece please enter API K una vez que lo seteas este netamente acá está escribiendo ya cada una de las partes es como estableces títulos digamos o pones el sideb el cómo generar respuestas y luego como lo evocas para deployarlo. Y bueno, esa es un poco la respuesta rápida, ¿no? Ahora, el extender esto hacia la forma que hemos visto de clases nos implicaría instanciar el streamlet que instancie justo el agente en forma de clase, ¿no?, que vendría a ser ya la siguiente este siguiente pedazo de código. Entonces, en este caso donde dice front inch ya es integrado. Entonces, yo debería luego poner cada uno de estos en un doctor container, digamos, y poderlos invocar por APIs. Bien, ya no tanto este por objetos, que es lo que estoy haciendo acá entre clases inicialmente, pero luego se podría implementar como un fast al final. Entonces el stream lead en este caso va a instanciar a restaurant agent. Ahora lo primero que hace es establece el título y ya no me olvido ya de cargar este variables y solamente le digo local agent. Este local agent le seta igual el espacio de memoria que estábamos usando y acá esto es netamente del de stream, el session state. Esto al final lo podría pasar a dentro del buffer memory y por eso se ha seteado acá como memory nada más, pero esto de mensajes también se podría actualizar como el estado de los mensajes que del front pasan al final dentro del buffer, ¿no? Entonces cada este mensaje vamos a tener el grite en la visualmente en la pantalla y este acá un poco la personalización de los mensajes, ¿no? Este es el hazme de tu hazme question es este de acá y después le establecemos un unos roles, ¿no? Dentro del chat message para escribirlo el la invocación del del modelo y su respuesta. Pues ahora lo vamos a ver. Pues de nuevo stream blit run y vamos a hacer front inty. Y ahora se levanta estado. otra opción ya con el agente que hemos construido. Esto me sirve para luego mostrarles algo siguiente. Ya. Entonces, igual recomiéndame sobre Ya vamos a poner homo saltado en Jesús María. Entonces aquí está instanciando uso mi propia agente y lo está invocando ahorita, ¿no? Conforme lo que tenemos en rack y lo hemos saltado con papas nativas fuego, presentación moderna recomendaciones, ideal para ejecutivo, platos gigantes, sabor casero, la nacional y el tronco, no me aparecían ahí como alternativas. Bueno, ahí este hecho netamente el Ring lo ha entregado conforme lo que le había pedido en Pidentic. Entonces esto al final podría ser engranable para otro agente, ¿verdad? Entonces, ¿qué pasa si esto lo volvemos un asul? O sea, que yo podría utilizar este mismo agente como si fuera una herramienta y luego lo engrano hacia otro agente, ¿no? Entonces, por ejemplo, si yo quiero generar este ciertas referencias eh de estos y publicar algo como, no sé, generar un tweet, por ejemplo, de de o como que el footer de un post de Instagram podría generar generarme con un agente la imagen y con otro generar como que el el texto y luego ponerlo en la publicación, ¿no? Entonces, eso mismo es lo que estamos viendo justo en la sesión pasada. Entonces, quería revisar un poco Lit. Ya ya hemos visto Streamlit para utilizar L Smith. Este, primero es crearse una cuenta de L Smith. Si se dan cuenta, acá yo tengo un Foody Agent. Este foody agent ya tiene varias interacciones acá y hay algunas que han demorado más o menos buen tiempo y esto creo que ha sido porque me he ido al pues al al Chroma que tengo acá local, ¿no? Podría mejorar un poco ahí la respuesta. Y si se dan cuenta, voy a poner un poquito más el zoom para que puedan ver qué es lo que tengo acá. Yo he entrado aent executor, ha sido este, ¿cuál es? Quiero ver cuál es el último. Acá tenemos un time stamp. O sea, ¿qué qué hice para llegar a este? Ya, primero debo mostrarles eso. Cuando yo llego a a mi home dentro del Lit y creé mi cuenta, te van a salir estos tres de acá, que es get started with tracing, the bagerate in studio o experiment in playground. Langsmith también me ayuda a hacer el debuging de Langraft, por eso también aparece acá este agent tools y también podría ser hacia otros agents. Recuerden que el Dmith también ofrece observabilidad hacia multiagente, que es lo que al final vamos a ver con Langraft. ¿Podríamos ver Landraft para un single agent? Este, yo creo que sí, pero la verdad es mucho más grande Lraft para comparar para ponerlo a prueba con un solo agente, ¿no? Yo creo que debería ser para un multiagent al menos. Entonces, tenerlo esto para un solo agente como que me parece demasiado. Entonces, cuando ustedes quieran empezar con el tracing, hacen clic acá, generan su IP y pues simplemente conforme lo que vayan a hacer integraciones. En nuestro caso es más L chain, podrían usar un open AI SK, inversal AK o bueno, quizás otro tipo de integración. Siendo LChain, una vez creado su IP aquí, pues este tendrían que establecer acá Python y pues se se genera pues una instalación de los requirements de Lchain, Lchain Op. ya lo hemos estado utilizando bastante tiempo. Lo que sí tenemos que establecer en nuestros nuestras variables de entorno es habilitar LMI tracing, el L point de Lmith, el keyo. Y el open AI key, pues es algo que ya deberíamos estar ya administrando también una vez establecidas estas configuraciones, luego este pues es ejecutar netamente el agente. No he tenido que agregar algo adicional para LMI, si se han dado cuenta. En este ejemplo que es el front de integración, pues he tenido no he puesto ni nada adicional para el LMI y simplemente con el cargar las variables y tenerlo ya en el root se ha establecido la configuración del proyecto. Ahora, en este caso el nombre del proyecto es PRHalthy folklore 22, ¿no? Pero yo le puedo cambiar el nombre del proyecto en mi en mi definición del punto inbunto de variables y ya poderlo este utilizar de esa forma. Ya. Entonces, una vez creado el IP key, pues ya lo podríamos implementar en el código, ¿no? En este caso está haciendo from launching. Create en agent y acá dice get weather, le está poniendo el agent y simplemente le está dando invoc y acá le está poniendo tools como geta you a helpful assistant. Entonces es una una la forma que tenemos ahora de invocar a agentes justamente y con eso estableces tu proyecto. Entonces, yo he creado este proyecto que se llama Foody Agent Peru y acá aparecer cuántas veces se han ejecutado un error rate. Ahí aparece ahorita cero en 7 días o también lo hemos ejecutado poquitas veces y a la vez te puedes crear dashboards propios, ¿no? Te puedes crear un espacio propio. ¿Qué cosas trae LChain al costado? No, primero Primero puedes hacer trazabilidad de tus proyectos. Puedes establecer un set de monitoreo con un dashboard que acá me parece con un dashboard. Puedes subir tus datas de prueba y un poco hacer experimentación si es que tienes acceso al playground y puedes obtener el historial de tus prompts y puedes acá hacer un tipo de historial o gestión de tipo de control de versiones de tu system prom. Podrías tener más de un system prom y llamarlos por medio de un webhook para tener el system prompt. Entonces, Al final te sirve como una bóveda para el system prompt y eso se engrana un poco al playground donde puedes seleccionar un prompt de los privados o los públicos y probarlo frente a un modelo. Obviamente acá tú tendrías que establecer una configuración de Open AI para poder seleccionar el modelo, configurarlo y tenerlo a manera de playground. Esto generalmente sirve bastante bien si es que estás utilizando modelos de Open AI o cualquier otro que no sea un premis. Acá puedes establecer el tipo de formato, este, puedes establecer este configuraciones de herramientas, ya se puede hacer todo este tipo de evaluación dentro de estudio. La idea es justo revisar los escenarios de Lraph que por ahora no los podemos a ver y hay un waiting list para agent builder dentro del LP, que la verdad va a ser muy parecido a la integración que hoy hay con el. Entonces, dentro de este proyecto y si regresando a esta parte de observabilidad Vamos a ver que dentro de este espacio de food g han he tenido varias ejecuciones, ¿no? Y acá pueden ver que está el input y está también el output, pero el output como que se ve medio truncado. Entonces en este puedo ver porque acá, por ejemplo, se me ha estimado 17 segundos como un tiempo elevado, ¿no? Por eso aparece medio rojito el time, o sea, ¿qué es el primer dolor? Ahorita es el time to first token. El primer token es el que está doliendo. Entonces está llegando a doler la llegada hacia el primer token. O sea, el primer token que llega está demorando 2 segundos, ¿no? Y mi latencia es de 17, por eso le apunta en rojo, ¿no? Tipo chain. Y acá me muestra cuánto he consumido en tokens. Creo que también había una preocupación de cuánto consumo en tokens en cada uno. Entonces de esta forma ya me permite saber, ¿no? Esto es conforme los costos que se tienen de Open AIT directamente, ¿no? Entonces, por eso me sale ya este costo de de esta interacción, ¿no? De input, de output también. Pues de input, miren cuánto tuvo, 2,931 tokens y de output tuvo 139 tokens. Y acá luego veo esta cadena de invocación, ¿no? Dentro del agent executor, ¿no? Chat open AI y acá retrieval este retrieval tool y el mesa 24 list tool. Entonces acá está entrando a la primera invocación que es esta de la de la la mesa 24 Tool y acá está justo la descripción, ¿no? Esta herramienta lista restaurante es lo mismo que puse en el código y acá muestra el input, ¿no? Es un experto y este es el system, ¿no? Al final, o sea, en sample for the esquema esto es traducido netamente de pidentic properties full id f list y acá require object esquema y hacía como un ejemplo le da, ¿no? O sea, esquema se lo pone para identica, que es todo. el output que le dije para Pythic. Ya. Y ese es el output que hemos estado revisando ahora. Luego se invocó a la herramienta mesa 247 que tuvo una latencia de 7 segundos. O sea, el rag el rag se se demoró 7 segundos al final. Ah, no, perdón, la invocación hacia Claro, la invocación al web que luego al final hizo el embeding del RAG, o sea, lo volvió a indexar y se podía haber reducido un poco ese proceso quizás no para que no lo vuelva a generar, a ponerlo como chunking y luego vector en base, toda esta información. Entonces eso sí se pudo aligerar y ahí me ve reducido un poco la latencia acá, ¿no? Que está en 6.9 segundos, casi 7 segundos. Luego invoca a chat open AI y ahora este le está agregando el invocación, ¿no?, del lomo saltado de que, o sea, al final todo lo que sal del homo saltado y dentro de eso también está agregando en la cadena lo hemos saltado en Jesús María como input al retrieval tool y salieron estos no la nacional calificación resumen saltado. Mir este es de San Isidro pero aún así lo indexó era justo de esos temas que me decían, ¿no? Este San Juan de Miraflores, San Miguel Isidro acá el index fue netamente a buscar este por contenido semántico el review del homo saltado y no tanto la ponderación del distrito. Entonces aquí el vector store retriever me va a dar un poco la visualización de cuánto demoró y este retriever cómo performó al final en obtener esa información. Y acá el cierre, digamos, ¿no? Ya teniendo las dos respuestas como elabora tu output final y acá el output conforme esto cercano. Bien, entonces este es como una revisión dentro de este trace. A la vez eso yo le puedo ver como threads de ejecución. Si es que yo puedo configurar acá un thread. Los threads al final son manejados por código, pero ahí si ya importo LMIT y empiezo a crear threads de ejecución. Y ahí es donde podemos tener esa personalización de, por ejemplo, invocación hacia hacia el LMI, pero ya con la información, digamos, procesada, ¿no? Sin esos datos sensibles. Podría guardar ahí los threads. Hay este evaluators que se pueden ejecutar en desde el inicio. Evaluaciones que pueden llevarse para obtener feedback a lo largo del tiempo de su solución, incluyendo acá el Lludge que habíamos hablado también. Entonces, eso también lo pueden establecer y ejecución de automatizaciones, ¿no? Si es que quieren automatizar este algunos escenarios de ejecutar, por ejemplo, trigger de un web cada vez que llegue data a ese tipo de escenarios. Con eso hemos visto este la parte de tracing y en dashboards este acá se establecerían se pueden pinear este diferentes tipos de de escenarios al chat, o sea, puedo crearme un chart que establezca cierto espacio de monitoreo, pero hay unos predefinidos. ¿Cómo deberíamos pensar sobre el costo de monitoring? Ahorita todo esto me está saliendo en free, pero sí hay ciertos escenarios que me pueden costar más. Generalmente van asociados por el costo hacia persistencia de información a lo largo del tiempo, ya, pero ahorita está justamente seteado con el el premium, digamos, ¿no? Después este si es que yo quiero tener mayores integraciones, si es que yo quiero tener mayor persistencia información, pues va a costar un poquito ya el servicio de LMI, que ya es el pagado. Y acá estamos viendo un poco el cómo se ha dado el la ejecución, ¿no? Los traces, cómo han ido creciendo acá los traces de latencia, el error rate, el LM latency, acá este justo de las dos invocaciones, dos runs, el count que se expuso que había un aumento de costo tokens y demoro un poquito a veces en cargarles, voy a ser sincero. Entonces, hay momentos en que tú le ejecutas, tienes que esperar más o menos unos 5 o 10 minutos para que cargue todo, pero estas son de las ejecciones que como vamos a hacer. Mir, por ejemplo, acá hay una del 1 de la tarde, uno cerca las 9. Es hace un instante, pues, ¿no? Y bueno, acá podrían tener más métricas y algunas de estas pinearlas, volverlas como pantalla completa para poder seguir revisando este cada una de estas métricas, ¿no? Al final este es el default y yo podría customizar uno para Foody Agent. Se pueden setear alertas para el proyecto, por ejemplo, para Foody Agent. Puedo poner un alert trigger. Al final el alert trigger puede tener enrolados para esta alerta, por ejemplo, en número de errores o de latencia, digamos, que í establezca, si establece cierto cierto threshold de latencia. Entonces, y puede podemos tener este integraciones a pager duty en este caso o un webbook donde nos diga, "Oye, acá pasó esto." Y que le envíe el request, ¿no? Y de esa forma creo la alerta. Podría ser un request de, no sé, un point de envío de correo o envío de WhatsApp, envío de un slack, o sea, hay diferentes mecanismos de integración ahí para para webs. Entonces, con eso es un poco la revisión que quería que hagamos en relación al smet stream lit. Hay otro apartado que quería revisar que era el de Rails. AI. Ya lo que vamos a hacer es este antes de continuar con Garrails eh primero quiero recoger preguntas antes de pasar a una pausa. ¿Cómo deberíamos pensar sobre? Bueno, es justo lo que habíamos hablado hace un instante. Ya si no hay consultas creo que hacemos una pausa de 10 minutitos y continuamos con lo último del la clase de hoy. 10 minutitos, muchachos, y volvemos entonces.

Todos los muchachos están ahí. Sí. Okay. No sé por qué estoy medio en otra cámara, pero al menos hay cámara. Voy a proceder ahora a compartir el pantalla. Es lo último. Ya deberían estar viendo ahora la PPT casi final. Entonces, eh arquitectura de Agent Tool en esta segunda parte y vamos a ver rapidito mult. Pues es el primer step del multi. La siguiente sesión sí es multidon. Entonces, el primer step, si es que yo pienso en multiagente, es tomar este justamente esta solución que ya habíamos revisado, esta solución donde tengo yo este un entra ido, tengo un front, bueno, tengo estas capacidades de infraestructura, tengo este web ya conforme lo que habíamos explicado y este después de haber implementado de SOL. Luego me preguntaría, "Oye, yo tengo un proyecto donde hay más de un agente, ¿cómo hago para que más de un agente pueda implementarse en esta condición?" Entonces, para pensar primero en un mecanismo en el cual yo podría sin tener que usar Lp, apoyarme solamente Lin y hacerlo, es pensar que mi agente es una herramienta. Entonces, yo podría tener una herramienta que es otra LL. Entonces, puedo luego actuar net este mecanismo react, pero también apoyado de un element que sea un tool. Entonces asuma que en vez de una herramienta, tengamos un instanciado herramienta como parte del método y este mismo ser declarado como agente herramienta que otra agente. Un agente por ejemplo que es lo hemos puesto como una clase que este luego sea instanciado en otra. Esperemos que un agente así se defina como parte del contexto del del anterior por medio de una herramienta y así empezó la colonización. Entonces, si lo veo esto a nivel de infraestructura más o menos sería algo así. Entonces, en este escenario al final yo tendría lo mismo que hemos visto en esta parte, este Streamlit, ¿ya? Y este lo que va a ser más crucial es la parte de integración herramientas, justamente acá donde podríamos definir un point de R agent. y mismo encapsular este agente como si fuera una herramienta del anterior y a la vez internamente podría estar invocando a otro point de open y como que acá el point 2 digamos point 2 y acá el point 1 podrían ser el mismo modelo, podrían ser modelos diferentes con este conforme la tarea nos lo demande pero de esa forma yo podría tener lo mismo que hemos visto como la sesión de una clase de invocación, digamos, este, esto lo podría tener igual de un docker y lo mismo lo podría implementar, ¿no? Dentro de un docker file, un docker container expuesto que al final sea este un API, ¿no? Un API que luego al final lo puede invocar. Entonces eso es netamente una invocación de agent tool, donde tomamos la misma aplicación de agente de otro agente, digamos, que esté asociado como grider o editor o review. o critic para luego implementarlo de una solución multiagente solamente con Lang Chain y Lmit como punto de recolección de observabilidad también. Obviamente acá tendríamos dos proyectos diferentes, ¿no? Uno propio de este proyecto y otro propio de este contexto de proyecto, ¿no? Este como de subagente y este el agente supervisor podríamos decir, ¿no? O el agente orquestador de otros agentes. Bien, entonces este lo que toca en esta parte de la sesión es de que es aterrizando proyectos dentro de Visual Studio Code. He visto que algunos ya habían tenido un buen avance en su proyecto. Ya. Este, entonces lo que vamos a hacer ahora, no sé si por ahí estás Augusto.
Hola. Hola. Aquí estoy.
Sí. Ya. Lo que vamos a hacer ahora es este porfa, ponernos en grupos para Okay. Si no lo tengo implementado todavía como parte de código es empezar a más o menos plantear un diagrama en el cual podríamos tener la instalción de el agente y qué cosas vamos a necesitar alrededor que no son del agente, sino a nivel de infraestructura. Plantearlo eso y después este de haber sido planteado, ¿cómo lo podríamos ir atacando el código? Si es que tienen código, genial. Y sobre ese código podemos empezar a elaborar algunos cambios que nos permitan este percibir, ¿no? Oye, esto que habíamos implementado con agente, esto lo podemos poner en el formo de una clase y luego al final esto se puede poner como un método como fastid, digamos, para os ar empezar a publicarlo, ¿no? Entonces, si es que me ayudas porfa, a ponernos en grupos, vamos a estar haciendo esta actividad unos 15 minutitos.
Serían los grupos de trabajo.
Sí, sí, por favor.
Entonces, ya creé la salas, chicos. Por favor, levanto ya la sala.
Sí, sí, sí, porfa.
Estoy abriendo la sala, chicos. Por favor, únanse a la sala. de su grupo, ¿no? Sala uno, grupo uno, sala dos, grupo dos. Vayan uniéndose a las sesiones correspondientes.

Buen ¿Qué tal? Buenas noches. ¿Querés hacer una consulta?
¿Qué tal? Sí, sí.
Eh, con respecto a la tarea, eh, quería preguntarte algo. ¿Cómo cómo hacemos? Eh, o sea, obviamente está he estado tratando de todas las maneras posibles para que cuando Cuando yo corro el programa, eh el el o sea, hagan lo que le pido, Cristiano, pero el problema es que como no es determinista el LM, a pesar de que le bajo la temperatura al mínimo, a veces el programa corre bien y a veces no corre bien por podría ser porque me pone unos corchetes donde no van o algo así. A veces llama las herramientas que quiero y a veces no. Y a veces sí el proceso, a pesar de que trato de decírselo de la manera más clara en el system prompt, eh trato de he tratado de ser clarísimo con los Tools, he jugado bastante con ello, pero igual no no es determinista, o sea, no es obvi tampoco espero que sea determinista como código, pero al menos esperaría que sí corra más o menos bien, ¿no? Cada vez que lo que corre el programa.
¿Y qué dónde estás ejecutando? O sea, ¿hacia qué modelo?
Como estoy usando ahorita eh Qu lo estoy tratando de correr de manera local.
M, ya mira, va a depender mucho el modelo, la verdad. O sea, yo te sugeriría que, o sea, probemos. Sé que parece un poco abrumante, pero al final este el modelo que mejor busque la participación de la conversación es el que te permita tener ciertos parámetros que te ayuden a administrar mejor la memoria, el context. este diferentes parámetros que te ayuden a persistir mejor el contexto, porque lo que por lo que me dices es un problema más que todo de establecer el contexto a lo largo de la conversación. Entonces ahí yo creo que fácil falta algo de gestión de memoria. Entonces, si es un poco de el balance entre el context window y la gestión de memoria, yo buscaría un modelo que me ayude a tener un contest que no sea pues No, pero al menos este me ayudó a soportar un poco la longitud de la conversación, la longitud de quizás el los puntos de inferencia que se haga por recuperación de información y herramientas más la gestión de la memoria, ¿ya? Y a la vez ese el recoger el feedback es importante, ¿no? Entonces si es que ahora te apoyas del LMI, podrías ver, por ejemplo, ese caso que falló, que fue estrepitoso, digamos, el cómo falló. ver si es que lo podemos actualizar, no solamente en en base a ser más específico, sino que quizás tenemos que escribir lo mismo, pero de otra forma, quizás en una forma quizás estructurada, ¿no? Dentro del prompt, dentro de las instrucciones, quizás en el output ponerle un parcer, ese tipo de escenarios podrían ayudar. Yo revisaría un poco la gestión de memoria, el contexto, si no probaría otro LLM que podría tener quizás el context window para el caso que estás viendo.
Entiendo. Entonces, jugar un poco con los proms, verlo de la memoria y al mismo tiempo probar otros elements.
Sí, sí.
Listo, gracias.
Okay. Okay. Bien, muchachos, estamos llegando ya al cierre de la sesión. Este, el día de hoy hemos visto justamente un poco lo que no es este agente y hemos estado revisando el cómo implementarlos, ¿no?, en diferentes escenarios. Ahora, lo que nos va a tocar en la próxima sesión es ya tocar el tema de multiagente y allí vamos a tener que buscar un escenario de implementación con APIs, un escenario de implementación que nos lleve justamente a tangibilizar fuera de esa infraestructura la invocación de de estos antes mencionados. Entonces, como tarea a nivel grupal. Ya esta revisión que hemos hecho ahora a nivel de el planteamiento de arquitectura, ya pensando en la infraestructura, este, vayámoslos tangibilizando en un diagrama. Hay algunos que los van a tocar totalmente en premis, otros se quieren apoyar en WS, otros estaban pensando en Ashur. Recuerden al final de que donde ustedes se sientan más cómodos el es el mejor espacio, ya sea a nivel de si tienen crédito o no en en alguna suscripción o si es que tienen un escenario más este cómodo, fácil o en premis. Al final recuerden que esto si es que tiene algunas limitantes de infraestructura también se va a poder sustentar. Lo que estamos viendo es un poco el el que ustedes pueden ser capaces de identificar qué capacidades de infraestructura necesitamos alrededor del agente para poderlos tangibilizar con componentes claros, ¿no? Entonces, esto Queda como si fuera una tarea para la entrega del 28, ya casi cerrando el mes. Bien, y esto es el último punto que quería tocar en la sesión de hoy. No sé si tienen alguna consulta, ¿no? Bien. Entonces, eh nada más a manera de recuerdo este del grupo dos no pude revisar una tarea este entré al Excel donde se suelen dejar las tareas y no pude encontrar una. El es la tarea número cinco de justamente los tipos de agentes reflexivos, ¿no? Y acá el grupo dos no dejó un enlace y ahí este no sé si alguien me puede ayudar por
Sí, sí. Bueno, yo lo compartí por correo solamente.
Ah, entonces ahí lo correo. Ajá.
Lo busco por correo en todo caso. No se olviden que tenemos el día de hoy el deadline también para la entrega de la siguiente tarea, la tarea número seis. Bien, eso es todo por hoy. Muchísimas gracias este por su participación en clase. Ya nos vamos a estar viendo la siguiente sesión con el tema de multiagente, orquestación multiagente. Ya miren, si se han dado cuenta, hemos avanzado progresivamente de agente a Has Tool como un objeto y ahora vamos a ver implementación de cada gente, pero separado y los vamos a empezar a invocar. Okay, entonces eso sería todo por hoy. Nos vemos en la siguiente clase, muchachos.
Muchas gracias.
Gracias. Buenas noches. Buenas noches.
Muchas gracias, chicos, por la participación el día de hoy. Muchas gracias por la clase.
Gracias. Nos vemos.
Cuídense mucho.