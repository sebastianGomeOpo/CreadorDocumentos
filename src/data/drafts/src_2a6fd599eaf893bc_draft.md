# Clase: src_2a6fd599eaf893bc

*Generado automáticamente el 2025-12-24 22:51*

**Secciones:** 14 | **Palabras:** 7,143

---

## Contenido

- [✓] [Motivaciones para evaluar: riesgo reputacional, pérdidas, daño al usuario y cumplimiento](#motivaciones-para-evaluar-riesgo-reputacional-perd)
- [✓] [Alta tasa de fracaso en proyectos de IA por falta de evaluación (referencia a MIT)](#alta-tasa-de-fracaso-en-proyectos-de-ia-por-falta-)
- [✓] [Evaluación de sistemas multiagente para asegurar valor y calidad](#evaluacion-de-sistemas-multiagente-para-asegurar-v)
- [✓] [Métricas de evaluación de respuestas: similitud, complejidad, pertinencia, cobertura y toxicidad](#metricas-de-evaluacion-de-respuestas-similitud-com)
- [✓] [Evaluación basada en datasets (pregunta–respuesta esperada) y comparación con LLM-as-a-judge](#evaluacion-basada-en-datasets-pregunta-respuesta-e)
- [✓] [Estrategias prácticas de evaluación: roles, personalidades, variación de inputs y orquestación evaluadora](#estrategias-practicas-de-evaluacion-roles-personal)
- [✓] [Patrones de coordinación multiagente aplicados a evaluación (voto, consenso y debate)](#patrones-de-coordinacion-multiagente-aplicados-a-e)
- [✓] [Evaluación comparativa (benchmarking) entre implementaciones y arquitecturas](#evaluacion-comparativa-benchmarking-entre-implemen)
- [✓] [Entornos caóticos y objetivos múltiples: evaluación por rol y por interacción con el entorno](#entornos-caoticos-y-objetivos-multiples-evaluacion)
- [✓] [Gobierno de datos y calidad documental como causa raíz de fallos en agentes](#gobierno-de-datos-y-calidad-documental-como-causa-)
- [✓] [Evaluación en RAG y control de alucinaciones con base de conocimiento (Confluence)](#evaluacion-en-rag-y-control-de-alucinaciones-con-b)
- [✓] [Métricas específicas de RAG: groundedness y relevance (calidad del retrieval)](#metricas-especificas-de-rag-groundedness-y-relevan)
- [✓] [Uso de grafos y “popularidad” del conocimiento como estrategia en bases caóticas](#uso-de-grafos-y-popularidad-del-conocimiento-como-)
- [✓] [Evaluación de seguridad en agentes conversacionales (ataques y pruebas adversarias)](#evaluacion-de-seguridad-en-agentes-conversacionale)

---

## Motivaciones para evaluar: riesgo reputacional, pérdidas, daño al usuario y cumplimiento

Evaluar sistemas de IA, y en particular implementaciones con múltiples agentes, es una práctica que se justifica desde el inicio por sus consecuencias directas en el mundo real. Antes de pensar en mejoras técnicas, la evaluación aparece como una forma de mantener control sobre cómo se usan datos sensibles y sobre las decisiones que el sistema toma o recomienda. En este marco, la evaluación no es un paso “decorativo”, sino un mecanismo para observar el comportamiento del sistema, identificar fallas y evitar que esas fallas se traduzcan en impactos negativos.

Una motivación central es el impacto en cliente. Cuando un agente interactúa con un usuario —sea cliente final u otro tipo de usuario— una respuesta incorrecta o mal contextualizada puede generar efectos inmediatos. En el material se describe, por ejemplo, que un agente puede informar mal si el archivo que consulta estaba caduco o si existían documentos contradictorios, llevando a información errónea entregada al cliente. Este tipo de fallas, además de afectar la experiencia, puede escalar rápidamente a un riesgo reputacional: el usuario atribuye lo ocurrido a la empresa, cuestiona su confiabilidad y exige explicaciones. La evaluación, en este sentido, ayuda a detectar estos desvíos antes y después del despliegue a producción, y a sostener un seguimiento periódico del comportamiento del sistema.

Estas situaciones también pueden traducirse en pérdidas económicas. El documento plantea que una interacción negativa con el cliente puede derivar en pérdidas, precisamente por el daño que provoca en la relación con la empresa y por las consecuencias de entregar información equivocada o insegura. La evaluación se vuelve entonces una forma de reducir la probabilidad de fallas costosas, especialmente cuando “todo el mundo avanzaba” hacia producción sin ejecutar evaluaciones rigurosas: el problema no era avanzar, sino hacerlo sin evidencia de cómo se comportaba el sistema en escenarios relevantes.

Otra motivación clave es prevenir comportamientos dañinos. En el contenido se mencionan ejemplos como contenido tóxico —cuando un sistema empieza a responder con vulgaridad—, así como conductas inseguras relacionadas con el manejo de información sensible y la divulgación involuntaria de datos personales. También se advierte que el comportamiento puede cambiar con condiciones de uso como conversaciones largas, y que un agente puede desviarse de su rol previsto. Evaluar permite mirar estos riesgos “desde diferentes ángulos”, incorporando controles y observando si el sistema mantiene un comportamiento aceptable en distintos escenarios.

Finalmente, la evaluación se relaciona con responsabilidad/ética aplicada. El texto plantea un debate inicial sobre el potencial de “salvar vidas” o mejorar condiciones de salud, pero llegando a una conclusión condicionada: implementar con controles y con cierto control sobre el uso de datos sensibles. Esta idea sitúa la evaluación como parte de una responsabilidad práctica: no basta con una intención positiva del caso de uso; hace falta demostrar, con resultados observables, que el sistema se comporta de manera consistente y que su interacción con datos y usuarios no produce daños evitables. Con esta base, en la siguiente sección se abordará cómo la falta de evaluación contribuye a que muchos proyectos de IA no lleguen a buen puerto.

---

## Alta tasa de fracaso en proyectos de IA por falta de evaluación (referencia a MIT)

Si en la sección anterior vimos por qué evaluar es una decisión necesaria antes de operar con estos sistemas, aquí conviene aterrizarlo en un hecho que aparece en la conversación: existe un reporte de MIT que se menciona para remarcar la alta proporción de fallos de proyectos de IA a nivel global. En esa referencia se indica que solo una fracción muy pequeña “pasaba la valla”, y la explicación que se introduce es directa: muchos equipos avanzan hasta el despliegue sin evaluación, es decir, llevan a producción implementaciones sin ejecutar evaluaciones de forma rigurosa y sin recolectar data del entorno.

Esa dinámica revela un problema de madurez de QA: se “crea el agente”, se valida de manera informal que “funciona”, y se asume que con eso basta. Sin embargo, en el propio discurso se plantea la idea de que un agente también debe “pasar el examen”: evaluar para saber si realmente completa su objetivo, si lo logra parcialmente o si no lo logra. En sistemas donde hay múltiples agentes con roles y objetivos distintos, esto se vuelve todavía más evidente, porque el desempeño depende del entorno (que puede ser confuso o ambiguo) y porque las salidas de un agente pueden influir en decisiones posteriores dentro del mismo plan de ejecución.

Para evitar que el paso a producción ocurra “a ciegas”, la evaluación se entiende como un conjunto de controles y una disciplina operativa. Se mencionan prácticas como agendar evaluaciones online en producción (por ejemplo, en horarios definidos) y observar métricas y telemetría de las ejecuciones. En el fondo, esto se conecta con la gestión de riesgo en producción: no se trata solo de construir, sino de instrumentar el sistema para medir cómo va, de manera recurrente, una vez desplegado.

En ese marco, las vallas/criterios de aceptación aparecen como el umbral que permite decidir si una implementación “queda aprobada” o no, y por qué. La conversación insiste en que esa aprobación no debe depender de intuición: si la mayoría de los fallos de proyectos de IA se asocian a avanzar sin evaluación, entonces elevar la madurez de QA implica formalizar esas vallas y sostenerlas con evaluaciones programadas y controles que transparenten el comportamiento del sistema en el entorno real. Con esto como base, en la siguiente parte podremos conectar estas ideas con la evaluación de sistemas multiagente, donde entran en juego patrones como votación, consenso, debate entre agentes y resolución de conflictos para llegar a conclusiones con controles.

---

## Evaluación de sistemas multiagente para asegurar valor y calidad

Después de reconocer que muchos proyectos de IA fallan por no ejecutar evaluaciones de forma rigurosa, el siguiente paso es concretar cómo evaluar un sistema multiagente para asegurar que realmente entregue valor y mantenga un nivel de calidad aceptable. Evaluar no es un evento aislado, sino parte de un ciclo de feedback continuo: los resultados de evaluación alimentan ajustes tanto en el sistema como en los recursos que lo soportan. La motivación de fondo es práctica: las decisiones que toma un agente pueden condicionar las decisiones del siguiente agente en el plan de ejecución, o incluso las acciones de una persona que recibe esa información. Si el sistema informa mal —por ejemplo, por usar información caduca o contradictoria— el impacto se traslada directamente al usuario y, por extensión, a la organización.

Un punto central de esta evaluación es distinguir entre valor esperado vs valor entregado. En términos operativos, el valor esperado se expresa como “se logró el objetivo” y debe poder verificarse con evidencias observables: si el sistema debía enviar un correo, producir un reporte o completar una acción, la evaluación debe confirmar si el entregable ocurrió o si falló (incluso por causas como una falla de una API). En ese sentido, la medición del grado de éxito del agente se puede plantear de forma gradual: lo logra, lo logra parcialmente o no lo logra. Esta medición no solo refleja el resultado final, sino también la confiabilidad del sistema cuando interactúa en situaciones que pueden ser confusas o ambiguas.

Para sostener esa medición, se requiere establecer criterios de calidad. En el material se plantea que estos criterios se relacionan con asegurar que “se entrega la información que debe ser” y con identificar riesgos como la expiración de la información o la presencia de documentos que se contradicen, porque eso puede llevar a informar erróneamente al cliente. Desde esta perspectiva, evaluar calidad no es solo revisar la forma de una respuesta, sino validar que lo comunicado sea adecuado para el propósito y no induzca decisiones incorrectas aguas abajo.

Con estos elementos, evaluar también significa observar el desempeño (performance) del sistema multiagente como implementación. El texto menciona la necesidad de comprender “cómo sé que me está yendo bien” en esa performance, y sugiere que la evaluación debe abarcar diferentes aspectos de la ejecución. Esto incluye instrumentar evaluaciones de manera recurrente, incluso en producción, programándolas en horarios específicos para monitorear cómo va el sistema con el tiempo. Junto a ello aparece la idea de apoyar la evaluación con telemetría de ejecuciones: métricas operativas como throughput y requests, vistas a lo largo del tiempo, aportan señales del funcionamiento sostenido del sistema y ayudan a detectar degradaciones o cambios.

La validación de comportamiento aparece como una motivación explícita: se evalúa para asegurarse de que “tal o cual comportamiento no debería pasar” y para transparentar las consecuencias de lo que el sistema hace. Esa validación puede apoyarse en mecanismos que emiten un veredicto y devuelven resultados utilizables como feedback para cambiar componentes del sistema cuando no se está cumpliendo algún criterio. En el texto se mencionan dos enfoques de evaluación basados en una invocación a un modelo y otro donde un evaluador puede operar con herramientas adicionales, incluso conectándose a procesos de negocio como herramientas para decidir si “pasó o no” un criterio interno de scoring. En ambos casos, lo importante es que los resultados de la evaluación regresan al ciclo: permiten identificar dónde se está ejecutando bien, pero también dónde no se está logrando un criterio definido.

Finalmente, el material introduce prácticas para probar el sistema desde múltiples ángulos: crear diferentes roles con personalidades, variar sistemáticamente los inputs y verificar consistencia de respuestas. La idea es que, al cambiar personalidades o inputs, el tipo de respuesta que el sistema debe dar se mantenga consistente o “siempre la misma o acercada siempre a la misma”. Además, se sugiere la orquestación de agentes evaluadores, donde diferentes agentes podrían revisar la corrección del resultado, lo que refuerza el enfoque de evaluación por múltiples ángulos. Con estas bases, la siguiente sección profundiza en métricas específicas para evaluar respuestas.

---

## Métricas de evaluación de respuestas: similitud, complejidad, pertinencia, cobertura y toxicidad

En la evaluación de sistemas, una parte central consiste en definir métricas que permitan analizar la calidad de las respuestas generadas según el caso de uso. La idea es seleccionar métricas genéricas que apliquen de forma amplia y, cuando sea necesario, complementar con métricas personalizadas definidas por el equipo. Esta selección no es estática: se ejecutan las evaluaciones, se validan resultados y se reevalúa periódicamente si las métricas siguen siendo relevantes para el objetivo del sistema.

Entre las métricas utilizadas aparece la **similitud semántica**, entendida como una forma de comparar respuestas considerando su cercanía en significado. Junto a ello, se puede medir la **complejidad de respuesta**, que en la práctica puede aproximarse con criterios simples y evaluables por reglas, como condiciones mínimas de longitud (por ejemplo, un umbral de caracteres) o estructura (como la cantidad de párrafos). Este tipo de evaluaciones por reglas permiten operacionalizar rápidamente ciertos estándares formales que se esperan del contenido.

Otra dimensión clave es la **answerability (responde a la pregunta)**: evaluar si el texto generado realmente contesta lo que se está preguntando. En el mismo grupo de criterios se incluye la **pertinencia/relevancia** de la respuesta respecto del input. En el material, se describe un enfoque donde esta pertinencia se determina “netamente por palabras clave y por reglas”, es decir, aplicando verificaciones directas sobre el contenido para estimar si se mantiene dentro del tema solicitado.

Además, la evaluación puede incluir **cobertura**, en el sentido de verificar si la respuesta aborda los elementos que deberían estar presentes para el caso. Aunque el documento no detalla una fórmula única para medirla, sí presenta la idea de inspeccionar características del contenido mediante reglas y criterios definidos para la evaluación, lo que permite aproximar si el resultado incluye aspectos suficientes del encargo.

Finalmente, se incorpora la evaluación de **toxicidad** y la **detección de lenguaje inapropiado** como parte de los controles de seguridad del contenido. Aquí se mencionan validaciones orientadas a identificar si el texto promueve buenas costumbres “sin ser ofensivo” y, de manera más concreta, la necesidad de prohibir o detectar categorías como discriminación, racismo, sexismo, incitación, odio o violencia extrema. En estos casos, el enfoque descrito vuelve a ser aplicable mediante reglas: buscar la presencia de ciertos términos o señales dentro del contenido para marcar incumplimientos.

En conjunto, estas métricas permiten pasar de una evaluación general de “calidad” a una medición más específica y accionable de propiedades de la respuesta —desde su similitud semántica y complejidad, hasta si responde a la pregunta, su pertinencia, su cobertura y su nivel de toxicidad—, preparando el terreno para los siguientes enfoques de evaluación que se verán a continuación.

---

## Evaluación basada en datasets (pregunta–respuesta esperada) y comparación con LLM-as-a-judge

Después de revisar métricas para analizar respuestas (por ejemplo, pertinencia, complejidad y toxicidad), el siguiente paso natural es aterrizar cómo se ejecuta la evaluación cuando contamos con un **dataset de evaluación**. En este enfoque, el dataset reúne ejemplos en los que se define un *input* (la pregunta) y una **ground truth / respuesta esperada** (un *output de referencia* o “caso positivo”). La evaluación se basa en la **comparación de respuestas**: se contrasta lo que produce el sistema frente a esa referencia para estimar qué tan alineada está la salida con lo esperado.

Ese dataset puede construirse de distintas maneras: puede ser manualmente curado, obtenido a partir de un modelo, o incluso generado de forma sintética con ejemplos creados por un modelo. Una vez que existe, se puede “probar con mi dataset” para pasar por ciclos de testing y certificación antes de producción. Sin embargo, el material también advierte un riesgo práctico: si los datasets confeccionados no incluyen usuarios o contextos diferentes a los que “presiden en el dataset”, el sistema puede fallar al enfrentarse a entradas nuevas. Por eso se remarca la importancia de recoger feedback y contar con mecanismos que permitan retroalimentar y ampliar el dataset con casos que no estaban cubiertos inicialmente.

En la práctica, esta evaluación se apoya en **scoring automático** a través de evaluadores (evaluators) que calculan métricas. Parte de ese scoring puede estar “hardcoded”, basado en reglas, y otra parte puede delegarse a un **LLM-as-a-judge**, al que se le asigna un rol explícito para juzgar algún atributo de la salida. En el ejemplo presentado, el judge se invoca para analizar el tono de un artículo y devolver una calificación numérica “de 0 al 10, sin decimales”, donde 0 es “nada cómico” y 10 “extremadamente cómico”. Ese patrón ilustra cómo el judge puede producir una señal cuantitativa que se integra al conjunto de métricas del experimento.

La comparación entre enfoques (reglas versus judge) se describe en términos de ventajas y limitaciones. Las reglas tienden a ser más rápidas y “súper reproducibles”; al estar determinísticas, no introducen variaciones atribuibles al evaluador, pero también pueden quedarse cortas para capturar aspectos de calidad más abstractos o contextuales. En cambio, con LLM-as-a-judge se puede incorporar contexto y razonamiento para aproximarse a métricas de calidad más difíciles de codificar, aunque esto introduce un costo computacional asociado (tokens y cómputo).

Operativamente, el flujo se plantea como un pipeline: desde la aplicación se obtiene o consolida un dataset, se ejecutan evaluators (reglas o judges), se obtiene un resultado agregado y luego se visualiza en un dashboard, ya sea desde un notebook o desde una plataforma con interfaz. En ese proceso, se enfatiza que primero se registra el input y el output de referencia, se guarda como dataset y luego se agregan filas para crecer el conjunto; finalmente se seleccionan los evaluators disponibles (por ejemplo, para corrección u otros atributos) y se ejecuta la evaluación.

Este tipo de ejecución sistemática habilita el **benchmarking interno**: repetir pruebas sobre datasets controlados y comparables en el tiempo para observar cómo se comporta una implementación a lo largo de iteraciones (por ejemplo, tras ajustes del sistema o del flujo). Con esta base, la siguiente sección se enfocará en estrategias prácticas para ampliar y tensionar estos datasets de evaluación mediante variación deliberada de inputs, roles y configuraciones de evaluación.

---

## Estrategias prácticas de evaluación: roles, personalidades, variación de inputs y orquestación evaluadora

Después de haber visto enfoques de evaluación apoyados en datasets de pregunta–respuesta esperada y otras alternativas para contrastar implementaciones, esta sección aterriza estrategias prácticas para evaluar agentes desde el diseño mismo de la interacción: usar **role-playing en agentes**, incorporar **personalidades**, aplicar **variación sistemática de prompts/inputs** y coordinar una **orquestación de agentes evaluadores** para lograr una **evaluación por múltiples ángulos**.

Una idea central es que, en la práctica, un mismo caso de uso puede evaluarse mejor cuando se analiza desde perspectivas distintas. En el contexto discutido, esto se traduce en crear instancias que miren el sistema con objetivos diferentes: por ejemplo, una instancia que observe el comportamiento desde el lado de seguridad, y otra que revise aspectos como la toxicidad o el estilo de interacción. Esta separación de miradas permite detectar problemas que, con una sola evaluación “general”, podrían pasar inadvertidos, y refuerza la noción de evaluar el caso de uso “desde diferentes ángulos”.

Dentro de estas estrategias, el **role-playing en agentes** aparece como un mecanismo para estructurar expectativas: definir roles como “Virtual Lab AI”, “software security AI agent” o “employee support AI agent” implica que cada agente opera con objetivos específicos, y la evaluación debe medir precisamente qué tan bien completa ese objetivo (si lo logra, lo logra parcialmente o no lo logra). Este encuadre por roles se puede enriquecer con **personalidades**, porque no solo interesa qué responde el agente, sino también cómo lo hace dentro de los límites aceptables del rol. En términos de evaluación, el rol y la personalidad sirven como condiciones explícitas contra las que se puede comparar el comportamiento observado, incluyendo casos en los que haya “violación en el rol” o intentos de empujar al agente a actuar fuera de lo que debía hacer.

A partir de ahí, una táctica directa es aplicar **variación sistemática de prompts/inputs**. La lógica descrita es sencilla: si se toma, por ejemplo, un agente de recursos humanos, se pueden entregar entradas distintas —y también variar personalidades— para observar si la conducta se mantiene alineada con lo esperado. Esto conecta con un criterio clave: la **consistencia de respuestas**. La expectativa planteada es que, pese a variar inputs y personalidades, la respuesta del agente “debería ser siempre la misma o acercada siempre a la misma”, es decir, estable dentro de lo que se considera correcto para el caso de uso.

Para sostener esta consistencia, cobra relevancia la **orquestación de agentes evaluadores**. La idea expuesta es que, con un sistema orquestado, agentes distintos pueden revisar la respuesta producida y ayudar a determinar si fue correcta, habilitando una evaluación más robusta que no dependa de una sola mirada. Además, esta evaluación no se entiende como un evento único: se mencionan prácticas como agendar evaluaciones periódicas en producción (por ejemplo, en horarios específicos) para monitorear cómo “vamos yendo”, reforzando que evaluar es un proceso iterativo, con criterios de éxito que a veces son subjetivos y requieren ciclos de ajuste y recolección de feedback.

Esta aproximación también se conecta con riesgos propios de agentes expuestos al público: se mencionan usuarios maliciosos que intentan saltarse capas de seguridad, inducir la exposición de información de la empresa o solicitar datos de otras personas, e incluso “romper” el prompt para modificar el comportamiento. En ese marco, el uso de roles, personalidades, la variación sistemática de entradas y la orquestación evaluadora ayudan a estresar el sistema y observar si mantiene su comportamiento esperado sin desviarse, especialmente en conversaciones largas o en condiciones donde la interacción puede volverse confusa o ambigua.

Con estas bases, queda preparado el terreno para profundizar en cómo coordinar múltiples agentes dentro del proceso evaluador y cómo estructurar patrones de coordinación aplicados a evaluación en la sección siguiente.

---

## Patrones de coordinación multiagente aplicados a evaluación (voto, consenso y debate)

En la sección anterior se revisaron estrategias prácticas para organizar evaluaciones en sistemas multiagente. Sobre esa base, aquí el foco se desplaza a cómo los patrones de coordinación entre agentes pueden aplicarse directamente a la evaluación, especialmente cuando aparecen posiciones u opiniones divergentes sobre si una ejecución “salió bien” o no. La idea central es que la evaluación no solo genera un resultado, sino que alimenta un ciclo de feedback continuo: el veredicto sirve para ajustar el sistema y, más ampliamente, los recursos y el entorno que lo rodean.

Cuando se trabaja en multiagente, la resolución de conflictos se vuelve un punto práctico de diseño: distintos componentes pueden interpretar de manera distinta el cumplimiento de un objetivo o la calidad de una salida. En ese contexto, patrones como la votación, el consenso y el debate entre agentes ofrecen formas de reconciliar discrepancias. La votación permite llegar a una decisión agregada cuando hay varias evaluaciones disponibles; el consenso busca una convergencia que reduzca la ambigüedad entre criterios o juicios; y el debate entre agentes hace explícitas las razones detrás de conclusiones distintas, lo que resulta útil precisamente cuando hay posiciones/opiniones divergentes.

Este tipo de coordinación también se refleja en la idea de combinar múltiples evaluadores para producir un resultado único. En el material se describe la posibilidad de usar un evaluador compuesto (“composite evaluator”) que integra varios evaluadores y produce un puntaje promedio o un veredicto final. En términos de coordinación, esto opera como un mecanismo de consenso: no depende de una sola señal, sino de la reconciliación de varias, con la intención de llegar a conclusiones con controles, es decir, conclusiones sustentadas por más de una mirada evaluadora. Esas conclusiones se convierten en feedback accionable para modificar aquello que no está cumpliendo cierto criterio.

Aplicar estos patrones a evaluación también implica que los resultados se integren a un proceso operativo. El documento menciona que pueden agendarse evaluaciones periódicas (por ejemplo, en horarios definidos) para revisar “cómo estamos yendo”, y que además puede incorporarse telemetría de ejecuciones de agentes (como throughput, requests y vistas por tiempo). En conjunto, estos elementos habilitan un control continuo: se evalúa, se obtienen resultados, se identifican brechas frente a criterios, y se ajusta el sistema para sostener una mejora continua. En la siguiente sección, este hilo se conecta con la necesidad de contrastar implementaciones y arquitecturas, ampliando la mirada más allá del patrón de coordinación interno.

---

## Evaluación comparativa (benchmarking) entre implementaciones y arquitecturas

Después de revisar patrones de coordinación multiagente aplicados a evaluación, el siguiente paso natural es usar la evaluación comparativa para sostener decisiones técnicas con evidencia. La idea es contrastar el desempeño de un sistema multiagente a lo largo del tiempo y, sobre todo, habilitar la comparación entre implementaciones: probar un sistema “uno contra el otro” y observar cómo cambian los resultados cuando variamos componentes o configuraciones.

Esta evaluación comparativa parte de definir criterios de comparación que hagan sentido para el caso de uso y que puedan medirse de forma consistente. En el material se mencionan métricas asociadas a la calidad del resultado, como intent resolution, coherence y reconciliation success, así como métricas operativas obtenidas de telemetría, por ejemplo agent runs, throughput y requests, con vistas por tiempo. También aparece la posibilidad de programar evaluaciones online en horarios definidos (por ejemplo, de madrugada en producción) para monitorear “cómo estamos yendo” sin depender de chequeos esporádicos. En conjunto, estos criterios permiten observar tanto la calidad como el comportamiento del sistema durante su operación.

Con esos criterios, se vuelve posible evaluar arquitecturas alternativas y medir sus consecuencias. El contexto describe que incluso se puede “hacerme otro sistema” y ver cómo cambian variables como el tiempo de ejecución y el costo de token, comparando directamente dos enfoques. Se comparte una experiencia donde, en un esquema multiagente, se usó un modelo más costoso como subagente y otro como supervisor; al ejecutar ciertas instrucciones, el sistema “se demoraba un montón” por la cantidad de intercambio entre agentes y por traducciones repetidas entre idiomas, lo que ilustra cómo una arquitectura puede degradar su desempeño por dinámicas internas, no solo por la tarea final.

El objetivo de este benchmarking no es únicamente optimizar métricas: es comprender resultados y trade-offs que impactan el despliegue a producción. Si un agente informa mal por archivos caducos o contradictorios, el impacto puede llegar directamente al cliente, con riesgo reputacional y pérdidas económicas para la empresa. Por eso, la evaluación comparativa se conecta con responsabilidad/ética aplicada: al transparentar cómo se comporta cada implementación y qué decisiones induce en ejecuciones posteriores o en usuarios, se reducen comportamientos dañinos o inseguros y se evita avanzar a producción sin una verificación rigurosa.

En la siguiente sección, este enfoque se ampliará hacia escenarios donde el entorno y los objetivos no son estables, y donde evaluar por rol e interacción con el entorno se vuelve clave para entender el rendimiento real del sistema.

---

## Entornos caóticos y objetivos múltiples: evaluación por rol y por interacción con el entorno

Tras comparar implementaciones y arquitecturas, el siguiente paso es entender cómo evaluar cuando el problema real no está completamente “cerrado” ni perfectamente definido. En esta perspectiva, el entorno (state) no se reduce a un conjunto de casos de prueba, sino que abarca “el problema documentado y no documentado” que existe en el mundo, y que el sistema intenta implementar en su contexto para cada usuario. Como cada usuario llega con un contexto distinto y, además, ese contexto se va moviendo durante la conversación, el entorno de ejecución de un agente autónomo termina siendo una agregación de múltiples contextos dinámicos que también debe entrar en la evaluación.

Esta variabilidad hace visible un punto crítico: la coherencia del entorno. Si el entorno no es coherente, no es adecuado o no es congruente con la acción que va a tomar el agente, el agente se va a equivocar. En la práctica, los humanos crean entornos caóticos que no son “blanco y negro”; pueden ser confusos y estar marcados por ambigüedad. En ese marco, cuando un sistema detecta un error en sus métricas, puede ocurrir que el usuario “se salió” hacia una zona que existe como entorno documentado por la humanidad, pero que no está contemplada dentro de la solución específica del agente. Dicho de otro modo, el agente puede estar operando en un entorno real más amplio que el que fue implementado o cubierto por su configuración y pruebas.

En escenarios así, la evaluación por objetivo se vuelve necesaria, pero no suficiente si se mira como un único criterio global. En sistemas con múltiples instancias o múltiples agentes, cada componente puede operar con objetivos por rol distintos. El material menciona ejemplos de roles diferentes (por ejemplo, agentes enfocados en laboratorio virtual, seguridad de software o soporte a empleados) que interactúan con entornos diferentes, e incluso pueden trabajar entre sí. Precisamente por esa diversidad, la evaluación debe preguntar, para cada rol, cuán bien completa su objetivo: si hay cumplimiento total/parcial, o si directamente no lo logra. Esta granularidad permite reconocer que un mismo caso de uso puede evaluarse desde varios ángulos, y que esas múltiples instancias potencian la evaluación al revelar fallos específicos del rol o del tipo de interacción con el entorno.

Aterrizar esta idea implica diseñar evaluaciones que consideren entradas variadas y contextos distintos por rol. Se sugiere crear diferentes roles con personalidades, entregar distintos inputs y verificar si la respuesta se mantiene como “la esperada” o al menos se acerca consistentemente a ella. También se menciona el uso de un dataset de preguntas con respuestas esperadas para someter al agente a pruebas repetibles. Con ello, la evaluación no solo observa el resultado final, sino su estabilidad frente a cambios razonables del contexto del usuario, que es justamente donde el entorno (state) puede volverse más caótico y ambiguo.

Finalmente, dado que estos sistemas operan en producción con usuarios y contextos reales, la evaluación debe sostenerse en el tiempo y no limitarse a una corrida puntual. El material plantea la posibilidad de agendar evaluaciones periódicas (por ejemplo, en horarios específicos) para monitorear cómo va el comportamiento del sistema. Esto prepara el terreno para analizar, en la siguiente sección, cómo ciertos problemas sistemáticos en las fuentes y artefactos de trabajo terminan manifestándose como fallos observables en los agentes.

---

## Gobierno de datos y calidad documental como causa raíz de fallos en agentes

En escenarios reales, incluso cuando ya se han ajustado el *system prompt*, la metadata de herramientas o la instrumentación, pueden persistir quejas de usuarios que no se explican con los logs ni con cambios en la configuración del agente. En esos casos, el problema suele estar en el entorno que el agente “hereda” para ejecutar: el contexto que llega con cada usuario. Ese contexto está compuesto, en gran parte, por la data del cliente que regresa de APIs y por información del ambiente que puede no estar documentada o no estar siendo procesada adecuadamente. Si esa data es deficiente, se cumple el principio de “garbage in, garbage out”: el agente falla no por su lógica interna, sino por el insumo sobre el que decide.

Aquí entra el **gobierno de datos** como medida estructural. No se trata solo de “tener documentos”, sino de establecer criterios para asegurar que la información entregada sea la que debe ser y que, si expira, se gestione como tal. Cuando esa disciplina no existe, se vuelve común que el agente informe mal al usuario, por ejemplo porque el archivo estaba caduco o porque el material publicado estaba en conflicto con otro. Ese tipo de error no es trivial: el usuario puede ejecutar pasos incorrectos basados en la respuesta del agente y luego atribuir el daño directamente a la organización.

Un foco recurrente de fallos es la **documentación contradictoria**. Si el agente recibe información que entra en conflicto, puede responder algo que no es cierto o que queda fuera del contexto correcto. Además, muchos errores nacen de **ambigüedad contextual (ubicación/tiempo)**: variables como la ubicación, el clima o el momento del año cambian la respuesta esperada, pero esas condiciones no siempre quedan reflejadas en la documentación. El resultado es que una misma pregunta puede requerir respuestas distintas según condiciones que no fueron explicitadas, y el agente termina contestando sin el marco adecuado.

Para reducir estos problemas, el texto enfatiza prácticas concretas dentro del gobierno de datos: **control de versiones/retención** y **unificación de fuentes**. Controlar versiones y retención implica que, si un documento “vence”, se considere fuera de vigencia y no se trate como verdad operativa. La unificación de fuentes apunta a que información clave —por ejemplo requisitos de un producto, qué cubre y qué no cubre, tasas o tarifas— esté en un solo documento único, lo que facilita referenciarla y evita que múltiples publicaciones compitan entre sí con definiciones distintas.

En conjunto, estas medidas elevan la **calidad de la base de conocimiento** que alimenta al agente. Cuando el entorno documental es coherente, vigente y unificado, el agente tiene más probabilidad de actuar de forma congruente con la acción que debe tomar para lograr su objetivo. Con esto, la evaluación de agentes no solo identifica fallos; también orienta el ciclo de retroalimentación hacia el lugar correcto: mejorar el gobierno de la información que el agente usa como contexto. En la siguiente sección, esta preocupación por la calidad y el control de la información se conectará con cómo evaluar respuestas frente a una base de conocimiento específica y cómo reducir respuestas fuera de contexto.

---

## Evaluación en RAG y control de alucinaciones con base de conocimiento (Confluence)

Después de reconocer que muchos fallos en agentes se explican por gobierno de datos y calidad documental, el siguiente paso es evaluar de manera sistemática cómo se comporta un agente cuando trabaja con RAG (retrieval-augmented generation) y una base de conocimiento concreta. En la práctica, estas evaluaciones buscan confirmar que el agente “entrega la información que debe ser” según lo disponible en su conocimiento, y detectar cuándo empieza a responder de forma incorrecta o a producir alucinaciones.

En implementaciones donde **Confluence** es la fuente principal, un punto recurrente de quiebre no es necesariamente el modelo, sino el documento “no correctamente administrado por humanos”. Por eso, la evaluación no se limita a mirar el resultado final del agente: también debe cubrir la **ingesta de datos**, es decir, el proceso mediante el cual la organización incorpora información a Confluence y la pone a disposición del sistema. En escenarios reales, se prueba por un lado que la ingesta efectivamente “insertó” la información esperada en Confluence, y por otro, que cuando el usuario consulta al agente, este responde con un contexto acorde y no se desvía hacia contenido no sustentado.

Un aspecto central del control de alucinaciones en este contexto es el **manejo de información ausente/expirada**. Si una página fue borrada o “ya venció”, el comportamiento esperado no es que el agente improvise: debería reconocer la ausencia y responder explícitamente que no tiene esa información. Esta conducta se relaciona con una **limitación por dominio**: incluso si el agente “podría” tener información por otras vías, aquí se asume que su respuesta está limitada a lo que fue incorporado y mantenido en su base de conocimiento; si no aparece ahí, corresponde admitirlo.

Para sostener esto en el tiempo, la evaluación debe operar como un ciclo continuo. El material describe la posibilidad de **agendar evaluaciones** periódicas en producción (por ejemplo, en la madrugada de un día fijo) para observar “cómo estamos yendo”, apoyándose en telemetría de ejecuciones del agente y en tableros que muestren tendencias. Con los resultados, se identifica el “grado de salud” del agente y se genera retroalimentación accionable: entender qué falló, qué funcionó, y qué ajustes aplicar (por ejemplo, al sistema asociado o a los controles que se hayan configurado).

Una forma concreta de instrumentar estas validaciones es construir un conjunto de casos con un **input** y un **output de referencia** (el “caso positivo”) y guardarlo como un dataset. Ese dataset se usa para ejecutar evaluaciones con distintos evaluadores disponibles en herramientas de evaluación, incluyendo uno orientado a alucinación. En este esquema, la evaluación no es un evento único, sino un mecanismo repetible que permite comparar ejecuciones y detectar degradaciones.

Esa repetibilidad es la base de las **pruebas de regresión de conocimiento**: cada vez que cambia la información en Confluence (por actualización, expiración o eliminación) o cada vez que se modifica la ingesta de datos, se vuelve a correr el mismo dataset de pruebas para verificar que el agente siga respondiendo dentro de lo esperado, que no invente, y que mantenga el “lo siento/no tengo esa información” cuando corresponde por ausencia o expiración. Esta disciplina prepara el terreno para profundizar, en la siguiente sección, en métricas específicas del desempeño en RAG.

---

## Métricas específicas de RAG: groundedness y relevance (calidad del retrieval)

En la evaluación de sistemas con recuperación aumentada, una parte clave es medir qué tan bien el agente se mantiene dentro del “entorno” que le corresponde: el problema documentado y accesible para la solución. El agente autónomo actúa en función de un entorno (state) que cambia a medida que el usuario avanza en la conversación y aporta contexto. Cuando aparecen fallas en las métricas, suele ser porque el agente “se salió” del entorno esperado: la información puede existir y estar documentada “por la humanidad”, pero no necesariamente está en la solución o en el contexto que el agente debía usar en ese momento. Por eso, las métricas de RAG se vuelven una forma práctica de comprobar coherencia del entorno y, en consecuencia, anticipar errores.

Dos métricas centrales en este punto son groundedness y relevance (calidad del retrieval). Groundedness describe cuán cerca está la respuesta de la data disponible para sustentar lo que se afirma; es decir, si el agente realmente está entregando información fiel a lo que recuperó como evidencia. Relevance, en cambio, se enfoca en si lo recuperado corresponde a la pregunta: si el sistema trajo un chunk que no se relaciona con lo que el usuario pidió, la respuesta puede desviarse y “responder cualquier cosa”. En ese escenario, el comportamiento termina viéndose como una alucinación, aunque el problema de origen haya sido la recuperación.

Desde esta perspectiva, la recuperación de evidencia no es solo un paso previo a generar texto, sino una parte evaluable del desempeño del agente. Si el agente toma un chunk inadecuado, el error se propaga: la respuesta pierde ajuste al objetivo del usuario y se incrementa la probabilidad de dar instrucciones o afirmaciones incongruentes con el entorno. De ahí que sea importante reconocer los errores de retrieval como una causa frecuente del problema, y no confundirlos automáticamente con fallas puras de generación.

Estas métricas también sirven para el diagnóstico de alucinación. Cuando una respuesta parece inventada, conviene preguntarse primero si el entorno que el agente “vio” era coherente: si el chunk recuperado no correspondía a la pregunta, el sistema puede producir una salida incorrecta aun cuando esté “anclándose” a algo. En cambio, si lo recuperado era pertinente pero la respuesta se aleja de esa evidencia, el problema se observa directamente en groundedness. En ambos casos, groundedness y relevance permiten separar con mayor claridad si el fallo proviene de la selección de evidencia o de cómo el agente la usa para cumplir total o parcialmente su objetivo por rol.

En la siguiente sección, este análisis se conecta con estrategias para manejar entornos especialmente caóticos, donde la organización del conocimiento complica todavía más la recuperación y, por extensión, la calidad del comportamiento del agente.

---

## Uso de grafos y “popularidad” del conocimiento como estrategia en bases caóticas

Cuando una base de conocimiento no está organizada como un documento único y consistente, aparece un escenario de caos: pueden convivir afirmaciones contradictorias dentro del mismo repositorio. En ese contexto, conectar “tal cual” una fuente amplia de información no garantiza que el sistema entregue respuestas coherentes, porque la misma colección puede contener versiones opuestas de un mismo hecho.

Una estrategia que se plantea para la gestión de conocimiento caótico es apoyarse en un **knowledge graph** (una base de datos de grafos). La idea es que el conocimiento no se trate solo como texto aislado, sino como puntos de información conectados entre sí. En este enfoque, la estructura del grafo importa: se considera la cantidad de **vértices y conectividad** asociados a cada punto de dato. A mayor cantidad de conexiones alrededor de una pieza de información, se interpreta que esa pieza “gana” relevancia dentro del sistema.

Esa conectividad funciona como **popularidad como señal**. En una base de grafos, un nodo puede volverse “popular” porque está vinculado a muchos otros nodos, y esa popularidad puede usarse para el **ranking de información**: lo más conectado tiende a subir en prioridad frente a lo que queda aislado o débilmente relacionado. El texto lo compara con una **analogía con búsqueda web**: así como los motores de búsqueda han usado (de manera cada vez más sofisticada) estructuras de grafos para ordenar resultados, aquí la conectividad se aprovecha para priorizar qué conocimiento se considera más relevante.

Ahora bien, este mismo mecanismo introduce un matiz importante: la popularidad puede terminar pareciéndose a un “factor de verdad” por repetición. Se menciona la idea de que, si algo se repite muchas veces, puede asumirse como cierto por el solo hecho de volverse popular. En un enfoque basado en grafos, esa dinámica puede reflejarse en el ranking: una afirmación muy conectada puede imponerse sobre otras, incluso si el motivo de su predominio es la frecuencia o la centralidad dentro del grafo. Con esto, la sección deja planteado que, en entornos caóticos, los grafos y el ranking por popularidad son una forma práctica de ordenar información, y que ese ordenamiento abre preguntas que más adelante conectarán con otras dimensiones de evaluación del comportamiento del sistema.

---

## Evaluación de seguridad en agentes conversacionales (ataques y pruebas adversarias)

En la sección anterior se discutían estrategias para ordenar conocimiento en entornos caóticos; aquí el foco cambia hacia un aspecto igual de decisivo al momento de llevar un agente a uso real: evaluar su seguridad. La motivación es práctica y directa: las decisiones y respuestas de un agente pueden gatillar acciones posteriores, ya sea dentro del propio plan de ejecución (cuando intervienen más componentes) o en manos del usuario que confía en la recomendación. Si el agente informa mal o conduce a un paso inseguro, ese error puede materializarse en pérdidas, problemas reputacionales o compromiso de información.

Este riesgo se amplifica por la **exposición pública** de muchos agentes conversacionales: al estar abiertos a interacción con personas externas, aparecen **usuarios maliciosos** que intentan forzar comportamientos no deseados. En ese contexto, la evaluación no puede limitarse a verificar que “funciona” en escenarios previstos; también debe medir qué pasa cuando el agente recibe entradas hostiles o inesperadas y si mantiene sus límites bajo presión.

Un objetivo central de estas evaluaciones es reducir la posibilidad de **exfiltración de información**, especialmente cuando el agente puede acceder a datos sensibles y el usuario intenta obtener “información propia de la empresa” o datos de otras personas. El problema no es solo que el agente “se equivoque”, sino que divulgue sin querer información que no debería entregar. Por eso, las pruebas de seguridad buscan evidenciar filtraciones y fallas de control antes de que el sistema quede operando frente a usuarios reales.

Entre los vectores más mencionados está el **prompt injection / jailbreak**, entendido como intentos deliberados por “romper el prompt” para que el agente responda de otra forma, cambie su rol o actúe fuera de su propósito. En el material se ejemplifica este tipo de técnicas como intentos de saltarse restricciones incluso con entradas aparentemente inocuas, como el caso de un emoji que “por dentro” contiene un prompt y termina eludiendo controles. Estos casos ilustran por qué la seguridad debe evaluarse con entradas adversas y no solo con interacciones normales.

Para abordar esto se recurre a **pruebas adversarias**: ejecutar ataques controlados, variar inputs y observar si el agente mantiene el comportamiento esperado y respeta sus límites. La idea es someter al sistema a escenarios diseñados para fallar, justamente para identificar debilidades en condiciones más cercanas a producción, donde la diversidad de contextos de los usuarios es enorme y no se puede anticipar todo desde un entorno de pruebas limitado.

Estas pruebas se conectan con la necesidad de definir **capas/controles de seguridad**. En el documento se sugiere que, ante casos de uso con datos sensibles, es necesario “implementar con controles” y establecer mecanismos que mantengan cierto control sobre el uso de la información. La evaluación, entonces, no solo pregunta si el agente responde bien, sino si esas capas efectivamente resisten intentos de evasión: si alguien trata de saltarse una capa, si logra cambiar instrucciones, o si el agente termina cumpliendo un objetivo distinto al original.

Finalmente, la evaluación de seguridad se plantea como una práctica que debe sostenerse en el tiempo: no basta con probar una vez y desplegar. Se menciona la posibilidad de agendar evaluaciones en línea en producción (por ejemplo, en horarios definidos) y observar métricas de éxito de ataque, como parte de una gestión continua del riesgo. Con esto se cierra el recorrido: después de explorar diseño, ejecución y evaluación de agentes, el mensaje final es que la seguridad debe comprobarse de forma deliberada y recurrente, especialmente cuando el agente está expuesto públicamente y puede ser presionado por usuarios maliciosos.

---

## Metadata de Generación

```yaml
generated_at: 2025-12-24T22:51:24.142721
total_sections: 14
total_words: 7143
successful_sections: 14
average_processing_time_ms: 17885
```

### Advertencias

- ⚠️ [Evaluación en RAG y control de alucinaciones con base de conocimiento (Confluence)] Faltan conceptos must_include: Confluence como fuente
- ⚠️ [Métricas específicas de RAG: groundedness y relevance (calidad del retrieval)] Faltan conceptos must_include: chunks
- ⚠️ [Evaluación de seguridad en agentes conversacionales (ataques y pruebas adversarias)] Faltan conceptos must_include: riesgo por exposición pública